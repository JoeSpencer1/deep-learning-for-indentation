PS C:\Users\josep\GitHub\deep-learning-for-indentation\src> python nn.py
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

               n     E (GPa)   sy (GPa)     C (GPa)  ...       n_80  E (GPa)_80  sy (GPa)_80  C (GPa)_80
count  76.000000   76.000000  76.000000   76.000000  ...  76.000000   76.000000    76.000000   76.000000
mean    0.225000  106.842105   1.322632   92.765355  ...   0.225000  106.842105     1.322632  251.095079
std     0.193305   68.278069   1.069160   65.732080  ...   0.193305   68.278069     1.069160  170.222934
min     0.000000   10.000000   0.030000    2.731300  ...   0.000000   10.000000     0.030000   10.179000
25%     0.075000   50.000000   0.300000   39.524750  ...   0.075000   50.000000     0.300000  121.250000
50%     0.200000   90.000000   1.000000   81.686500  ...   0.200000   90.000000     1.000000  224.500000
75%     0.350000  170.000000   2.000000  142.987500  ...   0.350000  170.000000     2.000000  385.250000
max     0.500000  210.000000   3.000000  261.330000  ...   0.500000  210.000000     3.000000  646.000000

[8 rows x 20 columns]

Cross-validation iteration: 1
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building feed-forward neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\fnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.050753 s

'compile' took 0.225599 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss              Test loss               Test metric
0         [4.65e+04, 6.46e-01]    [4.98e+04, 6.46e-01]    [4.98e+04]
1000      [9.60e+01, 3.45e-01]    [9.30e+01, 3.45e-01]    [9.30e+01]
2000      [9.86e+01, 3.20e-01]    [1.14e+02, 3.20e-01]    [1.14e+02]
3000      [1.02e+02, 3.01e-01]    [1.17e+02, 3.01e-01]    [1.17e+02]
4000      [9.30e+01, 2.85e-01]    [1.07e+02, 2.85e-01]    [1.07e+02]
5000      [6.31e+01, 2.71e-01]    [7.56e+01, 2.71e-01]    [7.56e+01]
6000      [8.38e+01, 2.60e-01]    [9.76e+01, 2.60e-01]    [9.76e+01]
7000      [6.06e+01, 2.50e-01]    [7.28e+01, 2.50e-01]    [7.28e+01]
8000      [1.89e+02, 2.41e-01]    [2.09e+02, 2.41e-01]    [2.09e+02]
9000      [8.11e+01, 2.32e-01]    [7.95e+01, 2.32e-01]    [7.95e+01]
10000     [1.14e+02, 2.25e-01]    [1.15e+02, 2.25e-01]    [1.15e+02]
11000     [1.84e+02, 2.18e-01]    [2.04e+02, 2.18e-01]    [2.04e+02]
12000     [8.01e+01, 2.12e-01]    [7.87e+01, 2.12e-01]    [7.87e+01]
13000     [1.10e+02, 2.06e-01]    [1.11e+02, 2.06e-01]    [1.11e+02]
14000     [1.56e+02, 2.00e-01]    [1.74e+02, 2.00e-01]    [1.74e+02]
15000     [6.58e+01, 1.94e-01]    [6.35e+01, 1.94e-01]    [6.35e+01]
16000     [9.57e+01, 1.88e-01]    [9.55e+01, 1.88e-01]    [9.55e+01]
17000     [1.31e+02, 1.83e-01]    [1.48e+02, 1.83e-01]    [1.48e+02]
18000     [5.06e+01, 1.77e-01]    [4.72e+01, 1.77e-01]    [4.72e+01]
19000     [6.26e+01, 1.73e-01]    [6.00e+01, 1.73e-01]    [6.00e+01]
20000     [8.93e+01, 1.68e-01]    [1.03e+02, 1.68e-01]    [1.03e+02]
21000     [4.47e+01, 1.63e-01]    [4.10e+01, 1.63e-01]    [4.10e+01]
22000     [6.81e+01, 1.58e-01]    [6.60e+01, 1.58e-01]    [6.60e+01]
23000     [9.68e+01, 1.53e-01]    [1.11e+02, 1.53e-01]    [1.11e+02]
24000     [5.13e+01, 1.48e-01]    [4.81e+01, 1.48e-01]    [4.81e+01]
25000     [5.72e+01, 1.44e-01]    [5.46e+01, 1.44e-01]    [5.46e+01]
26000     [8.28e+01, 1.39e-01]    [9.53e+01, 1.39e-01]    [9.53e+01]
27000     [4.33e+01, 1.35e-01]    [3.98e+01, 1.35e-01]    [3.98e+01]
28000     [3.10e+01, 1.31e-01]    [2.75e+01, 1.31e-01]    [2.75e+01]
29000     [3.30e+01, 1.27e-01]    [3.06e+01, 1.27e-01]    [3.06e+01]
30000     [3.61e+01, 1.27e-01]    [4.31e+01, 1.27e-01]    [4.31e+01]

Best model at step 28000:
  train loss: 3.11e+01
  test loss: 2.76e+01
  test metric: [2.75e+01]

'train' took 16.201427 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Cross-validation iteration: 2
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building feed-forward neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\fnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.037738 s

'compile' took 0.170482 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss              Test loss               Test metric
0         [2.12e+04, 6.32e-01]    [2.04e+04, 6.32e-01]    [2.04e+04]
1000      [2.86e+02, 3.43e-01]    [2.75e+02, 3.43e-01]    [2.75e+02]
2000      [3.00e+02, 3.26e-01]    [2.89e+02, 3.26e-01]    [2.89e+02]
3000      [2.96e+02, 3.16e-01]    [2.85e+02, 3.16e-01]    [2.85e+02]
4000      [3.29e+02, 3.08e-01]    [3.16e+02, 3.08e-01]    [3.16e+02]
5000      [2.65e+02, 3.01e-01]    [2.57e+02, 3.01e-01]    [2.57e+02]
6000      [2.36e+02, 2.95e-01]    [2.27e+02, 2.95e-01]    [2.27e+02]
7000      [2.39e+02, 2.90e-01]    [2.30e+02, 2.90e-01]    [2.30e+02]
8000      [2.57e+02, 2.84e-01]    [2.49e+02, 2.84e-01]    [2.49e+02]
9000      [3.90e+01, 2.78e-01]    [3.56e+01, 2.78e-01]    [3.56e+01]
10000     [7.69e+01, 2.73e-01]    [7.21e+01, 2.73e-01]    [7.21e+01]
11000     [2.53e+02, 2.67e-01]    [2.46e+02, 2.67e-01]    [2.46e+02]
12000     [4.64e+01, 2.62e-01]    [4.27e+01, 2.62e-01]    [4.27e+01]
13000     [7.22e+01, 2.58e-01]    [6.75e+01, 2.58e-01]    [6.75e+01]
14000     [2.07e+02, 2.53e-01]    [2.02e+02, 2.53e-01]    [2.02e+02]
15000     [9.07e+01, 2.48e-01]    [8.44e+01, 2.48e-01]    [8.44e+01]
16000     [9.79e+01, 2.43e-01]    [9.12e+01, 2.43e-01]    [9.12e+01]
17000     [1.45e+02, 2.38e-01]    [1.45e+02, 2.38e-01]    [1.45e+02]
18000     [5.25e+01, 2.32e-01]    [4.61e+01, 2.32e-01]    [4.61e+01]
19000     [7.03e+01, 2.26e-01]    [6.39e+01, 2.26e-01]    [6.39e+01]
20000     [1.36e+02, 2.20e-01]    [1.34e+02, 2.20e-01]    [1.34e+02]
21000     [3.95e+01, 2.14e-01]    [3.52e+01, 2.14e-01]    [3.52e+01]
22000     [5.43e+01, 2.08e-01]    [4.98e+01, 2.08e-01]    [4.98e+01]
23000     [1.13e+02, 2.02e-01]    [1.11e+02, 2.02e-01]    [1.11e+02]
24000     [3.15e+01, 1.96e-01]    [2.84e+01, 1.96e-01]    [2.84e+01]
25000     [4.36e+01, 1.90e-01]    [4.03e+01, 1.90e-01]    [4.03e+01]
26000     [1.02e+02, 1.85e-01]    [9.96e+01, 1.85e-01]    [9.96e+01]
27000     [3.07e+01, 1.79e-01]    [2.84e+01, 1.79e-01]    [2.84e+01]
28000     [3.72e+01, 1.74e-01]    [3.51e+01, 1.74e-01]    [3.51e+01]
29000     [9.48e+01, 1.69e-01]    [9.18e+01, 1.69e-01]    [9.18e+01]
30000     [2.50e+01, 1.64e-01]    [2.40e+01, 1.64e-01]    [2.40e+01]

Best model at step 30000:
  train loss: 2.52e+01
  test loss: 2.41e+01
  test metric: [2.40e+01]

'train' took 17.119082 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Cross-validation iteration: 3
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building feed-forward neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\fnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.038156 s

'compile' took 0.175437 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss              Test loss               Test metric
0         [6.38e+03, 5.92e-01]    [6.50e+03, 5.92e-01]    [6.50e+03]
1000      [1.36e+02, 2.98e-01]    [1.33e+02, 2.98e-01]    [1.33e+02]
2000      [4.83e+01, 2.83e-01]    [4.42e+01, 2.83e-01]    [4.42e+01]
3000      [8.84e+01, 2.72e-01]    [8.52e+01, 2.72e-01]    [8.52e+01]
4000      [8.03e+01, 2.63e-01]    [7.70e+01, 2.63e-01]    [7.70e+01]
5000      [7.56e+01, 2.55e-01]    [7.22e+01, 2.55e-01]    [7.22e+01]
6000      [7.18e+01, 2.48e-01]    [6.84e+01, 2.48e-01]    [6.84e+01]
7000      [6.64e+01, 2.42e-01]    [6.30e+01, 2.42e-01]    [6.30e+01]
8000      [5.95e+01, 2.36e-01]    [5.60e+01, 2.36e-01]    [5.60e+01]
9000      [5.17e+01, 2.31e-01]    [4.82e+01, 2.31e-01]    [4.82e+01]
10000     [4.37e+01, 2.27e-01]    [4.01e+01, 2.27e-01]    [4.01e+01]
11000     [3.44e+01, 2.21e-01]    [3.07e+01, 2.21e-01]    [3.07e+01]
12000     [5.58e+01, 2.16e-01]    [5.26e+01, 2.16e-01]    [5.26e+01]
13000     [4.80e+01, 2.10e-01]    [4.48e+01, 2.10e-01]    [4.48e+01]
14000     [4.45e+02, 2.03e-01]    [4.51e+02, 2.03e-01]    [4.51e+02]
15000     [6.14e+01, 1.97e-01]    [6.68e+01, 1.97e-01]    [6.68e+01]
16000     [1.18e+02, 1.91e-01]    [1.24e+02, 1.91e-01]    [1.24e+02]
17000     [1.19e+02, 1.86e-01]    [1.26e+02, 1.86e-01]    [1.26e+02]
18000     [1.01e+02, 1.81e-01]    [1.07e+02, 1.81e-01]    [1.07e+02]
19000     [1.33e+02, 1.78e-01]    [1.39e+02, 1.78e-01]    [1.39e+02]
20000     [1.15e+02, 1.75e-01]    [1.21e+02, 1.75e-01]    [1.21e+02]
21000     [9.80e+01, 1.72e-01]    [1.04e+02, 1.72e-01]    [1.04e+02]
22000     [8.30e+01, 1.69e-01]    [8.84e+01, 1.69e-01]    [8.84e+01]
23000     [1.06e+02, 1.66e-01]    [1.12e+02, 1.66e-01]    [1.12e+02]
24000     [8.54e+01, 1.64e-01]    [9.07e+01, 1.64e-01]    [9.07e+01]
25000     [9.28e+01, 1.61e-01]    [9.82e+01, 1.61e-01]    [9.82e+01]
26000     [7.23e+01, 1.59e-01]    [7.85e+01, 1.59e-01]    [7.85e+01]
27000     [3.68e+01, 1.57e-01]    [4.09e+01, 1.57e-01]    [4.09e+01]
28000     [6.59e+01, 1.54e-01]    [7.00e+01, 1.54e-01]    [7.00e+01]
29000     [5.37e+01, 1.52e-01]    [5.74e+01, 1.52e-01]    [5.74e+01]
30000     [4.95e+01, 1.50e-01]    [5.31e+01, 1.50e-01]    [5.31e+01]

Best model at step 11000:
  train loss: 3.47e+01
  test loss: 3.09e+01
  test metric: [3.07e+01]

'train' took 14.905568 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Cross-validation iteration: 4
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building feed-forward neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\fnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.036869 s

'compile' took 0.178761 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss              Test loss               Test metric
0         [4.26e+04, 6.29e-01]    [4.21e+04, 6.29e-01]    [4.21e+04]
1000      [1.35e+02, 4.23e-01]    [1.30e+02, 4.23e-01]    [1.30e+02]
2000      [1.53e+02, 3.97e-01]    [1.48e+02, 3.97e-01]    [1.48e+02]
3000      [3.22e+02, 3.77e-01]    [3.19e+02, 3.77e-01]    [3.19e+02]
4000      [1.14e+02, 3.58e-01]    [1.11e+02, 3.58e-01]    [1.11e+02]
5000      [1.04e+02, 3.41e-01]    [1.01e+02, 3.41e-01]    [1.01e+02]
6000      [2.71e+02, 3.27e-01]    [2.68e+02, 3.27e-01]    [2.68e+02]
7000      [6.29e+01, 3.13e-01]    [6.14e+01, 3.13e-01]    [6.14e+01]
8000      [1.38e+02, 3.01e-01]    [1.36e+02, 3.01e-01]    [1.36e+02]
9000      [2.24e+02, 2.90e-01]    [2.21e+02, 2.90e-01]    [2.21e+02]
10000     [6.22e+01, 2.79e-01]    [6.13e+01, 2.79e-01]    [6.13e+01]
11000     [8.79e+01, 2.69e-01]    [8.68e+01, 2.69e-01]    [8.68e+01]
12000     [2.16e+02, 2.59e-01]    [2.13e+02, 2.59e-01]    [2.13e+02]
13000     [5.26e+01, 2.49e-01]    [5.22e+01, 2.49e-01]    [5.22e+01]
14000     [8.30e+01, 2.39e-01]    [8.23e+01, 2.39e-01]    [8.23e+01]
15000     [1.74e+02, 2.30e-01]    [1.72e+02, 2.30e-01]    [1.72e+02]
16000     [7.66e+01, 2.22e-01]    [7.61e+01, 2.22e-01]    [7.61e+01]
17000     [7.49e+01, 2.14e-01]    [7.45e+01, 2.14e-01]    [7.45e+01]
18000     [1.30e+02, 2.07e-01]    [1.28e+02, 2.07e-01]    [1.28e+02]
19000     [6.01e+01, 2.00e-01]    [6.00e+01, 2.00e-01]    [6.00e+01]
20000     [6.42e+01, 1.94e-01]    [6.43e+01, 1.94e-01]    [6.43e+01]
21000     [1.12e+02, 1.88e-01]    [1.09e+02, 1.88e-01]    [1.09e+02]
22000     [4.31e+01, 1.83e-01]    [4.37e+01, 1.83e-01]    [4.37e+01]
23000     [5.83e+01, 1.79e-01]    [5.90e+01, 1.79e-01]    [5.90e+01]
24000     [9.57e+01, 1.75e-01]    [9.27e+01, 1.75e-01]    [9.27e+01]
25000     [5.19e+01, 1.71e-01]    [5.36e+01, 1.71e-01]    [5.36e+01]
26000     [5.87e+01, 1.67e-01]    [6.02e+01, 1.67e-01]    [6.02e+01]
27000     [8.67e+01, 1.64e-01]    [8.32e+01, 1.64e-01]    [8.32e+01]
28000     [2.96e+01, 1.60e-01]    [3.16e+01, 1.60e-01]    [3.16e+01]
29000     [4.77e+01, 1.57e-01]    [4.95e+01, 1.57e-01]    [4.95e+01]
30000     [6.83e+01, 1.53e-01]    [6.49e+01, 1.53e-01]    [6.49e+01]

Best model at step 28000:
  train loss: 2.98e+01
  test loss: 3.17e+01
  test metric: [3.16e+01]

'train' took 15.917157 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Cross-validation iteration: 5
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building feed-forward neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\fnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.041823 s

'compile' took 0.188166 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss              Test loss               Test metric
0         [2.93e+04, 6.39e-01]    [2.90e+04, 6.39e-01]    [2.90e+04]
1000      [2.13e+02, 2.89e-01]    [2.09e+02, 2.89e-01]    [2.09e+02]
2000      [2.49e+02, 2.65e-01]    [2.48e+02, 2.65e-01]    [2.48e+02]
3000      [5.24e+01, 2.50e-01]    [5.11e+01, 2.50e-01]    [5.11e+01]
4000      [8.69e+01, 2.38e-01]    [8.57e+01, 2.38e-01]    [8.57e+01]
5000      [1.41e+02, 2.29e-01]    [1.40e+02, 2.29e-01]    [1.40e+02]
6000      [8.83e+01, 2.21e-01]    [8.72e+01, 2.21e-01]    [8.72e+01]
7000      [1.07e+02, 2.14e-01]    [1.06e+02, 2.14e-01]    [1.06e+02]
8000      [1.40e+02, 2.08e-01]    [1.39e+02, 2.08e-01]    [1.39e+02]
9000      [9.92e+01, 2.02e-01]    [9.83e+01, 2.02e-01]    [9.83e+01]
10000     [1.31e+02, 1.97e-01]    [1.30e+02, 1.97e-01]    [1.30e+02]
11000     [1.12e+02, 1.93e-01]    [1.12e+02, 1.93e-01]    [1.12e+02]
12000     [6.23e+01, 1.88e-01]    [6.19e+01, 1.88e-01]    [6.19e+01]
13000     [9.28e+01, 1.84e-01]    [9.24e+01, 1.84e-01]    [9.24e+01]
14000     [1.26e+02, 1.80e-01]    [1.24e+02, 1.80e-01]    [1.24e+02]
15000     [6.62e+01, 1.75e-01]    [6.63e+01, 1.75e-01]    [6.63e+01]
16000     [7.07e+01, 1.70e-01]    [7.09e+01, 1.70e-01]    [7.09e+01]
17000     [1.01e+02, 1.65e-01]    [9.95e+01, 1.65e-01]    [9.95e+01]
18000     [5.97e+01, 1.59e-01]    [6.03e+01, 1.59e-01]    [6.03e+01]
19000     [8.27e+01, 1.55e-01]    [8.34e+01, 1.55e-01]    [8.34e+01]
20000     [9.16e+01, 1.50e-01]    [8.94e+01, 1.50e-01]    [8.94e+01]
21000     [6.08e+01, 1.46e-01]    [6.21e+01, 1.46e-01]    [6.21e+01]
22000     [7.00e+01, 1.42e-01]    [7.14e+01, 1.42e-01]    [7.14e+01]
23000     [7.04e+01, 1.39e-01]    [6.77e+01, 1.39e-01]    [6.77e+01]
24000     [4.59e+01, 1.35e-01]    [4.79e+01, 1.35e-01]    [4.79e+01]
25000     [6.36e+01, 1.32e-01]    [6.55e+01, 1.32e-01]    [6.55e+01]
26000     [6.32e+01, 1.28e-01]    [6.05e+01, 1.28e-01]    [6.05e+01]
27000     [4.26e+01, 1.23e-01]    [4.44e+01, 1.23e-01]    [4.44e+01]
28000     [5.19e+01, 1.19e-01]    [5.34e+01, 1.19e-01]    [5.34e+01]
29000     [5.88e+01, 1.15e-01]    [5.65e+01, 1.15e-01]    [5.65e+01]
30000     [3.24e+01, 1.11e-01]    [3.39e+01, 1.11e-01]    [3.39e+01]

Best model at step 30000:
  train loss: 3.26e+01
  test loss: 3.40e+01
  test metric: [3.39e+01]

'train' took 15.593100 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Cross-validation iteration: 6
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building feed-forward neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\fnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.037951 s

'compile' took 0.185386 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss              Test loss               Test metric
0         [7.58e+03, 6.17e-01]    [7.77e+03, 6.17e-01]    [7.77e+03]
1000      [8.20e+02, 3.05e-01]    [8.38e+02, 3.05e-01]    [8.38e+02]
2000      [6.20e+02, 2.86e-01]    [6.33e+02, 2.86e-01]    [6.33e+02]
3000      [5.75e+02, 2.73e-01]    [5.88e+02, 2.73e-01]    [5.88e+02]
4000      [5.20e+02, 2.62e-01]    [5.32e+02, 2.62e-01]    [5.32e+02]
5000      [5.82e+02, 2.53e-01]    [5.96e+02, 2.53e-01]    [5.96e+02]
6000      [5.17e+02, 2.43e-01]    [5.29e+02, 2.43e-01]    [5.29e+02]
7000      [5.15e+02, 2.34e-01]    [5.28e+02, 2.34e-01]    [5.28e+02]
8000      [4.46e+02, 2.26e-01]    [4.58e+02, 2.26e-01]    [4.58e+02]
9000      [3.10e+02, 2.17e-01]    [3.18e+02, 2.17e-01]    [3.18e+02]
10000     [3.15e+02, 2.09e-01]    [3.24e+02, 2.09e-01]    [3.24e+02]
11000     [6.25e+01, 2.01e-01]    [6.25e+01, 2.01e-01]    [6.25e+01]
12000     [5.82e+01, 1.91e-01]    [5.82e+01, 1.91e-01]    [5.82e+01]
13000     [5.39e+01, 1.83e-01]    [5.39e+01, 1.83e-01]    [5.39e+01]
14000     [4.78e+01, 1.75e-01]    [4.76e+01, 1.75e-01]    [4.76e+01]
15000     [4.63e+01, 1.69e-01]    [4.60e+01, 1.69e-01]    [4.60e+01]
16000     [4.24e+01, 1.62e-01]    [4.20e+01, 1.62e-01]    [4.20e+01]
17000     [5.18e+01, 1.56e-01]    [5.19e+01, 1.56e-01]    [5.19e+01]
18000     [4.51e+01, 1.49e-01]    [4.49e+01, 1.49e-01]    [4.49e+01]
19000     [3.92e+01, 1.43e-01]    [3.88e+01, 1.43e-01]    [3.88e+01]
20000     [3.72e+01, 1.38e-01]    [3.68e+01, 1.38e-01]    [3.68e+01]
21000     [8.08e+01, 1.36e-01]    [8.16e+01, 1.36e-01]    [8.16e+01]
22000     [1.33e+01, 1.32e-01]    [1.48e+01, 1.32e-01]    [1.48e+01]
23000     [2.95e+01, 1.29e-01]    [3.14e+01, 1.29e-01]    [3.14e+01]
24000     [7.90e+01, 1.26e-01]    [8.00e+01, 1.26e-01]    [8.00e+01]
25000     [1.69e+01, 1.22e-01]    [1.82e+01, 1.22e-01]    [1.82e+01]
26000     [3.35e+01, 1.18e-01]    [3.52e+01, 1.18e-01]    [3.52e+01]
27000     [5.00e+01, 1.15e-01]    [5.05e+01, 1.15e-01]    [5.05e+01]
28000     [1.64e+01, 1.12e-01]    [1.75e+01, 1.12e-01]    [1.75e+01]
29000     [2.76e+01, 1.09e-01]    [2.89e+01, 1.09e-01]    [2.89e+01]
30000     [4.20e+01, 1.06e-01]    [4.26e+01, 1.06e-01]    [4.26e+01]

Best model at step 22000:
  train loss: 1.34e+01
  test loss: 1.50e+01
  test metric: [1.48e+01]

'train' took 15.869311 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Cross-validation iteration: 7
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building feed-forward neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\fnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.051777 s

'compile' took 0.230251 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss              Test loss               Test metric
0         [5.30e+02, 6.81e-01]    [5.42e+02, 6.81e-01]    [5.42e+02]
1000      [1.29e+02, 3.60e-01]    [1.39e+02, 3.60e-01]    [1.39e+02]
2000      [6.49e+01, 3.42e-01]    [7.47e+01, 3.42e-01]    [7.47e+01]
3000      [1.16e+02, 3.32e-01]    [1.26e+02, 3.32e-01]    [1.26e+02]
4000      [8.67e+01, 3.25e-01]    [9.67e+01, 3.25e-01]    [9.67e+01]
5000      [1.14e+02, 3.19e-01]    [1.24e+02, 3.19e-01]    [1.24e+02]
6000      [2.81e+01, 3.14e-01]    [3.77e+01, 3.14e-01]    [3.77e+01]
7000      [6.03e+01, 3.10e-01]    [6.99e+01, 3.10e-01]    [6.99e+01]
8000      [2.83e+01, 3.05e-01]    [3.75e+01, 3.05e-01]    [3.75e+01]
9000      [3.41e+01, 3.00e-01]    [4.30e+01, 3.00e-01]    [4.30e+01]
10000     [4.21e+01, 2.94e-01]    [5.18e+01, 2.94e-01]    [5.18e+01]
11000     [3.50e+01, 2.89e-01]    [4.45e+01, 2.89e-01]    [4.45e+01]
12000     [1.80e+01, 2.83e-01]    [2.72e+01, 2.83e-01]    [2.72e+01]
13000     [6.61e+01, 2.77e-01]    [7.54e+01, 2.77e-01]    [7.54e+01]
14000     [3.52e+01, 2.71e-01]    [4.41e+01, 2.71e-01]    [4.41e+01]
15000     [1.87e+01, 2.65e-01]    [2.75e+01, 2.65e-01]    [2.75e+01]
16000     [3.05e+01, 2.58e-01]    [3.91e+01, 2.58e-01]    [3.91e+01]
17000     [4.37e+01, 2.51e-01]    [5.19e+01, 2.51e-01]    [5.19e+01]
18000     [2.90e+01, 2.44e-01]    [3.66e+01, 2.44e-01]    [3.66e+01]
19000     [3.98e+01, 2.37e-01]    [4.65e+01, 2.37e-01]    [4.65e+01]
20000     [7.94e+01, 2.30e-01]    [8.53e+01, 2.30e-01]    [8.53e+01]
21000     [6.03e+01, 2.22e-01]    [6.03e+01, 2.22e-01]    [6.03e+01]
22000     [6.91e+01, 2.16e-01]    [6.97e+01, 2.16e-01]    [6.97e+01]
23000     [4.75e+01, 2.11e-01]    [4.45e+01, 2.11e-01]    [4.45e+01]
24000     [4.87e+01, 2.05e-01]    [4.59e+01, 2.05e-01]    [4.59e+01]
25000     [6.52e+01, 1.99e-01]    [6.27e+01, 1.99e-01]    [6.27e+01]
26000     [5.71e+01, 1.93e-01]    [5.48e+01, 1.93e-01]    [5.48e+01]
27000     [6.03e+01, 1.87e-01]    [5.83e+01, 1.87e-01]    [5.83e+01]
28000     [5.93e+01, 1.81e-01]    [5.74e+01, 1.81e-01]    [5.74e+01]
29000     [4.01e+01, 1.76e-01]    [3.81e+01, 1.76e-01]    [3.81e+01]
30000     [2.89e+01, 1.71e-01]    [2.69e+01, 1.71e-01]    [2.69e+01]

Best model at step 12000:
  train loss: 1.83e+01
  test loss: 2.75e+01
  test metric: [2.72e+01]

'train' took 16.685762 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Cross-validation iteration: 8
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building feed-forward neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\fnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.037895 s

'compile' took 0.181204 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss              Test loss               Test metric
0         [5.66e+04, 6.49e-01]    [5.50e+04, 6.49e-01]    [5.50e+04]
1000      [2.05e+02, 3.80e-01]    [1.94e+02, 3.80e-01]    [1.94e+02]
2000      [1.60e+02, 3.53e-01]    [1.51e+02, 3.53e-01]    [1.51e+02]
3000      [2.12e+02, 3.34e-01]    [2.01e+02, 3.34e-01]    [2.01e+02]
4000      [2.24e+02, 3.19e-01]    [2.13e+02, 3.19e-01]    [2.13e+02]
5000      [2.19e+02, 3.06e-01]    [2.09e+02, 3.06e-01]    [2.09e+02]
6000      [2.18e+02, 2.96e-01]    [2.07e+02, 2.96e-01]    [2.07e+02]
7000      [1.90e+02, 2.87e-01]    [1.81e+02, 2.87e-01]    [1.81e+02]
8000      [1.67e+02, 2.80e-01]    [1.58e+02, 2.80e-01]    [1.58e+02]
9000      [1.32e+02, 2.75e-01]    [1.24e+02, 2.75e-01]    [1.24e+02]
10000     [1.90e+02, 2.69e-01]    [1.80e+02, 2.69e-01]    [1.80e+02]
11000     [1.46e+02, 2.64e-01]    [1.39e+02, 2.64e-01]    [1.39e+02]
12000     [1.22e+02, 2.59e-01]    [1.15e+02, 2.59e-01]    [1.15e+02]
13000     [1.67e+02, 2.55e-01]    [1.60e+02, 2.55e-01]    [1.60e+02]
14000     [1.58e+02, 2.50e-01]    [1.51e+02, 2.50e-01]    [1.51e+02]
15000     [1.29e+02, 2.45e-01]    [1.23e+02, 2.45e-01]    [1.23e+02]
16000     [1.79e+02, 2.40e-01]    [1.71e+02, 2.40e-01]    [1.71e+02]
17000     [1.28e+02, 2.35e-01]    [1.22e+02, 2.35e-01]    [1.22e+02]
18000     [1.54e+02, 2.29e-01]    [1.47e+02, 2.29e-01]    [1.47e+02]
19000     [1.71e+02, 2.23e-01]    [1.64e+02, 2.23e-01]    [1.64e+02]
20000     [9.65e+01, 2.16e-01]    [9.15e+01, 2.16e-01]    [9.15e+01]
21000     [1.03e+02, 2.09e-01]    [9.76e+01, 2.09e-01]    [9.76e+01]
22000     [1.37e+02, 2.03e-01]    [1.31e+02, 2.03e-01]    [1.31e+02]
23000     [1.58e+02, 1.97e-01]    [1.51e+02, 1.97e-01]    [1.51e+02]
24000     [9.33e+01, 1.92e-01]    [8.83e+01, 1.92e-01]    [8.83e+01]
25000     [1.07e+02, 1.87e-01]    [1.02e+02, 1.87e-01]    [1.02e+02]
26000     [1.20e+02, 1.83e-01]    [1.14e+02, 1.83e-01]    [1.14e+02]
27000     [1.32e+02, 1.79e-01]    [1.26e+02, 1.79e-01]    [1.26e+02]
28000     [1.44e+02, 1.76e-01]    [1.37e+02, 1.76e-01]    [1.37e+02]
29000     [8.15e+01, 1.73e-01]    [7.66e+01, 1.73e-01]    [7.66e+01]
30000     [9.04e+01, 1.70e-01]    [8.53e+01, 1.70e-01]    [8.53e+01]

Best model at step 29000:
  train loss: 8.16e+01
  test loss: 7.68e+01
  test metric: [7.66e+01]

'train' took 17.417860 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Cross-validation iteration: 9
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building feed-forward neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\fnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.035971 s

'compile' took 0.173513 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss              Test loss               Test metric
0         [7.67e+02, 6.14e-01]    [7.94e+02, 6.14e-01]    [7.94e+02]
1000      [7.35e+01, 3.45e-01]    [7.86e+01, 3.45e-01]    [7.86e+01]
2000      [7.05e+01, 3.23e-01]    [7.53e+01, 3.23e-01]    [7.53e+01]
3000      [7.11e+01, 3.05e-01]    [7.59e+01, 3.05e-01]    [7.59e+01]
4000      [6.51e+01, 2.90e-01]    [6.95e+01, 2.90e-01]    [6.95e+01]
5000      [6.28e+01, 2.77e-01]    [6.71e+01, 2.77e-01]    [6.71e+01]
6000      [9.13e+01, 2.68e-01]    [9.63e+01, 2.68e-01]    [9.63e+01]
7000      [9.23e+01, 2.60e-01]    [9.73e+01, 2.60e-01]    [9.73e+01]
8000      [8.58e+01, 2.53e-01]    [9.05e+01, 2.53e-01]    [9.05e+01]
9000      [7.48e+01, 2.47e-01]    [7.91e+01, 2.47e-01]    [7.91e+01]
10000     [4.78e+01, 2.41e-01]    [4.76e+01, 2.41e-01]    [4.76e+01]
11000     [4.93e+01, 2.35e-01]    [4.91e+01, 2.35e-01]    [4.91e+01]
12000     [5.87e+01, 2.29e-01]    [5.87e+01, 2.29e-01]    [5.87e+01]
13000     [5.41e+01, 2.23e-01]    [5.39e+01, 2.23e-01]    [5.39e+01]
14000     [5.10e+01, 2.17e-01]    [5.07e+01, 2.17e-01]    [5.07e+01]
15000     [5.11e+01, 2.11e-01]    [5.07e+01, 2.11e-01]    [5.07e+01]
16000     [4.89e+01, 2.05e-01]    [4.85e+01, 2.05e-01]    [4.85e+01]
17000     [4.99e+01, 1.98e-01]    [4.95e+01, 1.98e-01]    [4.95e+01]
18000     [9.15e+01, 1.93e-01]    [9.29e+01, 1.93e-01]    [9.29e+01]
19000     [1.24e+02, 1.88e-01]    [1.29e+02, 1.88e-01]    [1.29e+02]
20000     [6.55e+01, 1.83e-01]    [6.63e+01, 1.83e-01]    [6.63e+01]
21000     [7.19e+01, 1.78e-01]    [7.28e+01, 1.78e-01]    [7.28e+01]
22000     [6.89e+01, 1.73e-01]    [7.25e+01, 1.73e-01]    [7.25e+01]
23000     [5.07e+01, 1.69e-01]    [5.09e+01, 1.69e-01]    [5.09e+01]
24000     [6.31e+01, 1.66e-01]    [6.36e+01, 1.66e-01]    [6.36e+01]
25000     [6.13e+01, 1.63e-01]    [6.47e+01, 1.63e-01]    [6.47e+01]
26000     [4.19e+01, 1.59e-01]    [4.17e+01, 1.59e-01]    [4.17e+01]
27000     [4.89e+01, 1.56e-01]    [4.86e+01, 1.56e-01]    [4.86e+01]
28000     [5.37e+01, 1.52e-01]    [5.73e+01, 1.52e-01]    [5.73e+01]
29000     [3.05e+01, 1.48e-01]    [2.96e+01, 1.48e-01]    [2.96e+01]
30000     [3.62e+01, 1.44e-01]    [3.53e+01, 1.44e-01]    [3.53e+01]

Best model at step 29000:
  train loss: 3.07e+01
  test loss: 2.97e+01
  test metric: [2.96e+01]

'train' took 16.433881 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Cross-validation iteration: 10
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building feed-forward neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\fnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.035446 s

'compile' took 0.174629 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss              Test loss               Test metric
0         [4.56e+04, 5.92e-01]    [4.73e+04, 5.92e-01]    [4.73e+04]
1000      [1.72e+02, 2.81e-01]    [1.80e+02, 2.81e-01]    [1.80e+02]
2000      [3.70e+01, 2.52e-01]    [3.71e+01, 2.52e-01]    [3.71e+01]
3000      [6.13e+01, 2.35e-01]    [6.32e+01, 2.35e-01]    [6.32e+01]
4000      [1.51e+02, 2.24e-01]    [1.57e+02, 2.24e-01]    [1.57e+02]
5000      [2.51e+01, 2.16e-01]    [2.68e+01, 2.16e-01]    [2.68e+01]
6000      [4.60e+01, 2.11e-01]    [4.91e+01, 2.11e-01]    [4.91e+01]
7000      [1.22e+02, 2.07e-01]    [1.25e+02, 2.07e-01]    [1.25e+02]
8000      [1.70e+01, 2.03e-01]    [2.03e+01, 2.03e-01]    [2.03e+01]
9000      [4.13e+01, 1.99e-01]    [4.63e+01, 1.99e-01]    [4.63e+01]
10000     [1.06e+02, 1.96e-01]    [1.06e+02, 1.96e-01]    [1.06e+02]
11000     [3.20e+01, 1.92e-01]    [3.74e+01, 1.92e-01]    [3.74e+01]
12000     [3.94e+01, 1.88e-01]    [4.55e+01, 1.88e-01]    [4.55e+01]
13000     [9.19e+01, 1.84e-01]    [9.06e+01, 1.84e-01]    [9.06e+01]
14000     [2.37e+01, 1.79e-01]    [2.96e+01, 1.79e-01]    [2.96e+01]
15000     [3.52e+01, 1.73e-01]    [4.19e+01, 1.73e-01]    [4.19e+01]
16000     [7.54e+01, 1.67e-01]    [7.28e+01, 1.67e-01]    [7.28e+01]
17000     [7.98e+01, 1.66e-01]    [8.10e+01, 1.66e-01]    [8.10e+01]
18000     [1.98e+01, 1.57e-01]    [2.40e+01, 1.57e-01]    [2.40e+01]
19000     [3.25e+01, 1.48e-01]    [3.72e+01, 1.48e-01]    [3.72e+01]
20000     [5.90e+01, 1.40e-01]    [5.77e+01, 1.40e-01]    [5.77e+01]
21000     [2.00e+01, 1.31e-01]    [2.43e+01, 1.31e-01]    [2.43e+01]
22000     [1.32e+01, 1.24e-01]    [1.66e+01, 1.24e-01]    [1.66e+01]
23000     [1.81e+01, 1.18e-01]    [2.14e+01, 1.18e-01]    [2.14e+01]
24000     [1.78e+01, 1.12e-01]    [2.08e+01, 1.12e-01]    [2.08e+01]
25000     [1.91e+01, 1.06e-01]    [2.17e+01, 1.06e-01]    [2.17e+01]
26000     [1.44e+01, 1.01e-01]    [1.63e+01, 1.01e-01]    [1.63e+01]
27000     [3.17e+01, 9.73e-02]    [3.19e+01, 9.73e-02]    [3.19e+01]
28000     [4.29e+01, 9.30e-02]    [4.58e+01, 9.30e-02]    [4.58e+01]
29000     [1.84e+01, 8.90e-02]    [1.77e+01, 8.90e-02]    [1.77e+01]
30000     [2.91e+01, 8.55e-02]    [2.86e+01, 8.55e-02]    [2.86e+01]

Best model at step 22000:
  train loss: 1.33e+01
  test loss: 1.67e+01
  test metric: [1.66e+01]

'train' took 15.283119 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...
[27.495005, 23.965334, 30.704521, 31.576662, 33.855778, 14.8333235, 27.176214, 76.64637, 29.55126, 16.623674]
Estar 70 31.242817 16.242256
              Case          n     E (GPa)   sy (GPa)     C (GPa)    dP/dh (N/m)      Wp/Wt    E* (GPa)      sy/E*
count    97.000000  97.000000   97.000000  97.000000   97.000000      97.000000  97.000000   97.000000  97.000000
mean    279.030928   0.213917  107.163804   1.327507   92.953350  128652.067967   0.731227  102.813375   0.013835
std     411.446469   0.178797   67.175628   1.126477   66.232911   85768.846959   0.134844   60.541899   0.009753
min       1.000000   0.000000   10.000000   0.030000    2.731300    9801.900000   0.451835   10.880844   0.001399
25%      37.000000   0.100000   50.000000   0.300000   39.269000   53194.000000   0.628612   52.343315   0.005508
50%      67.000000   0.177243  100.806000   1.000000   80.813000  123181.000000   0.740598  100.685905   0.011463
75%      91.000000   0.300000  170.000000   2.000000  144.840000  194391.000000   0.830543  159.806250   0.019105
max    1023.000000   0.500000  210.000000   5.298021  261.330000  336838.000000   0.971835  190.913667   0.038209
              Case          n     E (GPa)   sy (GPa)     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  14.000000   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499   1.436338   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848   1.652156   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000   0.036792    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500   0.303588   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000   0.985823   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000   1.719617   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000   5.298021  239.235773  326727.270700   0.971982

Cross-validation iteration: 1
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.086807 s

'compile' took 0.377343 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.02e+02, 1.02e+02, 2.76e+00]    [0.00e+00, 9.98e+01, 2.76e+00]    [9.98e+01, 9.98e+01, 7.85e-01, 7.85e-01]
1000      [7.84e+01, 6.24e+01, 2.86e+00]    [0.00e+00, 9.50e+01, 2.86e+00]    [9.50e+01, 9.50e+01, 5.96e+00, 6.00e+00]
2000      [4.59e+01, 2.26e+01, 3.66e+00]    [0.00e+00, 6.87e+01, 3.66e+00]    [6.98e+01, 6.87e+01, 2.42e+01, 2.50e+01]
3000      [2.83e+01, 9.25e+00, 3.95e+00]    [0.00e+00, 4.57e+01, 3.95e+00]    [4.72e+01, 4.57e+01, 2.91e+01, 2.99e+01]
4000      [1.52e+01, 3.98e+00, 4.14e+00]    [0.00e+00, 2.71e+01, 4.14e+00]    [2.74e+01, 2.71e+01, 2.36e+01, 2.34e+01]
5000      [9.25e+00, 2.80e+00, 4.22e+00]    [0.00e+00, 1.51e+01, 4.22e+00]    [1.51e+01, 1.51e+01, 1.19e+01, 1.17e+01]
6000      [6.63e+00, 1.81e+00, 4.20e+00]    [0.00e+00, 9.73e+00, 4.20e+00]    [9.73e+00, 9.73e+00, 5.09e+00, 5.09e+00]
7000      [5.41e+00, 1.21e+00, 4.19e+00]    [0.00e+00, 6.66e+00, 4.19e+00]    [6.63e+00, 6.66e+00, 2.15e+00, 2.21e+00]
8000      [4.88e+00, 6.61e-01, 4.18e+00]    [0.00e+00, 5.13e+00, 4.18e+00]    [5.07e+00, 5.13e+00, 2.62e+00, 2.41e+00]
9000      [4.60e+00, 8.11e-02, 4.16e+00]    [0.00e+00, 4.61e+00, 4.16e+00]    [4.53e+00, 4.61e+00, 3.01e+00, 2.70e+00]
10000     [4.40e+00, 6.74e-02, 4.12e+00]    [0.00e+00, 4.59e+00, 4.12e+00]    [4.50e+00, 4.59e+00, 2.95e+00, 2.59e+00]
11000     [4.20e+00, 8.50e-02, 4.09e+00]    [0.00e+00, 4.56e+00, 4.09e+00]    [4.45e+00, 4.56e+00, 2.95e+00, 2.51e+00]
12000     [4.07e+00, 5.61e-02, 4.06e+00]    [0.00e+00, 4.53e+00, 4.06e+00]    [4.41e+00, 4.53e+00, 2.99e+00, 2.46e+00]
13000     [3.96e+00, 6.75e-02, 4.03e+00]    [0.00e+00, 4.51e+00, 4.03e+00]    [4.38e+00, 4.51e+00, 3.01e+00, 2.41e+00]
14000     [3.86e+00, 8.54e-02, 4.00e+00]    [0.00e+00, 4.46e+00, 4.00e+00]    [4.31e+00, 4.46e+00, 3.02e+00, 2.37e+00]
15000     [3.75e+00, 7.06e-02, 3.98e+00]    [0.00e+00, 4.42e+00, 3.98e+00]    [4.26e+00, 4.42e+00, 3.01e+00, 2.31e+00]
16000     [3.69e+00, 1.80e-01, 3.95e+00]    [0.00e+00, 4.40e+00, 3.95e+00]    [4.23e+00, 4.40e+00, 2.96e+00, 2.24e+00]
17000     [3.59e+00, 6.86e-02, 3.92e+00]    [0.00e+00, 4.28e+00, 3.92e+00]    [4.11e+00, 4.28e+00, 2.99e+00, 2.26e+00]
18000     [3.51e+00, 5.14e-02, 3.90e+00]    [0.00e+00, 4.19e+00, 3.90e+00]    [4.02e+00, 4.19e+00, 3.01e+00, 2.25e+00]
19000     [3.43e+00, 3.64e-02, 3.88e+00]    [0.00e+00, 4.14e+00, 3.88e+00]    [3.95e+00, 4.14e+00, 3.03e+00, 2.24e+00]
20000     [3.37e+00, 2.56e-02, 3.85e+00]    [0.00e+00, 4.08e+00, 3.85e+00]    [3.89e+00, 4.08e+00, 3.05e+00, 2.24e+00]
21000     [3.28e+00, 3.15e-02, 3.83e+00]    [0.00e+00, 4.03e+00, 3.83e+00]    [3.83e+00, 4.03e+00, 3.05e+00, 2.23e+00]
22000     [3.23e+00, 6.25e-02, 3.81e+00]    [0.00e+00, 3.97e+00, 3.81e+00]    [3.77e+00, 3.97e+00, 3.04e+00, 2.21e+00]
23000     [3.18e+00, 1.31e-01, 3.79e+00]    [0.00e+00, 3.97e+00, 3.79e+00]    [3.77e+00, 3.97e+00, 3.00e+00, 2.16e+00]
24000     [3.11e+00, 2.48e-02, 3.77e+00]    [0.00e+00, 3.97e+00, 3.77e+00]    [3.76e+00, 3.97e+00, 2.96e+00, 2.10e+00]
25000     [3.15e+00, 1.62e-01, 3.75e+00]    [0.00e+00, 3.98e+00, 3.75e+00]    [3.76e+00, 3.98e+00, 2.91e+00, 2.03e+00]
26000     [3.07e+00, 7.95e-02, 3.73e+00]    [0.00e+00, 3.95e+00, 3.73e+00]    [3.73e+00, 3.95e+00, 2.86e+00, 1.99e+00]
27000     [3.06e+00, 8.44e-02, 3.71e+00]    [0.00e+00, 3.95e+00, 3.71e+00]    [3.72e+00, 3.95e+00, 2.82e+00, 1.95e+00]
28000     [3.01e+00, 3.18e-02, 3.69e+00]    [0.00e+00, 3.95e+00, 3.69e+00]    [3.72e+00, 3.95e+00, 2.75e+00, 1.88e+00]
29000     [3.05e+00, 1.45e-01, 3.67e+00]    [0.00e+00, 3.94e+00, 3.67e+00]    [3.70e+00, 3.94e+00, 2.66e+00, 1.83e+00]
30000     [2.99e+00, 1.08e-01, 3.65e+00]    [0.00e+00, 3.92e+00, 3.65e+00]    [3.67e+00, 3.92e+00, 2.61e+00, 1.75e+00]

Best model at step 28000:
  train loss: 6.73e+00
  test loss: 7.64e+00
  test metric: [3.72e+00, 3.95e+00, 2.75e+00, 1.88e+00]

'train' took 36.003104 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Cross-validation iteration: 2
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.080771 s

'compile' took 0.384039 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [9.98e+01, 9.99e+01, 2.85e+00]    [0.00e+00, 9.75e+01, 2.85e+00]    [9.75e+01, 9.75e+01, 2.38e+00, 2.38e+00]
1000      [7.44e+01, 7.13e+01, 2.98e+00]    [0.00e+00, 8.45e+01, 2.98e+00]    [8.46e+01, 8.45e+01, 2.03e+01, 2.04e+01]
2000      [4.21e+01, 3.26e+01, 3.80e+00]    [0.00e+00, 7.03e+01, 3.80e+00]    [7.12e+01, 7.03e+01, 3.00e+01, 3.08e+01]
3000      [2.33e+01, 1.28e+01, 4.15e+00]    [0.00e+00, 4.76e+01, 4.15e+00]    [4.89e+01, 4.76e+01, 2.63e+01, 2.69e+01]
4000      [1.17e+01, 2.44e+00, 4.36e+00]    [0.00e+00, 2.01e+01, 4.36e+00]    [2.13e+01, 2.01e+01, 1.56e+01, 1.58e+01]
5000      [8.06e+00, 2.10e+00, 4.30e+00]    [0.00e+00, 1.30e+01, 4.30e+00]    [1.36e+01, 1.30e+01, 1.06e+01, 1.06e+01]
6000      [5.90e+00, 1.74e+00, 4.25e+00]    [0.00e+00, 9.53e+00, 4.25e+00]    [9.55e+00, 9.53e+00, 7.34e+00, 7.31e+00]
7000      [4.95e+00, 1.69e+00, 4.22e+00]    [0.00e+00, 7.10e+00, 4.22e+00]    [6.83e+00, 7.10e+00, 5.25e+00, 5.26e+00]
8000      [4.15e+00, 1.55e+00, 4.19e+00]    [0.00e+00, 5.36e+00, 4.19e+00]    [4.64e+00, 5.36e+00, 4.10e+00, 4.06e+00]
9000      [3.79e+00, 1.43e+00, 4.16e+00]    [0.00e+00, 4.56e+00, 4.16e+00]    [3.42e+00, 4.56e+00, 3.72e+00, 3.67e+00]
10000     [3.54e+00, 1.22e+00, 4.13e+00]    [0.00e+00, 4.39e+00, 4.13e+00]    [2.93e+00, 4.39e+00, 3.12e+00, 3.17e+00]
11000     [3.33e+00, 1.09e+00, 4.10e+00]    [0.00e+00, 4.63e+00, 4.10e+00]    [2.82e+00, 4.63e+00, 2.59e+00, 2.76e+00]
12000     [3.19e+00, 9.67e-01, 4.07e+00]    [0.00e+00, 4.67e+00, 4.07e+00]    [2.58e+00, 4.67e+00, 2.20e+00, 2.46e+00]
13000     [3.06e+00, 8.25e-01, 4.05e+00]    [0.00e+00, 4.96e+00, 4.05e+00]    [2.65e+00, 4.96e+00, 2.10e+00, 2.40e+00]
14000     [3.04e+00, 7.68e-01, 4.03e+00]    [0.00e+00, 4.71e+00, 4.03e+00]    [2.40e+00, 4.71e+00, 1.93e+00, 2.31e+00]
15000     [2.94e+00, 6.63e-01, 4.01e+00]    [0.00e+00, 4.85e+00, 4.01e+00]    [2.57e+00, 4.85e+00, 1.99e+00, 2.45e+00]
16000     [2.87e+00, 5.13e-01, 3.99e+00]    [0.00e+00, 4.69e+00, 3.99e+00]    [2.46e+00, 4.69e+00, 1.96e+00, 2.49e+00]
17000     [2.86e+00, 5.34e-01, 3.97e+00]    [0.00e+00, 4.40e+00, 3.97e+00]    [2.26e+00, 4.40e+00, 1.86e+00, 2.47e+00]
18000     [2.77e+00, 3.74e-01, 3.95e+00]    [0.00e+00, 4.37e+00, 3.95e+00]    [2.31e+00, 4.37e+00, 1.80e+00, 2.55e+00]
19000     [2.71e+00, 2.89e-01, 3.92e+00]    [0.00e+00, 4.41e+00, 3.92e+00]    [2.32e+00, 4.41e+00, 1.79e+00, 2.56e+00]
20000     [2.76e+00, 2.92e-01, 3.89e+00]    [0.00e+00, 4.26e+00, 3.89e+00]    [2.22e+00, 4.26e+00, 1.79e+00, 2.54e+00]
21000     [2.71e+00, 1.75e-01, 3.87e+00]    [0.00e+00, 4.28e+00, 3.87e+00]    [2.31e+00, 4.28e+00, 1.70e+00, 2.55e+00]
22000     [2.74e+00, 1.96e-01, 3.84e+00]    [0.00e+00, 4.27e+00, 3.84e+00]    [2.34e+00, 4.27e+00, 1.65e+00, 2.54e+00]
23000     [2.75e+00, 2.16e-01, 3.81e+00]    [0.00e+00, 4.13e+00, 3.81e+00]    [2.09e+00, 4.13e+00, 1.87e+00, 2.54e+00]
24000     [2.66e+00, 1.11e-01, 3.78e+00]    [0.00e+00, 4.21e+00, 3.78e+00]    [2.05e+00, 4.21e+00, 1.92e+00, 2.52e+00]
25000     [2.64e+00, 1.07e-01, 3.75e+00]    [0.00e+00, 4.36e+00, 3.75e+00]    [2.12e+00, 4.36e+00, 1.94e+00, 2.51e+00]
26000     [2.62e+00, 1.20e-01, 3.73e+00]    [0.00e+00, 4.32e+00, 3.73e+00]    [1.99e+00, 4.32e+00, 2.01e+00, 2.49e+00]
27000     [2.67e+00, 1.47e-01, 3.70e+00]    [0.00e+00, 4.12e+00, 3.70e+00]    [1.74e+00, 4.12e+00, 2.17e+00, 2.52e+00]
28000     [2.58e+00, 3.56e-02, 3.68e+00]    [0.00e+00, 4.22e+00, 3.68e+00]    [1.83e+00, 4.22e+00, 2.10e+00, 2.47e+00]
29000     [2.58e+00, 3.31e-02, 3.66e+00]    [0.00e+00, 4.09e+00, 3.66e+00]    [1.67e+00, 4.09e+00, 2.15e+00, 2.47e+00]
30000     [2.57e+00, 4.51e-02, 3.64e+00]    [0.00e+00, 4.15e+00, 3.64e+00]    [1.73e+00, 4.15e+00, 2.10e+00, 2.45e+00]

Best model at step 30000:
  train loss: 6.25e+00
  test loss: 7.79e+00
  test metric: [1.73e+00, 4.15e+00, 2.10e+00, 2.45e+00]

'train' took 37.162608 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Cross-validation iteration: 3
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.077707 s

'compile' took 0.379224 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [9.93e+01, 9.97e+01, 2.82e+00]    [0.00e+00, 9.98e+01, 2.82e+00]    [9.98e+01, 9.98e+01, 4.04e-01, 4.04e-01]
1000      [7.80e+01, 5.95e+01, 2.94e+00]    [0.00e+00, 9.06e+01, 2.94e+00]    [9.07e+01, 9.06e+01, 7.14e+00, 7.20e+00]
2000      [5.24e+01, 3.84e+01, 3.58e+00]    [0.00e+00, 5.91e+01, 3.58e+00]    [6.04e+01, 5.91e+01, 1.21e+01, 1.25e+01]
3000      [9.98e+00, 2.93e+00, 4.51e+00]    [0.00e+00, 6.36e+00, 4.51e+00]    [9.69e+00, 6.36e+00, 6.91e+00, 4.69e+00]
4000      [5.54e+00, 1.97e+00, 4.52e+00]    [0.00e+00, 6.07e+00, 4.52e+00]    [6.58e+00, 6.07e+00, 3.68e+00, 4.17e+00]
5000      [4.49e+00, 1.45e+00, 4.48e+00]    [0.00e+00, 6.11e+00, 4.48e+00]    [6.42e+00, 6.11e+00, 2.68e+00, 3.86e+00]
6000      [4.17e+00, 1.13e+00, 4.42e+00]    [0.00e+00, 5.54e+00, 4.42e+00]    [5.73e+00, 5.54e+00, 2.88e+00, 3.56e+00]
7000      [3.92e+00, 9.69e-01, 4.37e+00]    [0.00e+00, 5.10e+00, 4.37e+00]    [5.21e+00, 5.10e+00, 3.01e+00, 3.40e+00]
8000      [3.72e+00, 8.12e-01, 4.32e+00]    [0.00e+00, 4.80e+00, 4.32e+00]    [4.87e+00, 4.80e+00, 3.11e+00, 3.33e+00]
9000      [3.65e+00, 7.19e-01, 4.26e+00]    [0.00e+00, 4.52e+00, 4.26e+00]    [4.55e+00, 4.52e+00, 2.97e+00, 3.07e+00]
10000     [3.53e+00, 6.92e-01, 4.22e+00]    [0.00e+00, 4.34e+00, 4.22e+00]    [4.34e+00, 4.34e+00, 2.92e+00, 2.91e+00]
11000     [3.52e+00, 7.59e-01, 4.17e+00]    [0.00e+00, 4.17e+00, 4.17e+00]    [4.15e+00, 4.17e+00, 2.83e+00, 2.77e+00]
12000     [3.46e+00, 5.34e-01, 4.12e+00]    [0.00e+00, 4.04e+00, 4.12e+00]    [3.99e+00, 4.04e+00, 2.76e+00, 2.64e+00]
13000     [3.43e+00, 6.20e-01, 4.08e+00]    [0.00e+00, 3.85e+00, 4.08e+00]    [3.78e+00, 3.85e+00, 2.63e+00, 2.48e+00]
14000     [3.38e+00, 4.38e-01, 4.04e+00]    [0.00e+00, 3.69e+00, 4.04e+00]    [3.62e+00, 3.69e+00, 2.54e+00, 2.39e+00]
15000     [3.35e+00, 4.03e-01, 4.01e+00]    [0.00e+00, 3.53e+00, 4.01e+00]    [3.50e+00, 3.53e+00, 2.41e+00, 2.33e+00]
16000     [3.36e+00, 5.33e-01, 3.97e+00]    [0.00e+00, 3.40e+00, 3.97e+00]    [3.43e+00, 3.40e+00, 2.29e+00, 2.32e+00]
17000     [3.31e+00, 4.00e-01, 3.94e+00]    [0.00e+00, 3.22e+00, 3.94e+00]    [3.31e+00, 3.22e+00, 2.19e+00, 2.33e+00]
18000     [3.28e+00, 3.41e-01, 3.91e+00]    [0.00e+00, 3.10e+00, 3.91e+00]    [3.25e+00, 3.10e+00, 2.11e+00, 2.35e+00]
19000     [3.28e+00, 3.58e-01, 3.88e+00]    [0.00e+00, 2.93e+00, 3.88e+00]    [3.15e+00, 2.93e+00, 2.02e+00, 2.39e+00]
20000     [3.26e+00, 2.89e-01, 3.86e+00]    [0.00e+00, 2.91e+00, 3.86e+00]    [3.17e+00, 2.91e+00, 2.03e+00, 2.42e+00]
21000     [3.22e+00, 2.05e-01, 3.83e+00]    [0.00e+00, 2.90e+00, 3.83e+00]    [3.18e+00, 2.90e+00, 2.01e+00, 2.41e+00]
22000     [3.21e+00, 1.76e-01, 3.81e+00]    [0.00e+00, 2.91e+00, 3.81e+00]    [3.20e+00, 2.91e+00, 2.02e+00, 2.40e+00]
23000     [3.19e+00, 1.63e-01, 3.79e+00]    [0.00e+00, 2.90e+00, 3.79e+00]    [3.21e+00, 2.90e+00, 2.03e+00, 2.40e+00]
24000     [3.19e+00, 1.05e-01, 3.77e+00]    [0.00e+00, 2.87e+00, 3.77e+00]    [3.20e+00, 2.87e+00, 2.05e+00, 2.41e+00]
25000     [3.20e+00, 1.74e-01, 3.75e+00]    [0.00e+00, 2.85e+00, 3.75e+00]    [3.26e+00, 2.85e+00, 2.02e+00, 2.45e+00]
26000     [3.16e+00, 5.77e-02, 3.73e+00]    [0.00e+00, 2.82e+00, 3.73e+00]    [3.26e+00, 2.82e+00, 1.96e+00, 2.44e+00]
27000     [3.15e+00, 9.10e-02, 3.71e+00]    [0.00e+00, 2.76e+00, 3.71e+00]    [3.24e+00, 2.76e+00, 1.89e+00, 2.41e+00]
28000     [3.13e+00, 4.75e-02, 3.69e+00]    [0.00e+00, 2.70e+00, 3.69e+00]    [3.23e+00, 2.70e+00, 1.83e+00, 2.40e+00]
29000     [3.11e+00, 8.68e-02, 3.67e+00]    [0.00e+00, 2.63e+00, 3.67e+00]    [3.17e+00, 2.63e+00, 1.76e+00, 2.35e+00]
30000     [3.09e+00, 4.48e-02, 3.65e+00]    [0.00e+00, 2.57e+00, 3.65e+00]    [3.13e+00, 2.57e+00, 1.70e+00, 2.29e+00]

Best model at step 30000:
  train loss: 6.78e+00
  test loss: 6.22e+00
  test metric: [3.13e+00, 2.57e+00, 1.70e+00, 2.29e+00]

'train' took 36.642905 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Cross-validation iteration: 4
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.077884 s

'compile' took 0.374844 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.03e+02, 1.04e+02, 2.77e+00]    [0.00e+00, 1.03e+02, 2.77e+00]    [1.03e+02, 1.03e+02, 4.75e+00, 4.75e+00]
1000      [7.60e+01, 6.57e+01, 2.96e+00]    [0.00e+00, 8.21e+01, 2.96e+00]    [8.22e+01, 8.21e+01, 2.30e+01, 2.31e+01]
2000      [4.23e+01, 2.66e+01, 3.83e+00]    [0.00e+00, 5.60e+01, 3.83e+00]    [5.69e+01, 5.60e+01, 2.61e+01, 2.67e+01]
3000      [1.94e+01, 8.83e+00, 4.22e+00]    [0.00e+00, 3.38e+01, 4.22e+00]    [3.51e+01, 3.38e+01, 1.48e+01, 1.51e+01]
4000      [7.93e+00, 1.00e+00, 4.35e+00]    [0.00e+00, 1.74e+01, 4.35e+00]    [1.79e+01, 1.74e+01, 1.24e+01, 1.20e+01]
5000      [6.43e+00, 5.43e-01, 4.30e+00]    [0.00e+00, 1.39e+01, 4.30e+00]    [1.42e+01, 1.39e+01, 1.24e+01, 1.22e+01]
6000      [5.73e+00, 5.29e-01, 4.25e+00]    [0.00e+00, 1.18e+01, 4.25e+00]    [1.19e+01, 1.18e+01, 1.21e+01, 1.21e+01]
7000      [5.35e+00, 4.65e-01, 4.21e+00]    [0.00e+00, 1.09e+01, 4.21e+00]    [1.09e+01, 1.09e+01, 1.17e+01, 1.17e+01]
8000      [5.13e+00, 4.44e-01, 4.16e+00]    [0.00e+00, 1.04e+01, 4.16e+00]    [1.03e+01, 1.04e+01, 1.14e+01, 1.14e+01]
9000      [4.97e+00, 4.83e-01, 4.12e+00]    [0.00e+00, 1.01e+01, 4.12e+00]    [9.95e+00, 1.01e+01, 1.10e+01, 1.11e+01]
10000     [4.82e+00, 3.93e-01, 4.07e+00]    [0.00e+00, 9.91e+00, 4.07e+00]    [9.64e+00, 9.91e+00, 1.05e+01, 1.06e+01]
11000     [4.66e+00, 3.31e-01, 4.03e+00]    [0.00e+00, 9.89e+00, 4.03e+00]    [9.51e+00, 9.89e+00, 1.02e+01, 1.03e+01]
12000     [4.54e+00, 3.09e-01, 4.00e+00]    [0.00e+00, 9.72e+00, 4.00e+00]    [9.22e+00, 9.72e+00, 9.82e+00, 1.00e+01]
13000     [4.47e+00, 3.78e-01, 3.96e+00]    [0.00e+00, 9.41e+00, 3.96e+00]    [8.81e+00, 9.41e+00, 9.37e+00, 9.58e+00]
14000     [4.30e+00, 2.27e-01, 3.93e+00]    [0.00e+00, 9.36e+00, 3.93e+00]    [8.67e+00, 9.36e+00, 9.17e+00, 9.42e+00]
15000     [4.22e+00, 2.50e-01, 3.90e+00]    [0.00e+00, 9.21e+00, 3.90e+00]    [8.44e+00, 9.21e+00, 8.74e+00, 9.04e+00]
16000     [4.11e+00, 2.03e-01, 3.87e+00]    [0.00e+00, 9.01e+00, 3.87e+00]    [8.17e+00, 9.01e+00, 8.53e+00, 8.87e+00]
17000     [4.02e+00, 1.88e-01, 3.84e+00]    [0.00e+00, 8.76e+00, 3.84e+00]    [7.84e+00, 8.76e+00, 8.33e+00, 8.69e+00]
18000     [3.98e+00, 2.46e-01, 3.81e+00]    [0.00e+00, 8.44e+00, 3.81e+00]    [7.45e+00, 8.44e+00, 7.89e+00, 8.29e+00]
19000     [3.87e+00, 1.99e-01, 3.78e+00]    [0.00e+00, 8.29e+00, 3.78e+00]    [7.23e+00, 8.29e+00, 7.80e+00, 8.23e+00]
20000     [3.81e+00, 1.66e-01, 3.75e+00]    [0.00e+00, 8.10e+00, 3.75e+00]    [6.97e+00, 8.10e+00, 7.38e+00, 7.86e+00]
21000     [3.74e+00, 1.47e-01, 3.73e+00]    [0.00e+00, 7.96e+00, 3.73e+00]    [6.76e+00, 7.96e+00, 7.05e+00, 7.59e+00]
22000     [3.67e+00, 1.48e-01, 3.70e+00]    [0.00e+00, 7.92e+00, 3.70e+00]    [6.65e+00, 7.92e+00, 6.85e+00, 7.45e+00]
23000     [3.61e+00, 1.09e-01, 3.68e+00]    [0.00e+00, 7.76e+00, 3.68e+00]    [6.45e+00, 7.76e+00, 6.56e+00, 7.20e+00]
24000     [3.58e+00, 1.52e-01, 3.65e+00]    [0.00e+00, 7.60e+00, 3.65e+00]    [6.28e+00, 7.60e+00, 6.35e+00, 7.02e+00]
25000     [3.55e+00, 6.64e-02, 3.63e+00]    [0.00e+00, 7.40e+00, 3.63e+00]    [6.07e+00, 7.40e+00, 6.02e+00, 6.74e+00]
26000     [3.54e+00, 1.29e-01, 3.61e+00]    [0.00e+00, 7.33e+00, 3.61e+00]    [6.00e+00, 7.33e+00, 5.90e+00, 6.63e+00]
27000     [3.54e+00, 1.11e-01, 3.59e+00]    [0.00e+00, 7.13e+00, 3.59e+00]    [5.78e+00, 7.13e+00, 5.51e+00, 6.27e+00]
28000     [3.49e+00, 5.58e-02, 3.56e+00]    [0.00e+00, 7.07e+00, 3.56e+00]    [5.68e+00, 7.07e+00, 5.34e+00, 6.10e+00]
29000     [3.45e+00, 5.88e-02, 3.54e+00]    [0.00e+00, 6.98e+00, 3.54e+00]    [5.56e+00, 6.98e+00, 5.14e+00, 5.91e+00]
30000     [3.43e+00, 2.88e-02, 3.52e+00]    [0.00e+00, 6.86e+00, 3.52e+00]    [5.43e+00, 6.86e+00, 4.89e+00, 5.68e+00]

Best model at step 30000:
  train loss: 6.98e+00
  test loss: 1.04e+01
  test metric: [5.43e+00, 6.86e+00, 4.89e+00, 5.68e+00]

'train' took 39.452350 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Cross-validation iteration: 5
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.078602 s

'compile' took 0.382438 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.03e+02, 1.04e+02, 2.91e+00]    [0.00e+00, 1.02e+02, 2.91e+00]    [1.02e+02, 1.02e+02, 3.61e+00, 3.61e+00]
1000      [7.69e+01, 7.12e+01, 3.04e+00]    [0.00e+00, 7.49e+01, 3.04e+00]    [7.50e+01, 7.49e+01, 2.62e+01, 2.63e+01]
2000      [4.58e+01, 3.12e+01, 4.02e+00]    [0.00e+00, 4.92e+01, 4.02e+00]    [4.96e+01, 4.92e+01, 1.71e+01, 1.84e+01]
3000      [1.81e+01, 6.51e+00, 4.60e+00]    [0.00e+00, 2.86e+01, 4.60e+00]    [2.81e+01, 2.86e+01, 6.92e+00, 1.09e+01]
4000      [8.04e+00, 1.65e+00, 4.69e+00]    [0.00e+00, 1.51e+01, 4.69e+00]    [1.41e+01, 1.51e+01, 8.71e+00, 1.03e+01]
5000      [6.70e+00, 1.48e-01, 4.61e+00]    [0.00e+00, 9.87e+00, 4.61e+00]    [9.43e+00, 9.87e+00, 3.56e+00, 4.75e+00]
6000      [5.78e+00, 2.72e-01, 4.54e+00]    [0.00e+00, 7.12e+00, 4.54e+00]    [6.87e+00, 7.12e+00, 2.86e+00, 3.48e+00]
7000      [5.08e+00, 3.51e-01, 4.47e+00]    [0.00e+00, 5.32e+00, 4.47e+00]    [5.34e+00, 5.32e+00, 3.65e+00, 4.08e+00]
8000      [4.67e+00, 4.40e-01, 4.41e+00]    [0.00e+00, 5.15e+00, 4.41e+00]    [5.20e+00, 5.15e+00, 3.33e+00, 3.59e+00]
9000      [4.38e+00, 2.89e-01, 4.36e+00]    [0.00e+00, 5.08e+00, 4.36e+00]    [5.09e+00, 5.08e+00, 3.16e+00, 3.24e+00]
10000     [4.11e+00, 2.21e-01, 4.30e+00]    [0.00e+00, 4.56e+00, 4.30e+00]    [4.55e+00, 4.56e+00, 3.46e+00, 3.40e+00]
11000     [3.87e+00, 3.00e-01, 4.26e+00]    [0.00e+00, 4.43e+00, 4.26e+00]    [4.40e+00, 4.43e+00, 3.32e+00, 3.12e+00]
12000     [3.73e+00, 1.56e-01, 4.22e+00]    [0.00e+00, 4.40e+00, 4.22e+00]    [4.32e+00, 4.40e+00, 3.20e+00, 2.74e+00]
13000     [3.64e+00, 1.39e-01, 4.18e+00]    [0.00e+00, 4.38e+00, 4.18e+00]    [4.22e+00, 4.38e+00, 3.20e+00, 2.35e+00]
14000     [3.57e+00, 1.05e-01, 4.15e+00]    [0.00e+00, 4.32e+00, 4.15e+00]    [4.12e+00, 4.32e+00, 3.11e+00, 2.12e+00]
15000     [3.46e+00, 2.44e-02, 4.11e+00]    [0.00e+00, 4.02e+00, 4.11e+00]    [3.79e+00, 4.02e+00, 3.17e+00, 2.20e+00]
16000     [3.47e+00, 1.69e-01, 4.08e+00]    [0.00e+00, 3.78e+00, 4.08e+00]    [3.54e+00, 3.78e+00, 3.23e+00, 2.28e+00]
17000     [3.37e+00, 4.06e-02, 4.04e+00]    [0.00e+00, 3.78e+00, 4.04e+00]    [3.53e+00, 3.78e+00, 3.21e+00, 2.26e+00]
18000     [3.38e+00, 1.15e-01, 4.01e+00]    [0.00e+00, 3.74e+00, 4.01e+00]    [3.48e+00, 3.74e+00, 3.21e+00, 2.27e+00]
19000     [3.31e+00, 7.90e-02, 3.98e+00]    [0.00e+00, 3.56e+00, 3.98e+00]    [3.40e+00, 3.56e+00, 3.22e+00, 2.35e+00]
20000     [3.32e+00, 1.63e-01, 3.95e+00]    [0.00e+00, 3.45e+00, 3.95e+00]    [3.41e+00, 3.45e+00, 3.15e+00, 2.39e+00]
21000     [3.25e+00, 4.03e-02, 3.92e+00]    [0.00e+00, 3.50e+00, 3.92e+00]    [3.54e+00, 3.50e+00, 2.98e+00, 2.36e+00]
22000     [3.30e+00, 1.86e-01, 3.89e+00]    [0.00e+00, 3.32e+00, 3.89e+00]    [3.46e+00, 3.32e+00, 3.01e+00, 2.44e+00]
23000     [3.24e+00, 1.18e-01, 3.87e+00]    [0.00e+00, 3.28e+00, 3.87e+00]    [3.49e+00, 3.28e+00, 2.93e+00, 2.45e+00]
24000     [3.21e+00, 7.69e-02, 3.84e+00]    [0.00e+00, 3.31e+00, 3.84e+00]    [3.57e+00, 3.31e+00, 2.80e+00, 2.43e+00]
25000     [3.18e+00, 8.82e-02, 3.81e+00]    [0.00e+00, 3.32e+00, 3.81e+00]    [3.62e+00, 3.32e+00, 2.71e+00, 2.40e+00]
26000     [3.15e+00, 1.20e-01, 3.79e+00]    [0.00e+00, 3.21e+00, 3.79e+00]    [3.52e+00, 3.21e+00, 2.79e+00, 2.36e+00]
27000     [3.22e+00, 2.50e-01, 3.77e+00]    [0.00e+00, 3.23e+00, 3.77e+00]    [3.53e+00, 3.23e+00, 2.77e+00, 2.28e+00]
28000     [3.10e+00, 2.63e-02, 3.74e+00]    [0.00e+00, 3.35e+00, 3.74e+00]    [3.66e+00, 3.35e+00, 2.61e+00, 2.19e+00]
29000     [3.07e+00, 3.38e-02, 3.72e+00]    [0.00e+00, 3.35e+00, 3.72e+00]    [3.66e+00, 3.35e+00, 2.57e+00, 2.16e+00]
30000     [3.10e+00, 1.31e-01, 3.70e+00]    [0.00e+00, 3.39e+00, 3.70e+00]    [3.70e+00, 3.39e+00, 2.51e+00, 2.12e+00]

Best model at step 29000:
  train loss: 6.82e+00
  test loss: 7.07e+00
  test metric: [3.66e+00, 3.35e+00, 2.57e+00, 2.16e+00]

'train' took 36.227584 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Cross-validation iteration: 6
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.080321 s

'compile' took 0.384174 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [9.93e+01, 9.97e+01, 2.85e+00]    [0.00e+00, 1.01e+02, 2.85e+00]    [1.01e+02, 1.01e+02, 5.76e-01, 5.76e-01]
1000      [6.41e+01, 4.87e+01, 3.47e+00]    [0.00e+00, 7.12e+01, 3.47e+00]    [7.09e+01, 7.12e+01, 3.80e+01, 3.83e+01]
2000      [2.83e+01, 1.18e+01, 4.20e+00]    [0.00e+00, 4.17e+01, 4.20e+00]    [4.30e+01, 4.17e+01, 3.16e+01, 3.17e+01]
3000      [1.21e+01, 1.85e+00, 4.43e+00]    [0.00e+00, 2.31e+01, 4.43e+00]    [2.44e+01, 2.31e+01, 1.79e+01, 1.83e+01]
4000      [7.90e+00, 1.26e+00, 4.40e+00]    [0.00e+00, 1.57e+01, 4.40e+00]    [1.63e+01, 1.57e+01, 9.53e+00, 9.69e+00]
5000      [5.17e+00, 9.36e-01, 4.37e+00]    [0.00e+00, 9.41e+00, 4.37e+00]    [9.42e+00, 9.41e+00, 4.87e+00, 4.77e+00]
6000      [4.14e+00, 8.75e-01, 4.33e+00]    [0.00e+00, 6.40e+00, 4.33e+00]    [6.15e+00, 6.40e+00, 2.51e+00, 2.78e+00]
7000      [3.66e+00, 8.15e-01, 4.30e+00]    [0.00e+00, 5.04e+00, 4.30e+00]    [4.39e+00, 5.04e+00, 2.16e+00, 2.74e+00]
8000      [3.38e+00, 7.12e-01, 4.27e+00]    [0.00e+00, 4.86e+00, 4.27e+00]    [3.92e+00, 4.86e+00, 1.94e+00, 2.66e+00]
9000      [3.25e+00, 6.12e-01, 4.23e+00]    [0.00e+00, 4.97e+00, 4.23e+00]    [3.81e+00, 4.97e+00, 1.50e+00, 2.34e+00]
10000     [3.18e+00, 4.50e-01, 4.20e+00]    [0.00e+00, 4.90e+00, 4.20e+00]    [3.62e+00, 4.90e+00, 1.42e+00, 2.26e+00]
11000     [3.15e+00, 3.98e-01, 4.17e+00]    [0.00e+00, 4.70e+00, 4.17e+00]    [3.32e+00, 4.70e+00, 1.55e+00, 2.64e+00]
12000     [3.15e+00, 2.88e-01, 4.14e+00]    [0.00e+00, 4.68e+00, 4.14e+00]    [3.23e+00, 4.68e+00, 1.57e+00, 2.20e+00]
13000     [3.06e+00, 1.66e-01, 4.12e+00]    [0.00e+00, 4.48e+00, 4.12e+00]    [2.96e+00, 4.48e+00, 1.66e+00, 2.54e+00]
14000     [3.06e+00, 9.47e-02, 4.09e+00]    [0.00e+00, 4.36e+00, 4.09e+00]    [2.93e+00, 4.36e+00, 1.65e+00, 2.41e+00]
15000     [3.03e+00, 6.88e-02, 4.05e+00]    [0.00e+00, 4.25e+00, 4.05e+00]    [2.86e+00, 4.25e+00, 1.64e+00, 2.46e+00]
16000     [2.99e+00, 3.92e-02, 4.01e+00]    [0.00e+00, 4.13e+00, 4.01e+00]    [2.69e+00, 4.13e+00, 1.66e+00, 2.47e+00]
17000     [3.01e+00, 7.91e-02, 3.97e+00]    [0.00e+00, 4.02e+00, 3.97e+00]    [2.56e+00, 4.02e+00, 1.78e+00, 2.32e+00]
18000     [2.93e+00, 4.87e-02, 3.94e+00]    [0.00e+00, 3.88e+00, 3.94e+00]    [2.39e+00, 3.88e+00, 1.66e+00, 2.49e+00]
19000     [2.91e+00, 4.45e-02, 3.90e+00]    [0.00e+00, 3.80e+00, 3.90e+00]    [2.29e+00, 3.80e+00, 1.69e+00, 2.44e+00]
20000     [2.89e+00, 3.34e-02, 3.86e+00]    [0.00e+00, 3.81e+00, 3.86e+00]    [2.30e+00, 3.81e+00, 1.77e+00, 2.25e+00]
21000     [2.86e+00, 6.18e-02, 3.83e+00]    [0.00e+00, 3.70e+00, 3.83e+00]    [2.19e+00, 3.70e+00, 1.68e+00, 2.33e+00]
22000     [2.85e+00, 3.83e-02, 3.80e+00]    [0.00e+00, 3.64e+00, 3.80e+00]    [2.23e+00, 3.64e+00, 1.62e+00, 2.29e+00]
23000     [2.83e+00, 4.57e-02, 3.77e+00]    [0.00e+00, 3.57e+00, 3.77e+00]    [2.21e+00, 3.57e+00, 1.52e+00, 2.31e+00]
24000     [2.80e+00, 2.90e-02, 3.74e+00]    [0.00e+00, 3.60e+00, 3.74e+00]    [2.41e+00, 3.60e+00, 1.44e+00, 2.18e+00]
25000     [2.82e+00, 7.15e-02, 3.71e+00]    [0.00e+00, 3.51e+00, 3.71e+00]    [2.30e+00, 3.51e+00, 1.37e+00, 2.28e+00]
26000     [2.76e+00, 2.64e-02, 3.68e+00]    [0.00e+00, 3.52e+00, 3.68e+00]    [2.45e+00, 3.52e+00, 1.29e+00, 2.21e+00]
27000     [2.80e+00, 5.19e-02, 3.65e+00]    [0.00e+00, 3.51e+00, 3.65e+00]    [2.64e+00, 3.51e+00, 1.23e+00, 2.18e+00]
28000     [2.74e+00, 5.59e-03, 3.63e+00]    [0.00e+00, 3.41e+00, 3.63e+00]    [2.52e+00, 3.41e+00, 1.12e+00, 2.27e+00]
29000     [2.74e+00, 2.81e-02, 3.60e+00]    [0.00e+00, 3.38e+00, 3.60e+00]    [2.61e+00, 3.38e+00, 1.06e+00, 2.27e+00]
30000     [2.73e+00, 1.94e-02, 3.58e+00]    [0.00e+00, 3.32e+00, 3.58e+00]    [2.57e+00, 3.32e+00, 9.86e-01, 2.33e+00]

Best model at step 30000:
  train loss: 6.33e+00
  test loss: 6.90e+00
  test metric: [2.57e+00, 3.32e+00, 9.86e-01, 2.33e+00]

'train' took 36.776465 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Cross-validation iteration: 7
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.076745 s

'compile' took 0.365785 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.02e+02, 1.03e+02, 2.58e+00]    [0.00e+00, 1.00e+02, 2.58e+00]    [1.00e+02, 1.00e+02, 1.21e+00, 1.21e+00]
1000      [7.97e+01, 6.07e+01, 2.66e+00]    [0.00e+00, 9.54e+01, 2.66e+00]    [9.55e+01, 9.54e+01, 9.67e+00, 9.79e+00]
2000      [4.77e+01, 2.78e+01, 3.43e+00]    [0.00e+00, 6.09e+01, 3.43e+00]    [6.38e+01, 6.09e+01, 2.07e+01, 2.23e+01]
3000      [1.95e+01, 3.46e+00, 3.94e+00]    [0.00e+00, 2.68e+01, 3.94e+00]    [3.03e+01, 2.68e+01, 1.63e+01, 1.15e+01]
4000      [8.04e+00, 2.06e+00, 4.01e+00]    [0.00e+00, 6.67e+00, 4.01e+00]    [1.04e+01, 6.67e+00, 6.24e+00, 5.07e+00]
5000      [4.77e+00, 1.92e+00, 4.03e+00]    [0.00e+00, 3.27e+00, 4.03e+00]    [4.88e+00, 3.27e+00, 2.08e+00, 3.99e+00]
6000      [3.79e+00, 1.55e+00, 4.02e+00]    [0.00e+00, 3.69e+00, 4.02e+00]    [2.59e+00, 3.69e+00, 2.86e+00, 3.73e+00]
7000      [3.50e+00, 1.34e+00, 3.99e+00]    [0.00e+00, 4.43e+00, 3.99e+00]    [2.88e+00, 4.43e+00, 2.80e+00, 3.20e+00]
8000      [3.36e+00, 1.27e+00, 3.96e+00]    [0.00e+00, 4.41e+00, 3.96e+00]    [2.99e+00, 4.41e+00, 2.58e+00, 2.67e+00]
9000      [3.24e+00, 1.10e+00, 3.93e+00]    [0.00e+00, 4.46e+00, 3.93e+00]    [3.30e+00, 4.46e+00, 2.42e+00, 2.45e+00]
10000     [3.24e+00, 1.03e+00, 3.89e+00]    [0.00e+00, 4.32e+00, 3.89e+00]    [3.35e+00, 4.32e+00, 2.17e+00, 2.19e+00]
11000     [3.19e+00, 8.75e-01, 3.86e+00]    [0.00e+00, 4.21e+00, 3.86e+00]    [3.36e+00, 4.21e+00, 1.94e+00, 1.96e+00]
12000     [3.15e+00, 9.35e-01, 3.83e+00]    [0.00e+00, 4.17e+00, 3.83e+00]    [3.36e+00, 4.17e+00, 1.74e+00, 1.75e+00]
13000     [3.12e+00, 7.67e-01, 3.80e+00]    [0.00e+00, 4.21e+00, 3.80e+00]    [3.40e+00, 4.21e+00, 1.64e+00, 1.66e+00]
14000     [3.10e+00, 7.17e-01, 3.77e+00]    [0.00e+00, 4.17e+00, 3.77e+00]    [3.40e+00, 4.17e+00, 1.55e+00, 1.57e+00]
15000     [3.08e+00, 7.00e-01, 3.74e+00]    [0.00e+00, 4.12e+00, 3.74e+00]    [3.41e+00, 4.12e+00, 1.48e+00, 1.49e+00]
16000     [3.06e+00, 6.36e-01, 3.72e+00]    [0.00e+00, 4.05e+00, 3.72e+00]    [3.39e+00, 4.05e+00, 1.40e+00, 1.42e+00]
17000     [3.05e+00, 6.59e-01, 3.70e+00]    [0.00e+00, 4.10e+00, 3.70e+00]    [3.46e+00, 4.10e+00, 1.36e+00, 1.37e+00]
18000     [3.11e+00, 6.66e-01, 3.67e+00]    [0.00e+00, 4.15e+00, 3.67e+00]    [3.52e+00, 4.15e+00, 1.33e+00, 1.34e+00]
19000     [3.04e+00, 5.44e-01, 3.65e+00]    [0.00e+00, 4.16e+00, 3.65e+00]    [3.57e+00, 4.16e+00, 1.30e+00, 1.31e+00]
20000     [3.06e+00, 4.99e-01, 3.63e+00]    [0.00e+00, 4.16e+00, 3.63e+00]    [3.63e+00, 4.16e+00, 1.30e+00, 1.31e+00]
21000     [3.04e+00, 5.02e-01, 3.60e+00]    [0.00e+00, 4.06e+00, 3.60e+00]    [3.60e+00, 4.06e+00, 1.28e+00, 1.29e+00]
22000     [3.00e+00, 4.30e-01, 3.58e+00]    [0.00e+00, 4.04e+00, 3.58e+00]    [3.62e+00, 4.04e+00, 1.26e+00, 1.27e+00]
23000     [2.99e+00, 4.23e-01, 3.55e+00]    [0.00e+00, 4.02e+00, 3.55e+00]    [3.63e+00, 4.02e+00, 1.25e+00, 1.26e+00]
24000     [3.02e+00, 4.85e-01, 3.53e+00]    [0.00e+00, 3.96e+00, 3.53e+00]    [3.60e+00, 3.96e+00, 1.25e+00, 1.26e+00]
25000     [3.02e+00, 5.44e-01, 3.51e+00]    [0.00e+00, 3.96e+00, 3.51e+00]    [3.62e+00, 3.96e+00, 1.25e+00, 1.25e+00]
26000     [2.96e+00, 4.18e-01, 3.49e+00]    [0.00e+00, 3.90e+00, 3.49e+00]    [3.59e+00, 3.90e+00, 1.23e+00, 1.24e+00]
27000     [2.96e+00, 3.41e-01, 3.47e+00]    [0.00e+00, 3.90e+00, 3.47e+00]    [3.61e+00, 3.90e+00, 1.21e+00, 1.22e+00]
28000     [2.96e+00, 3.94e-01, 3.45e+00]    [0.00e+00, 3.89e+00, 3.45e+00]    [3.62e+00, 3.89e+00, 1.20e+00, 1.21e+00]
29000     [2.94e+00, 3.11e-01, 3.43e+00]    [0.00e+00, 3.84e+00, 3.43e+00]    [3.60e+00, 3.84e+00, 1.19e+00, 1.20e+00]
30000     [2.99e+00, 3.83e-01, 3.41e+00]    [0.00e+00, 3.80e+00, 3.41e+00]    [3.57e+00, 3.80e+00, 1.20e+00, 1.21e+00]

Best model at step 29000:
  train loss: 6.68e+00
  test loss: 7.27e+00
  test metric: [3.60e+00, 3.84e+00, 1.19e+00, 1.20e+00]

'train' took 37.322119 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Cross-validation iteration: 8
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.076891 s

'compile' took 0.394677 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.03e+02, 1.04e+02, 2.79e+00]    [0.00e+00, 1.01e+02, 2.79e+00]    [1.01e+02, 1.01e+02, 2.75e+00, 2.75e+00]
1000      [7.95e+01, 6.49e+01, 2.87e+00]    [0.00e+00, 8.91e+01, 2.87e+00]    [8.92e+01, 8.91e+01, 6.59e+00, 6.62e+00]
2000      [5.22e+01, 3.55e+01, 3.52e+00]    [0.00e+00, 5.70e+01, 3.52e+00]    [5.79e+01, 5.70e+01, 9.72e+00, 9.95e+00]
3000      [1.07e+01, 6.61e+00, 4.28e+00]    [0.00e+00, 1.21e+01, 4.28e+00]    [1.43e+01, 1.21e+01, 5.60e+00, 4.12e+00]
4000      [5.50e+00, 1.22e+00, 4.28e+00]    [0.00e+00, 2.33e+00, 4.28e+00]    [3.10e+00, 2.33e+00, 1.86e+00, 9.34e-01]
5000      [4.77e+00, 1.00e+00, 4.23e+00]    [0.00e+00, 2.33e+00, 4.23e+00]    [2.09e+00, 2.33e+00, 1.76e+00, 8.30e-01]
6000      [4.44e+00, 9.18e-01, 4.18e+00]    [0.00e+00, 2.44e+00, 4.18e+00]    [2.27e+00, 2.44e+00, 1.57e+00, 1.01e+00]
7000      [4.19e+00, 8.61e-01, 4.14e+00]    [0.00e+00, 2.54e+00, 4.14e+00]    [2.42e+00, 2.54e+00, 1.29e+00, 9.89e-01]
8000      [3.97e+00, 8.17e-01, 4.11e+00]    [0.00e+00, 2.49e+00, 4.11e+00]    [2.40e+00, 2.49e+00, 1.18e+00, 1.02e+00]
9000      [3.75e+00, 7.11e-01, 4.08e+00]    [0.00e+00, 2.48e+00, 4.08e+00]    [2.41e+00, 2.48e+00, 1.14e+00, 1.05e+00]
10000     [3.61e+00, 6.53e-01, 4.06e+00]    [0.00e+00, 2.47e+00, 4.06e+00]    [2.41e+00, 2.47e+00, 1.14e+00, 1.10e+00]
11000     [3.45e+00, 6.26e-01, 4.04e+00]    [0.00e+00, 2.49e+00, 4.04e+00]    [2.44e+00, 2.49e+00, 1.23e+00, 1.23e+00]
12000     [3.33e+00, 5.26e-01, 4.01e+00]    [0.00e+00, 2.57e+00, 4.01e+00]    [2.54e+00, 2.57e+00, 1.32e+00, 1.34e+00]
13000     [3.24e+00, 5.03e-01, 3.99e+00]    [0.00e+00, 2.58e+00, 3.99e+00]    [2.55e+00, 2.58e+00, 1.40e+00, 1.40e+00]
14000     [3.16e+00, 4.26e-01, 3.97e+00]    [0.00e+00, 2.61e+00, 3.97e+00]    [2.59e+00, 2.61e+00, 1.33e+00, 1.32e+00]
15000     [3.13e+00, 3.94e-01, 3.95e+00]    [0.00e+00, 2.72e+00, 3.95e+00]    [2.73e+00, 2.72e+00, 1.27e+00, 1.27e+00]
16000     [3.12e+00, 4.77e-01, 3.92e+00]    [0.00e+00, 2.73e+00, 3.92e+00]    [2.78e+00, 2.73e+00, 1.22e+00, 1.26e+00]
17000     [3.09e+00, 3.24e-01, 3.90e+00]    [0.00e+00, 2.79e+00, 3.90e+00]    [2.87e+00, 2.79e+00, 1.14e+00, 1.21e+00]
18000     [3.05e+00, 2.94e-01, 3.88e+00]    [0.00e+00, 2.85e+00, 3.88e+00]    [2.97e+00, 2.85e+00, 1.08e+00, 1.18e+00]
19000     [3.04e+00, 2.54e-01, 3.86e+00]    [0.00e+00, 2.92e+00, 3.86e+00]    [3.08e+00, 2.92e+00, 1.02e+00, 1.16e+00]
20000     [3.02e+00, 2.52e-01, 3.84e+00]    [0.00e+00, 2.97e+00, 3.84e+00]    [3.16e+00, 2.97e+00, 9.81e-01, 1.16e+00]
21000     [3.02e+00, 2.28e-01, 3.82e+00]    [0.00e+00, 3.01e+00, 3.82e+00]    [3.22e+00, 3.01e+00, 9.66e-01, 1.17e+00]
22000     [2.99e+00, 1.80e-01, 3.81e+00]    [0.00e+00, 3.07e+00, 3.81e+00]    [3.30e+00, 3.07e+00, 9.97e-01, 1.22e+00]
23000     [2.99e+00, 1.67e-01, 3.79e+00]    [0.00e+00, 3.13e+00, 3.79e+00]    [3.37e+00, 3.13e+00, 1.01e+00, 1.25e+00]
24000     [3.01e+00, 2.59e-01, 3.77e+00]    [0.00e+00, 3.17e+00, 3.77e+00]    [3.44e+00, 3.17e+00, 9.95e-01, 1.26e+00]
25000     [2.98e+00, 1.86e-01, 3.75e+00]    [0.00e+00, 3.16e+00, 3.75e+00]    [3.46e+00, 3.16e+00, 1.03e+00, 1.31e+00]
26000     [2.98e+00, 1.55e-01, 3.74e+00]    [0.00e+00, 3.18e+00, 3.74e+00]    [3.50e+00, 3.18e+00, 1.04e+00, 1.34e+00]
27000     [2.96e+00, 1.21e-01, 3.72e+00]    [0.00e+00, 3.16e+00, 3.72e+00]    [3.50e+00, 3.16e+00, 1.10e+00, 1.41e+00]
28000     [2.95e+00, 1.31e-01, 3.70e+00]    [0.00e+00, 3.20e+00, 3.70e+00]    [3.54e+00, 3.20e+00, 1.11e+00, 1.43e+00]
29000     [2.93e+00, 1.06e-01, 3.69e+00]    [0.00e+00, 3.18e+00, 3.69e+00]    [3.54e+00, 3.18e+00, 1.16e+00, 1.47e+00]
30000     [2.94e+00, 1.44e-01, 3.67e+00]    [0.00e+00, 3.22e+00, 3.67e+00]    [3.59e+00, 3.22e+00, 1.17e+00, 1.50e+00]

Best model at step 29000:
  train loss: 6.72e+00
  test loss: 6.87e+00
  test metric: [3.54e+00, 3.18e+00, 1.16e+00, 1.47e+00]

'train' took 36.980333 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Cross-validation iteration: 9
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.077483 s

'compile' took 0.376945 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [9.94e+01, 9.84e+01, 2.76e+00]    [0.00e+00, 9.96e+01, 2.76e+00]    [9.96e+01, 9.96e+01, 7.15e-01, 7.15e-01]
1000      [7.82e+01, 5.61e+01, 2.99e+00]    [0.00e+00, 8.86e+01, 2.99e+00]    [8.87e+01, 8.86e+01, 7.83e+00, 7.89e+00]
2000      [4.59e+01, 2.70e+01, 3.74e+00]    [0.00e+00, 6.33e+01, 3.74e+00]    [6.42e+01, 6.33e+01, 6.53e+00, 6.70e+00]
3000      [1.71e+01, 7.98e+00, 4.36e+00]    [0.00e+00, 2.60e+01, 4.36e+00]    [2.70e+01, 2.60e+01, 1.12e+01, 1.13e+01]
4000      [5.04e+00, 6.56e-01, 4.49e+00]    [0.00e+00, 4.53e+00, 4.49e+00]    [4.74e+00, 4.53e+00, 2.21e+00, 2.48e+00]
5000      [4.38e+00, 5.03e-01, 4.43e+00]    [0.00e+00, 4.16e+00, 4.43e+00]    [4.15e+00, 4.16e+00, 2.20e+00, 2.18e+00]
6000      [4.10e+00, 3.54e-01, 4.37e+00]    [0.00e+00, 4.16e+00, 4.37e+00]    [4.10e+00, 4.16e+00, 2.19e+00, 1.96e+00]
7000      [3.94e+00, 2.51e-01, 4.32e+00]    [0.00e+00, 4.02e+00, 4.32e+00]    [3.91e+00, 4.02e+00, 2.51e+00, 2.08e+00]
8000      [3.80e+00, 1.58e-01, 4.27e+00]    [0.00e+00, 3.86e+00, 4.27e+00]    [3.72e+00, 3.86e+00, 2.58e+00, 2.03e+00]
9000      [3.72e+00, 2.49e-01, 4.22e+00]    [0.00e+00, 3.72e+00, 4.22e+00]    [3.56e+00, 3.72e+00, 2.53e+00, 1.98e+00]
10000     [3.59e+00, 6.64e-02, 4.18e+00]    [0.00e+00, 3.68e+00, 4.18e+00]    [3.49e+00, 3.68e+00, 2.50e+00, 1.84e+00]
11000     [3.50e+00, 9.05e-02, 4.15e+00]    [0.00e+00, 3.60e+00, 4.15e+00]    [3.39e+00, 3.60e+00, 2.42e+00, 1.74e+00]
12000     [3.43e+00, 4.91e-02, 4.11e+00]    [0.00e+00, 3.55e+00, 4.11e+00]    [3.32e+00, 3.55e+00, 2.36e+00, 1.62e+00]
13000     [3.40e+00, 2.01e-01, 4.08e+00]    [0.00e+00, 3.48e+00, 4.08e+00]    [3.23e+00, 3.48e+00, 2.30e+00, 1.58e+00]
14000     [3.33e+00, 1.08e-01, 4.05e+00]    [0.00e+00, 3.43e+00, 4.05e+00]    [3.16e+00, 3.43e+00, 2.27e+00, 1.50e+00]
15000     [3.30e+00, 3.83e-02, 4.01e+00]    [0.00e+00, 3.39e+00, 4.01e+00]    [3.10e+00, 3.39e+00, 2.24e+00, 1.46e+00]
16000     [3.24e+00, 2.99e-02, 3.98e+00]    [0.00e+00, 3.36e+00, 3.98e+00]    [3.05e+00, 3.36e+00, 2.22e+00, 1.41e+00]
17000     [3.22e+00, 6.56e-02, 3.96e+00]    [0.00e+00, 3.33e+00, 3.96e+00]    [3.00e+00, 3.33e+00, 2.21e+00, 1.40e+00]
18000     [3.21e+00, 8.10e-02, 3.93e+00]    [0.00e+00, 3.30e+00, 3.93e+00]    [2.96e+00, 3.30e+00, 2.20e+00, 1.36e+00]
19000     [3.20e+00, 7.33e-02, 3.90e+00]    [0.00e+00, 3.24e+00, 3.90e+00]    [2.90e+00, 3.24e+00, 2.18e+00, 1.40e+00]
20000     [3.15e+00, 3.95e-02, 3.87e+00]    [0.00e+00, 3.22e+00, 3.87e+00]    [2.87e+00, 3.22e+00, 2.15e+00, 1.35e+00]
21000     [3.14e+00, 7.30e-02, 3.85e+00]    [0.00e+00, 3.20e+00, 3.85e+00]    [2.84e+00, 3.20e+00, 2.14e+00, 1.31e+00]
22000     [3.12e+00, 4.53e-02, 3.82e+00]    [0.00e+00, 3.16e+00, 3.82e+00]    [2.80e+00, 3.16e+00, 2.10e+00, 1.30e+00]
23000     [3.12e+00, 4.85e-02, 3.80e+00]    [0.00e+00, 3.12e+00, 3.80e+00]    [2.75e+00, 3.12e+00, 2.10e+00, 1.31e+00]
24000     [3.08e+00, 3.77e-02, 3.78e+00]    [0.00e+00, 3.10e+00, 3.78e+00]    [2.73e+00, 3.10e+00, 2.08e+00, 1.26e+00]
25000     [3.10e+00, 2.01e-01, 3.76e+00]    [0.00e+00, 3.05e+00, 3.76e+00]    [2.67e+00, 3.05e+00, 2.03e+00, 1.30e+00]
26000     [3.06e+00, 7.55e-02, 3.73e+00]    [0.00e+00, 3.05e+00, 3.73e+00]    [2.67e+00, 3.05e+00, 2.03e+00, 1.20e+00]
27000     [3.03e+00, 1.77e-02, 3.71e+00]    [0.00e+00, 3.01e+00, 3.71e+00]    [2.62e+00, 3.01e+00, 2.01e+00, 1.23e+00]
28000     [3.01e+00, 5.12e-02, 3.69e+00]    [0.00e+00, 2.98e+00, 3.69e+00]    [2.59e+00, 2.98e+00, 1.99e+00, 1.22e+00]
29000     [3.00e+00, 6.81e-02, 3.67e+00]    [0.00e+00, 2.97e+00, 3.67e+00]    [2.57e+00, 2.97e+00, 1.99e+00, 1.18e+00]
30000     [2.98e+00, 6.79e-02, 3.66e+00]    [0.00e+00, 2.92e+00, 3.66e+00]    [2.52e+00, 2.92e+00, 1.97e+00, 1.22e+00]

Best model at step 30000:
  train loss: 6.71e+00
  test loss: 6.57e+00
  test metric: [2.52e+00, 2.92e+00, 1.97e+00, 1.22e+00]

'train' took 35.856351 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Cross-validation iteration: 10
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.078972 s

'compile' took 0.395988 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.01e+02, 9.93e+01, 2.83e+00]    [0.00e+00, 9.94e+01, 2.83e+00]    [9.94e+01, 9.94e+01, 1.04e+00, 1.04e+00]
1000      [7.68e+01, 5.56e+01, 3.00e+00]    [0.00e+00, 8.98e+01, 3.00e+00]    [8.98e+01, 8.98e+01, 2.42e+00, 2.43e+00]
2000      [4.34e+01, 2.52e+01, 3.76e+00]    [0.00e+00, 5.71e+01, 3.76e+00]    [5.79e+01, 5.71e+01, 1.24e+01, 1.26e+01]
3000      [1.77e+01, 6.91e+00, 4.27e+00]    [0.00e+00, 1.67e+01, 4.27e+00]    [1.81e+01, 1.67e+01, 7.92e+00, 8.03e+00]
4000      [5.82e+00, 8.76e-01, 4.39e+00]    [0.00e+00, 5.72e+00, 4.39e+00]    [5.50e+00, 5.72e+00, 1.27e+00, 8.41e-01]
5000      [5.16e+00, 2.41e-01, 4.31e+00]    [0.00e+00, 6.13e+00, 4.31e+00]    [6.01e+00, 6.13e+00, 1.03e+00, 1.04e+00]
6000      [4.67e+00, 2.08e-01, 4.26e+00]    [0.00e+00, 5.82e+00, 4.26e+00]    [5.73e+00, 5.82e+00, 1.36e+00, 1.48e+00]
7000      [4.42e+00, 6.19e-02, 4.20e+00]    [0.00e+00, 5.64e+00, 4.20e+00]    [5.58e+00, 5.64e+00, 1.59e+00, 1.70e+00]
8000      [4.20e+00, 1.54e-01, 4.16e+00]    [0.00e+00, 5.55e+00, 4.16e+00]    [5.52e+00, 5.55e+00, 2.15e+00, 2.23e+00]
9000      [3.98e+00, 4.49e-02, 4.13e+00]    [0.00e+00, 5.45e+00, 4.13e+00]    [5.44e+00, 5.45e+00, 2.65e+00, 2.69e+00]
10000     [3.83e+00, 8.21e-02, 4.09e+00]    [0.00e+00, 5.38e+00, 4.09e+00]    [5.38e+00, 5.38e+00, 2.89e+00, 2.90e+00]
11000     [3.72e+00, 4.09e-02, 4.06e+00]    [0.00e+00, 5.32e+00, 4.06e+00]    [5.33e+00, 5.32e+00, 3.03e+00, 2.99e+00]
12000     [3.71e+00, 1.55e-01, 4.03e+00]    [0.00e+00, 5.28e+00, 4.03e+00]    [5.32e+00, 5.28e+00, 3.04e+00, 2.93e+00]
13000     [3.62e+00, 5.78e-02, 4.00e+00]    [0.00e+00, 5.22e+00, 4.00e+00]    [5.29e+00, 5.22e+00, 3.09e+00, 2.89e+00]
14000     [3.55e+00, 1.28e-01, 3.98e+00]    [0.00e+00, 5.16e+00, 3.98e+00]    [5.28e+00, 5.16e+00, 3.13e+00, 2.85e+00]
15000     [3.48e+00, 2.66e-02, 3.95e+00]    [0.00e+00, 5.12e+00, 3.95e+00]    [5.27e+00, 5.12e+00, 3.15e+00, 2.75e+00]
16000     [3.48e+00, 1.32e-01, 3.93e+00]    [0.00e+00, 5.08e+00, 3.93e+00]    [5.24e+00, 5.08e+00, 3.16e+00, 2.63e+00]
17000     [3.42e+00, 1.67e-01, 3.90e+00]    [0.00e+00, 5.02e+00, 3.90e+00]    [5.20e+00, 5.02e+00, 3.17e+00, 2.57e+00]
18000     [3.38e+00, 1.17e-01, 3.88e+00]    [0.00e+00, 4.99e+00, 3.88e+00]    [5.18e+00, 4.99e+00, 3.11e+00, 2.48e+00]
19000     [3.38e+00, 1.15e-01, 3.86e+00]    [0.00e+00, 4.98e+00, 3.86e+00]    [5.18e+00, 4.98e+00, 3.03e+00, 2.36e+00]
20000     [3.32e+00, 1.11e-01, 3.83e+00]    [0.00e+00, 4.95e+00, 3.83e+00]    [5.15e+00, 4.95e+00, 2.96e+00, 2.29e+00]
21000     [3.29e+00, 4.55e-02, 3.81e+00]    [0.00e+00, 4.93e+00, 3.81e+00]    [5.13e+00, 4.93e+00, 2.88e+00, 2.20e+00]
22000     [3.32e+00, 2.01e-01, 3.79e+00]    [0.00e+00, 4.91e+00, 3.79e+00]    [5.11e+00, 4.91e+00, 2.82e+00, 2.15e+00]
23000     [3.27e+00, 5.64e-02, 3.77e+00]    [0.00e+00, 4.91e+00, 3.77e+00]    [5.11e+00, 4.91e+00, 2.74e+00, 2.05e+00]
24000     [3.29e+00, 9.06e-02, 3.75e+00]    [0.00e+00, 4.90e+00, 3.75e+00]    [5.10e+00, 4.90e+00, 2.66e+00, 1.97e+00]
25000     [3.24e+00, 3.96e-02, 3.73e+00]    [0.00e+00, 4.88e+00, 3.73e+00]    [5.09e+00, 4.88e+00, 2.63e+00, 1.93e+00]
26000     [3.23e+00, 1.38e-01, 3.71e+00]    [0.00e+00, 4.85e+00, 3.71e+00]    [5.06e+00, 4.85e+00, 2.56e+00, 1.85e+00]
27000     [3.23e+00, 1.14e-01, 3.69e+00]    [0.00e+00, 4.85e+00, 3.69e+00]    [5.05e+00, 4.85e+00, 2.52e+00, 1.81e+00]
28000     [3.18e+00, 2.34e-02, 3.67e+00]    [0.00e+00, 4.84e+00, 3.67e+00]    [5.05e+00, 4.84e+00, 2.46e+00, 1.74e+00]
29000     [3.19e+00, 2.94e-02, 3.65e+00]    [0.00e+00, 4.83e+00, 3.65e+00]    [5.04e+00, 4.83e+00, 2.41e+00, 1.68e+00]
30000     [3.17e+00, 5.78e-02, 3.63e+00]    [0.00e+00, 4.82e+00, 3.63e+00]    [5.03e+00, 4.82e+00, 2.35e+00, 1.61e+00]

Best model at step 30000:
  train loss: 6.86e+00
  test loss: 8.46e+00
  test metric: [5.03e+00, 4.82e+00, 2.35e+00, 1.61e+00]

'train' took 36.748359 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...
[3.953871553187787, 4.152188072971206, 2.5684811324029697, 6.8574518595244855, 3.3520241774440245, 3.3245536079635896, 3.8427049581142634, 3.181761243971821, 2.9172407906747866, 4.820339159829347]
Estar 9 3.897061655608428 1.1640139235943012
              Case          n     E (GPa)   sy (GPa)     C (GPa)    dP/dh (N/m)      Wp/Wt    E* (GPa)      sy/E*
count    97.000000  97.000000   97.000000  97.000000   97.000000      97.000000  97.000000   97.000000  97.000000
mean    279.030928   0.213917  107.163804   1.327507   92.953350  128652.067967   0.731227  102.813375   0.013835
std     411.446469   0.178797   67.175628   1.126477   66.232911   85768.846959   0.134844   60.541899   0.009753
min       1.000000   0.000000   10.000000   0.030000    2.731300    9801.900000   0.451835   10.880844   0.001399
25%      37.000000   0.100000   50.000000   0.300000   39.269000   53194.000000   0.628612   52.343315   0.005508
50%      67.000000   0.177243  100.806000   1.000000   80.813000  123181.000000   0.740598  100.685905   0.011463
75%      91.000000   0.300000  170.000000   2.000000  144.840000  194391.000000   0.830543  159.806250   0.019105
max    1023.000000   0.500000  210.000000   5.298021  261.330000  336838.000000   0.971835  190.913667   0.038209
              Case          n     E (GPa)   sy (GPa)     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  14.000000   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499   1.436338   83.395179  122797.459494   0.757835
std     412.214557   0.087468   70.142848   1.652156   75.629024   92835.843040   0.157921
min       6.000000   0.000000   10.000000   0.036792    5.391397   12832.983733   0.452806
25%    1001.250000   0.077031   37.524500   0.303588   30.061256   40728.231654   0.675230
50%    1007.000000   0.150378   79.808000   0.985823   71.391348   95187.916112   0.784977
75%    1012.750000   0.195295  155.424000   1.719617   97.621153  195369.671865   0.870086
max    1018.000000   0.300000  210.000000   5.298021  239.235773  315808.364481   0.971982

Iteration: 0
Compiling model...
Building feed-forward neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\fnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.061352 s

'compile' took 0.246798 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss              Test loss               Test metric
0         [7.21e+04, 6.51e-01]    [7.33e+04, 6.51e-01]    [7.33e+04]
1000      [1.13e+01, 5.23e-01]    [1.15e+01, 5.23e-01]    [1.15e+01]
2000      [1.03e+01, 4.48e-01]    [9.45e+00, 4.48e-01]    [9.45e+00]
3000      [7.13e+00, 4.00e-01]    [6.08e+00, 4.00e-01]    [6.08e+00]
4000      [8.15e+00, 3.69e-01]    [7.50e+00, 3.69e-01]    [7.50e+00]
5000      [9.33e+00, 3.46e-01]    [8.82e+00, 3.46e-01]    [8.82e+00]
6000      [2.20e+01, 3.28e-01]    [2.21e+01, 3.28e-01]    [2.21e+01]
7000      [2.62e+01, 3.12e-01]    [2.64e+01, 3.12e-01]    [2.64e+01]
8000      [2.18e+01, 3.01e-01]    [2.20e+01, 3.01e-01]    [2.20e+01]
9000      [2.80e+01, 2.92e-01]    [2.83e+01, 2.92e-01]    [2.83e+01]
10000     [4.32e+01, 2.85e-01]    [4.41e+01, 2.85e-01]    [4.41e+01]
11000     [2.00e+01, 2.80e-01]    [2.01e+01, 2.80e-01]    [2.01e+01]
12000     [2.65e+01, 2.77e-01]    [2.69e+01, 2.77e-01]    [2.69e+01]
13000     [4.36e+01, 2.74e-01]    [4.45e+01, 2.74e-01]    [4.45e+01]
14000     [1.95e+01, 2.72e-01]    [1.98e+01, 2.72e-01]    [1.98e+01]
15000     [2.62e+01, 2.70e-01]    [2.66e+01, 2.70e-01]    [2.66e+01]
16000     [4.39e+01, 2.69e-01]    [4.46e+01, 2.69e-01]    [4.46e+01]
17000     [1.87e+01, 2.67e-01]    [1.91e+01, 2.67e-01]    [1.91e+01]
18000     [2.54e+01, 2.65e-01]    [2.59e+01, 2.65e-01]    [2.59e+01]
19000     [4.46e+01, 2.64e-01]    [4.53e+01, 2.64e-01]    [4.53e+01]
20000     [2.29e+01, 2.62e-01]    [2.34e+01, 2.62e-01]    [2.34e+01]
21000     [2.92e+01, 2.60e-01]    [2.99e+01, 2.60e-01]    [2.99e+01]
22000     [4.07e+01, 2.58e-01]    [4.12e+01, 2.58e-01]    [4.12e+01]
23000     [2.08e+01, 2.56e-01]    [2.14e+01, 2.56e-01]    [2.14e+01]
24000     [2.70e+01, 2.54e-01]    [2.77e+01, 2.54e-01]    [2.77e+01]
25000     [4.26e+01, 2.51e-01]    [4.30e+01, 2.51e-01]    [4.30e+01]
26000     [1.89e+01, 2.49e-01]    [1.96e+01, 2.49e-01]    [1.96e+01]
27000     [2.51e+01, 2.47e-01]    [2.59e+01, 2.47e-01]    [2.59e+01]
28000     [3.89e+01, 2.46e-01]    [3.92e+01, 2.46e-01]    [3.92e+01]
29000     [2.12e+01, 2.44e-01]    [2.20e+01, 2.44e-01]    [2.20e+01]
30000     [2.61e+01, 2.43e-01]    [2.70e+01, 2.43e-01]    [2.70e+01]

Best model at step 3000:
  train loss: 7.53e+00
  test loss: 6.49e+00
  test metric: [6.08e+00]

'train' took 17.564095 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 1
Compiling model...
Building feed-forward neural network...
'build' took 0.036060 s

'compile' took 0.212257 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss              Test loss               Test metric
0         [3.02e+04, 1.44e+00]    [3.07e+04, 1.44e+00]    [3.07e+04]
1000      [2.57e+01, 1.01e+00]    [2.37e+01, 1.01e+00]    [2.37e+01]
2000      [1.76e+01, 7.69e-01]    [1.76e+01, 7.69e-01]    [1.76e+01]
3000      [1.73e+01, 6.29e-01]    [1.89e+01, 6.29e-01]    [1.89e+01]
4000      [1.52e+01, 5.40e-01]    [1.38e+01, 5.40e-01]    [1.38e+01]
5000      [1.23e+01, 4.79e-01]    [1.11e+01, 4.79e-01]    [1.11e+01]
6000      [1.31e+01, 4.34e-01]    [1.26e+01, 4.34e-01]    [1.26e+01]
7000      [7.06e+00, 4.00e-01]    [6.00e+00, 4.00e-01]    [6.00e+00]
8000      [3.14e+01, 3.73e-01]    [3.15e+01, 3.73e-01]    [3.15e+01]
9000      [3.84e+01, 3.55e-01]    [3.86e+01, 3.55e-01]    [3.86e+01]
10000     [4.43e+01, 3.42e-01]    [4.56e+01, 3.42e-01]    [4.56e+01]
11000     [2.39e+01, 3.34e-01]    [2.39e+01, 3.34e-01]    [2.39e+01]
12000     [3.16e+01, 3.28e-01]    [3.16e+01, 3.28e-01]    [3.16e+01]
13000     [4.49e+01, 3.25e-01]    [4.62e+01, 3.25e-01]    [4.62e+01]
14000     [2.41e+01, 3.22e-01]    [2.40e+01, 3.22e-01]    [2.40e+01]
15000     [3.20e+01, 3.20e-01]    [3.21e+01, 3.20e-01]    [3.21e+01]
16000     [4.47e+01, 3.18e-01]    [4.59e+01, 3.18e-01]    [4.59e+01]
17000     [2.75e+01, 3.16e-01]    [2.75e+01, 3.16e-01]    [2.75e+01]
18000     [3.19e+01, 3.15e-01]    [3.17e+01, 3.15e-01]    [3.17e+01]
19000     [4.06e+01, 3.13e-01]    [4.20e+01, 3.13e-01]    [4.20e+01]
20000     [2.65e+01, 3.11e-01]    [2.62e+01, 3.11e-01]    [2.62e+01]
21000     [3.32e+01, 3.10e-01]    [3.30e+01, 3.10e-01]    [3.30e+01]
22000     [4.04e+01, 3.08e-01]    [4.18e+01, 3.08e-01]    [4.18e+01]
23000     [2.58e+01, 3.05e-01]    [2.55e+01, 3.05e-01]    [2.55e+01]
24000     [3.31e+01, 3.03e-01]    [3.29e+01, 3.03e-01]    [3.29e+01]
25000     [4.03e+01, 3.00e-01]    [4.17e+01, 3.00e-01]    [4.17e+01]
26000     [2.46e+01, 2.97e-01]    [2.43e+01, 2.97e-01]    [2.43e+01]
27000     [3.08e+01, 2.94e-01]    [3.06e+01, 2.94e-01]    [3.06e+01]
28000     [4.11e+01, 2.92e-01]    [4.25e+01, 2.92e-01]    [4.25e+01]
29000     [2.52e+01, 2.90e-01]    [2.48e+01, 2.90e-01]    [2.48e+01]
30000     [3.28e+01, 2.88e-01]    [3.26e+01, 2.88e-01]    [3.26e+01]

Best model at step 7000:
  train loss: 7.46e+00
  test loss: 6.40e+00
  test metric: [6.00e+00]

'train' took 15.409163 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 2
Compiling model...
Building feed-forward neural network...
'build' took 0.054393 s

'compile' took 0.328965 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss              Test loss               Test metric
0         [1.47e+04, 1.95e+00]    [1.50e+04, 1.95e+00]    [1.50e+04]
1000      [7.50e+00, 1.24e+00]    [7.13e+00, 1.24e+00]    [7.13e+00]
2000      [6.01e+00, 8.77e-01]    [5.82e+00, 8.77e-01]    [5.82e+00]
3000      [7.47e+00, 6.71e-01]    [7.44e+00, 6.71e-01]    [7.44e+00]
4000      [1.43e+01, 5.47e-01]    [1.41e+01, 5.47e-01]    [1.41e+01]
5000      [2.54e+01, 4.65e-01]    [2.56e+01, 4.65e-01]    [2.56e+01]
6000      [4.77e+00, 4.06e-01]    [5.31e+00, 4.06e-01]    [5.31e+00]
7000      [5.12e+00, 3.62e-01]    [5.51e+00, 3.62e-01]    [5.51e+00]
8000      [5.38e+00, 3.29e-01]    [5.80e+00, 3.29e-01]    [5.80e+00]
9000      [1.15e+01, 3.06e-01]    [1.21e+01, 3.06e-01]    [1.21e+01]
10000     [4.71e+00, 2.89e-01]    [5.34e+00, 2.89e-01]    [5.34e+00]
11000     [4.79e+00, 2.78e-01]    [5.42e+00, 2.78e-01]    [5.42e+00]
12000     [6.01e+00, 2.70e-01]    [6.54e+00, 2.70e-01]    [6.54e+00]
13000     [1.72e+01, 2.64e-01]    [1.79e+01, 2.64e-01]    [1.79e+01]
14000     [3.07e+01, 2.60e-01]    [3.09e+01, 2.60e-01]    [3.09e+01]
15000     [5.26e+00, 2.58e-01]    [5.78e+00, 2.58e-01]    [5.78e+00]
16000     [4.84e+00, 2.56e-01]    [5.48e+00, 2.56e-01]    [5.48e+00]
17000     [7.10e+01, 2.54e-01]    [7.26e+01, 2.54e-01]    [7.26e+01]
18000     [4.78e+01, 2.52e-01]    [4.90e+01, 2.52e-01]    [4.90e+01]
19000     [4.23e+01, 2.50e-01]    [4.34e+01, 2.50e-01]    [4.34e+01]
20000     [4.10e+01, 2.49e-01]    [4.21e+01, 2.49e-01]    [4.21e+01]
21000     [4.02e+01, 2.47e-01]    [4.13e+01, 2.47e-01]    [4.13e+01]
22000     [3.97e+01, 2.46e-01]    [4.08e+01, 2.46e-01]    [4.08e+01]
23000     [4.01e+01, 2.45e-01]    [4.12e+01, 2.45e-01]    [4.12e+01]
24000     [4.83e+00, 2.43e-01]    [5.46e+00, 2.43e-01]    [5.46e+00]
25000     [6.66e+00, 2.42e-01]    [7.36e+00, 2.42e-01]    [7.36e+00]
26000     [5.40e+00, 2.40e-01]    [5.77e+00, 2.40e-01]    [5.77e+00]
27000     [6.08e+00, 2.38e-01]    [6.19e+00, 2.38e-01]    [6.19e+00]
28000     [1.57e+01, 2.37e-01]    [1.64e+01, 2.37e-01]    [1.64e+01]
29000     [6.90e+00, 2.36e-01]    [6.73e+00, 2.36e-01]    [6.73e+00]
30000     [5.39e+00, 2.34e-01]    [5.77e+00, 2.34e-01]    [5.77e+00]

Best model at step 10000:
  train loss: 4.99e+00
  test loss: 5.62e+00
  test metric: [5.34e+00]

'train' took 18.065597 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 3
Compiling model...
Building feed-forward neural network...
'build' took 0.052197 s

'compile' took 0.364699 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss              Test loss               Test metric
0         [1.37e+04, 2.67e+00]    [1.39e+04, 2.67e+00]    [1.39e+04]
1000      [8.21e+01, 1.70e+00]    [8.25e+01, 1.70e+00]    [8.25e+01]
2000      [3.18e+01, 1.19e+00]    [3.15e+01, 1.19e+00]    [3.15e+01]
3000      [2.78e+01, 9.05e-01]    [2.75e+01, 9.05e-01]    [2.75e+01]
4000      [2.83e+01, 7.27e-01]    [2.80e+01, 7.27e-01]    [2.80e+01]
5000      [2.64e+01, 6.09e-01]    [2.60e+01, 6.09e-01]    [2.60e+01]
6000      [2.56e+01, 5.25e-01]    [2.53e+01, 5.25e-01]    [2.53e+01]
7000      [2.64e+01, 4.64e-01]    [2.61e+01, 4.64e-01]    [2.61e+01]
8000      [4.72e+01, 4.21e-01]    [4.81e+01, 4.21e-01]    [4.81e+01]
9000      [6.53e+01, 3.91e-01]    [6.62e+01, 3.91e-01]    [6.62e+01]
10000     [2.09e+01, 3.70e-01]    [2.14e+01, 3.70e-01]    [2.14e+01]
11000     [3.41e+01, 3.58e-01]    [3.48e+01, 3.58e-01]    [3.48e+01]
12000     [7.00e+01, 3.50e-01]    [7.10e+01, 3.50e-01]    [7.10e+01]
13000     [2.16e+01, 3.45e-01]    [2.22e+01, 3.45e-01]    [2.22e+01]
14000     [3.36e+01, 3.42e-01]    [3.45e+01, 3.42e-01]    [3.45e+01]
15000     [7.07e+01, 3.41e-01]    [7.16e+01, 3.41e-01]    [7.16e+01]
16000     [1.99e+01, 3.39e-01]    [2.06e+01, 3.39e-01]    [2.06e+01]
17000     [3.14e+01, 3.38e-01]    [3.23e+01, 3.38e-01]    [3.23e+01]
18000     [6.56e+01, 3.36e-01]    [6.63e+01, 3.36e-01]    [6.63e+01]
19000     [2.35e+01, 3.35e-01]    [2.43e+01, 3.35e-01]    [2.43e+01]
20000     [3.38e+01, 3.34e-01]    [3.48e+01, 3.34e-01]    [3.48e+01]
21000     [6.37e+01, 3.32e-01]    [6.43e+01, 3.32e-01]    [6.43e+01]
22000     [2.49e+01, 3.31e-01]    [2.58e+01, 3.31e-01]    [2.58e+01]
23000     [3.44e+01, 3.29e-01]    [3.55e+01, 3.29e-01]    [3.55e+01]
24000     [7.00e+01, 3.28e-01]    [7.07e+01, 3.28e-01]    [7.07e+01]
25000     [2.57e+01, 3.26e-01]    [2.67e+01, 3.26e-01]    [2.67e+01]
26000     [3.45e+01, 3.25e-01]    [3.57e+01, 3.25e-01]    [3.57e+01]
27000     [6.37e+01, 3.23e-01]    [6.41e+01, 3.23e-01]    [6.41e+01]
28000     [2.35e+01, 3.21e-01]    [2.45e+01, 3.21e-01]    [2.45e+01]
29000     [3.26e+01, 3.20e-01]    [3.38e+01, 3.20e-01]    [3.38e+01]
30000     [6.55e+01, 3.18e-01]    [6.59e+01, 3.18e-01]    [6.59e+01]

Best model at step 16000:
  train loss: 2.03e+01
  test loss: 2.09e+01
  test metric: [2.06e+01]

'train' took 18.260477 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 4
Compiling model...
Building feed-forward neural network...
'build' took 0.047187 s

'compile' took 0.408659 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss              Test loss               Test metric
0         [3.39e+04, 3.21e+00]    [3.45e+04, 3.21e+00]    [3.45e+04]
1000      [2.90e+01, 1.95e+00]    [2.88e+01, 1.95e+00]    [2.88e+01]
2000      [1.12e+01, 1.31e+00]    [1.06e+01, 1.31e+00]    [1.06e+01]
3000      [7.19e+00, 9.50e-01]    [8.40e+00, 9.50e-01]    [8.40e+00]
4000      [4.60e+00, 7.38e-01]    [5.45e+00, 7.38e-01]    [5.45e+00]
5000      [6.24e+00, 5.98e-01]    [5.99e+00, 5.98e-01]    [5.99e+00]
6000      [4.72e+00, 4.99e-01]    [5.31e+00, 4.99e-01]    [5.31e+00]
7000      [1.47e+01, 4.25e-01]    [1.40e+01, 4.25e-01]    [1.40e+01]
8000      [5.61e+00, 3.72e-01]    [5.75e+00, 3.72e-01]    [5.75e+00]
9000      [4.89e+00, 3.33e-01]    [5.61e+00, 3.33e-01]    [5.61e+00]
10000     [1.12e+01, 3.08e-01]    [1.19e+01, 3.08e-01]    [1.19e+01]
11000     [5.38e+00, 2.91e-01]    [5.73e+00, 2.91e-01]    [5.73e+00]
12000     [4.97e+00, 2.81e-01]    [5.42e+00, 2.81e-01]    [5.42e+00]
13000     [5.23e+00, 2.75e-01]    [5.60e+00, 2.75e-01]    [5.60e+00]
14000     [5.20e+00, 2.72e-01]    [5.45e+00, 2.72e-01]    [5.45e+00]
15000     [9.58e+00, 2.69e-01]    [9.08e+00, 2.69e-01]    [9.08e+00]
16000     [4.62e+00, 2.67e-01]    [5.45e+00, 2.67e-01]    [5.45e+00]
17000     [5.24e+00, 2.65e-01]    [5.95e+00, 2.65e-01]    [5.95e+00]
18000     [4.93e+00, 2.63e-01]    [5.36e+00, 2.63e-01]    [5.36e+00]
19000     [4.76e+00, 2.61e-01]    [5.25e+00, 2.61e-01]    [5.25e+00]
20000     [5.12e+00, 2.60e-01]    [5.88e+00, 2.60e-01]    [5.88e+00]
21000     [9.41e+00, 2.58e-01]    [8.92e+00, 2.58e-01]    [8.92e+00]
22000     [7.15e+00, 2.56e-01]    [6.88e+00, 2.56e-01]    [6.88e+00]
23000     [5.48e+01, 2.54e-01]    [5.52e+01, 2.54e-01]    [5.52e+01]
24000     [1.02e+01, 2.52e-01]    [1.10e+01, 2.52e-01]    [1.10e+01]
25000     [1.95e+01, 2.50e-01]    [1.92e+01, 2.50e-01]    [1.92e+01]
26000     [5.85e+00, 2.49e-01]    [5.93e+00, 2.49e-01]    [5.93e+00]
27000     [5.72e+00, 2.46e-01]    [6.56e+00, 2.46e-01]    [6.56e+00]
28000     [1.01e+01, 2.45e-01]    [1.10e+01, 2.45e-01]    [1.10e+01]
29000     [5.81e+00, 2.43e-01]    [5.85e+00, 2.43e-01]    [5.85e+00]
30000     [3.37e+01, 2.42e-01]    [3.37e+01, 2.42e-01]    [3.37e+01]

Best model at step 16000:
  train loss: 4.89e+00
  test loss: 5.71e+00
  test metric: [5.45e+00]

'train' took 18.912321 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 5
Compiling model...
Building feed-forward neural network...
'build' took 0.053558 s

'compile' took 0.498615 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss              Test loss               Test metric
0         [2.98e+04, 3.86e+00]    [3.03e+04, 3.86e+00]    [3.03e+04]
1000      [9.92e+00, 2.35e+00]    [1.09e+01, 2.35e+00]    [1.09e+01]
2000      [2.53e+01, 1.58e+00]    [2.43e+01, 1.58e+00]    [2.43e+01]
3000      [1.88e+01, 1.15e+00]    [2.06e+01, 1.15e+00]    [2.06e+01]
4000      [1.47e+01, 8.88e-01]    [1.71e+01, 8.88e-01]    [1.71e+01]
5000      [1.66e+01, 7.20e-01]    [1.85e+01, 7.20e-01]    [1.85e+01]
6000      [1.74e+01, 6.01e-01]    [1.91e+01, 6.01e-01]    [1.91e+01]
7000      [1.67e+01, 5.12e-01]    [1.84e+01, 5.12e-01]    [1.84e+01]
8000      [1.60e+01, 4.47e-01]    [1.78e+01, 4.47e-01]    [1.78e+01]
9000      [1.53e+01, 4.00e-01]    [1.72e+01, 4.00e-01]    [1.72e+01]
10000     [1.60e+01, 3.68e-01]    [1.77e+01, 3.68e-01]    [1.77e+01]
11000     [1.74e+01, 3.46e-01]    [1.89e+01, 3.46e-01]    [1.89e+01]
12000     [1.44e+01, 3.33e-01]    [1.63e+01, 3.33e-01]    [1.63e+01]
13000     [1.86e+01, 3.24e-01]    [2.00e+01, 3.24e-01]    [2.00e+01]
14000     [1.55e+01, 3.19e-01]    [1.70e+01, 3.19e-01]    [1.70e+01]
15000     [1.73e+01, 3.15e-01]    [1.65e+01, 3.15e-01]    [1.65e+01]
16000     [1.58e+01, 3.13e-01]    [1.52e+01, 3.13e-01]    [1.52e+01]
17000     [2.75e+01, 3.10e-01]    [2.70e+01, 3.10e-01]    [2.70e+01]
18000     [3.78e+01, 3.08e-01]    [3.75e+01, 3.08e-01]    [3.75e+01]
19000     [3.40e+01, 3.05e-01]    [3.56e+01, 3.05e-01]    [3.56e+01]
20000     [2.86e+01, 3.03e-01]    [2.81e+01, 3.03e-01]    [2.81e+01]
21000     [3.74e+01, 3.01e-01]    [3.70e+01, 3.01e-01]    [3.70e+01]
22000     [3.42e+01, 2.99e-01]    [3.57e+01, 2.99e-01]    [3.57e+01]
23000     [2.97e+01, 2.96e-01]    [2.92e+01, 2.96e-01]    [2.92e+01]
24000     [3.65e+01, 2.94e-01]    [3.61e+01, 2.94e-01]    [3.61e+01]
25000     [3.55e+01, 2.92e-01]    [3.71e+01, 2.92e-01]    [3.71e+01]
26000     [2.83e+01, 2.90e-01]    [2.78e+01, 2.90e-01]    [2.78e+01]
27000     [3.50e+01, 2.88e-01]    [3.46e+01, 2.88e-01]    [3.46e+01]
28000     [3.66e+01, 2.86e-01]    [3.82e+01, 2.86e-01]    [3.82e+01]
29000     [2.69e+01, 2.84e-01]    [2.64e+01, 2.84e-01]    [2.64e+01]
30000     [3.33e+01, 2.82e-01]    [3.29e+01, 2.82e-01]    [3.29e+01]

Best model at step 1000:
  train loss: 1.23e+01
  test loss: 1.32e+01
  test metric: [1.09e+01]

'train' took 21.770873 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 6
Compiling model...
Building feed-forward neural network...
'build' took 0.051447 s

'compile' took 0.547211 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss              Test loss               Test metric
0         [2.02e+04, 4.46e+00]    [2.05e+04, 4.46e+00]    [2.05e+04]
1000      [3.09e+01, 2.70e+00]    [3.55e+01, 2.70e+00]    [3.55e+01]
2000      [2.23e+01, 1.80e+00]    [2.59e+01, 1.80e+00]    [2.59e+01]
3000      [7.57e+01, 1.30e+00]    [7.33e+01, 1.30e+00]    [7.33e+01]
4000      [7.24e+01, 1.01e+00]    [7.54e+01, 1.01e+00]    [7.54e+01]
5000      [6.61e+01, 8.16e-01]    [6.55e+01, 8.16e-01]    [6.55e+01]
6000      [2.80e+01, 6.82e-01]    [3.02e+01, 6.82e-01]    [3.02e+01]
7000      [3.93e+01, 5.84e-01]    [4.17e+01, 5.84e-01]    [4.17e+01]
8000      [6.39e+01, 5.14e-01]    [6.32e+01, 5.14e-01]    [6.32e+01]
9000      [3.13e+01, 4.66e-01]    [3.37e+01, 4.66e-01]    [3.37e+01]
10000     [4.32e+01, 4.33e-01]    [4.58e+01, 4.33e-01]    [4.58e+01]
11000     [6.77e+01, 4.13e-01]    [6.70e+01, 4.13e-01]    [6.70e+01]
12000     [2.79e+01, 4.00e-01]    [3.03e+01, 4.00e-01]    [3.03e+01]
13000     [4.03e+01, 3.93e-01]    [4.29e+01, 3.93e-01]    [4.29e+01]
14000     [6.18e+01, 3.89e-01]    [6.10e+01, 3.89e-01]    [6.10e+01]
15000     [2.61e+01, 3.86e-01]    [2.88e+01, 3.86e-01]    [2.88e+01]
16000     [3.69e+01, 3.84e-01]    [3.95e+01, 3.84e-01]    [3.95e+01]
17000     [6.46e+01, 3.81e-01]    [6.38e+01, 3.81e-01]    [6.38e+01]
18000     [3.05e+01, 3.79e-01]    [3.30e+01, 3.79e-01]    [3.30e+01]
19000     [4.11e+01, 3.77e-01]    [4.38e+01, 3.77e-01]    [4.38e+01]
20000     [6.71e+01, 3.75e-01]    [6.62e+01, 3.75e-01]    [6.62e+01]
21000     [2.69e+01, 3.73e-01]    [2.98e+01, 3.73e-01]    [2.98e+01]
22000     [3.94e+01, 3.71e-01]    [4.21e+01, 3.71e-01]    [4.21e+01]
23000     [6.22e+01, 3.69e-01]    [6.12e+01, 3.69e-01]    [6.12e+01]
24000     [2.99e+01, 3.67e-01]    [3.25e+01, 3.67e-01]    [3.25e+01]
25000     [3.84e+01, 3.65e-01]    [4.12e+01, 3.65e-01]    [4.12e+01]
26000     [6.46e+01, 3.63e-01]    [6.36e+01, 3.63e-01]    [6.36e+01]
27000     [3.02e+01, 3.61e-01]    [3.29e+01, 3.61e-01]    [3.29e+01]
28000     [4.08e+01, 3.60e-01]    [4.37e+01, 3.60e-01]    [4.37e+01]
29000     [6.11e+01, 3.58e-01]    [5.99e+01, 3.58e-01]    [5.99e+01]
30000     [2.88e+01, 3.57e-01]    [3.19e+01, 3.57e-01]    [3.19e+01]

Best model at step 2000:
  train loss: 2.41e+01
  test loss: 2.77e+01
  test metric: [2.59e+01]

'train' took 20.426507 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 7
Compiling model...
Building feed-forward neural network...
'build' took 0.056305 s

'compile' took 0.610854 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss              Test loss               Test metric
0         [7.28e+03, 5.37e+00]    [7.41e+03, 5.37e+00]    [7.41e+03]
1000      [1.51e+01, 3.24e+00]    [1.38e+01, 3.24e+00]    [1.38e+01]
2000      [1.09e+01, 2.15e+00]    [1.10e+01, 2.15e+00]    [1.10e+01]
3000      [2.02e+01, 1.53e+00]    [2.09e+01, 1.53e+00]    [2.09e+01]
4000      [1.70e+01, 1.16e+00]    [1.77e+01, 1.16e+00]    [1.77e+01]
5000      [1.78e+01, 9.14e-01]    [1.85e+01, 9.14e-01]    [1.85e+01]
6000      [1.76e+01, 7.36e-01]    [1.82e+01, 7.36e-01]    [1.82e+01]
7000      [1.75e+01, 6.03e-01]    [1.81e+01, 6.03e-01]    [1.81e+01]
8000      [1.77e+01, 5.05e-01]    [1.83e+01, 5.05e-01]    [1.83e+01]
9000      [1.74e+01, 4.35e-01]    [1.79e+01, 4.35e-01]    [1.79e+01]
10000     [1.74e+01, 3.86e-01]    [1.79e+01, 3.86e-01]    [1.79e+01]
11000     [1.72e+01, 3.53e-01]    [1.77e+01, 3.53e-01]    [1.77e+01]
12000     [1.65e+01, 3.32e-01]    [1.69e+01, 3.32e-01]    [1.69e+01]
13000     [1.84e+01, 3.20e-01]    [1.88e+01, 3.20e-01]    [1.88e+01]
14000     [1.63e+01, 3.14e-01]    [1.66e+01, 3.14e-01]    [1.66e+01]
15000     [1.81e+01, 3.10e-01]    [1.85e+01, 3.10e-01]    [1.85e+01]
16000     [1.73e+01, 3.08e-01]    [1.77e+01, 3.08e-01]    [1.77e+01]
17000     [1.91e+01, 3.06e-01]    [1.95e+01, 3.06e-01]    [1.95e+01]
18000     [1.73e+01, 3.04e-01]    [1.76e+01, 3.04e-01]    [1.76e+01]
19000     [1.86e+01, 3.03e-01]    [1.90e+01, 3.03e-01]    [1.90e+01]
20000     [1.72e+01, 3.01e-01]    [1.75e+01, 3.01e-01]    [1.75e+01]
21000     [1.93e+01, 3.00e-01]    [1.96e+01, 3.00e-01]    [1.96e+01]
22000     [1.62e+01, 2.98e-01]    [1.65e+01, 2.98e-01]    [1.65e+01]
23000     [4.07e+01, 2.97e-01]    [4.14e+01, 2.97e-01]    [4.14e+01]
24000     [1.98e+01, 2.95e-01]    [2.02e+01, 2.95e-01]    [2.02e+01]
25000     [2.53e+01, 2.94e-01]    [2.57e+01, 2.94e-01]    [2.57e+01]
26000     [4.09e+01, 2.93e-01]    [4.16e+01, 2.93e-01]    [4.16e+01]
27000     [1.71e+01, 2.92e-01]    [1.73e+01, 2.92e-01]    [1.73e+01]
28000     [2.28e+01, 2.90e-01]    [2.32e+01, 2.90e-01]    [2.32e+01]
29000     [4.35e+01, 2.89e-01]    [4.44e+01, 2.89e-01]    [4.44e+01]
30000     [1.64e+01, 2.88e-01]    [1.65e+01, 2.88e-01]    [1.65e+01]

Best model at step 2000:
  train loss: 1.30e+01
  test loss: 1.31e+01
  test metric: [1.10e+01]

'train' took 20.951930 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 8
Compiling model...
Building feed-forward neural network...
'build' took 0.054092 s

'compile' took 0.637202 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss              Test loss               Test metric
0         [2.80e+04, 5.89e+00]    [2.85e+04, 5.89e+00]    [2.85e+04]
1000      [2.90e+01, 3.55e+00]    [2.91e+01, 3.55e+00]    [2.91e+01]
2000      [2.83e+01, 2.35e+00]    [2.84e+01, 2.35e+00]    [2.84e+01]
3000      [2.81e+01, 1.67e+00]    [2.81e+01, 1.67e+00]    [2.81e+01]
4000      [2.80e+01, 1.26e+00]    [2.80e+01, 1.26e+00]    [2.80e+01]
5000      [2.68e+01, 9.93e-01]    [2.68e+01, 9.93e-01]    [2.68e+01]
6000      [2.61e+01, 8.01e-01]    [2.60e+01, 8.01e-01]    [2.60e+01]
7000      [2.51e+01, 6.61e-01]    [2.50e+01, 6.61e-01]    [2.50e+01]
8000      [2.80e+01, 5.60e-01]    [2.80e+01, 5.60e-01]    [2.80e+01]
9000      [2.73e+01, 4.90e-01]    [2.72e+01, 4.90e-01]    [2.72e+01]
10000     [2.63e+01, 4.44e-01]    [2.62e+01, 4.44e-01]    [2.62e+01]
11000     [2.58e+01, 4.14e-01]    [2.57e+01, 4.14e-01]    [2.57e+01]
12000     [2.55e+01, 3.96e-01]    [2.54e+01, 3.96e-01]    [2.54e+01]
13000     [2.79e+01, 3.85e-01]    [2.78e+01, 3.85e-01]    [2.78e+01]
14000     [2.64e+01, 3.79e-01]    [2.62e+01, 3.79e-01]    [2.62e+01]
15000     [2.83e+01, 3.76e-01]    [2.81e+01, 3.76e-01]    [2.81e+01]
16000     [2.66e+01, 3.73e-01]    [2.65e+01, 3.73e-01]    [2.65e+01]
17000     [2.65e+01, 3.71e-01]    [2.63e+01, 3.71e-01]    [2.63e+01]
18000     [2.52e+01, 3.70e-01]    [2.50e+01, 3.70e-01]    [2.50e+01]
19000     [2.71e+01, 3.67e-01]    [2.69e+01, 3.67e-01]    [2.69e+01]
20000     [2.86e+01, 3.65e-01]    [2.84e+01, 3.65e-01]    [2.84e+01]
21000     [2.42e+01, 3.63e-01]    [2.40e+01, 3.63e-01]    [2.40e+01]
22000     [2.52e+01, 3.60e-01]    [2.49e+01, 3.60e-01]    [2.49e+01]
23000     [2.54e+01, 3.57e-01]    [2.52e+01, 3.57e-01]    [2.52e+01]
24000     [2.64e+01, 3.55e-01]    [2.61e+01, 3.55e-01]    [2.61e+01]
25000     [2.55e+01, 3.52e-01]    [2.52e+01, 3.52e-01]    [2.52e+01]
26000     [2.54e+01, 3.50e-01]    [2.52e+01, 3.50e-01]    [2.52e+01]
27000     [2.52e+01, 3.48e-01]    [2.49e+01, 3.48e-01]    [2.49e+01]
28000     [2.57e+01, 3.47e-01]    [2.54e+01, 3.47e-01]    [2.54e+01]
29000     [1.19e+01, 3.46e-01]    [1.15e+01, 3.46e-01]    [1.15e+01]
30000     [6.95e+01, 3.45e-01]    [7.08e+01, 3.45e-01]    [7.08e+01]

Best model at step 29000:
  train loss: 1.23e+01
  test loss: 1.19e+01
  test metric: [1.15e+01]

'train' took 22.865079 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 9
Compiling model...
Building feed-forward neural network...
'build' took 0.057325 s

'compile' took 0.718966 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss              Test loss               Test metric
0         [3.49e+04, 6.48e+00]    [3.55e+04, 6.48e+00]    [3.55e+04]
1000      [1.01e+01, 3.81e+00]    [9.60e+00, 3.81e+00]    [9.60e+00]
2000      [1.84e+01, 2.44e+00]    [1.83e+01, 2.44e+00]    [1.83e+01]
3000      [1.50e+01, 1.68e+00]    [1.48e+01, 1.68e+00]    [1.48e+01]
4000      [1.82e+01, 1.23e+00]    [1.82e+01, 1.23e+00]    [1.82e+01]
5000      [1.81e+01, 9.32e-01]    [1.81e+01, 9.32e-01]    [1.81e+01]
6000      [1.80e+01, 7.23e-01]    [1.80e+01, 7.23e-01]    [1.80e+01]
7000      [1.53e+01, 5.71e-01]    [1.53e+01, 5.71e-01]    [1.53e+01]
8000      [1.61e+01, 4.61e-01]    [1.62e+01, 4.61e-01]    [1.62e+01]
9000      [1.61e+01, 3.84e-01]    [1.62e+01, 3.84e-01]    [1.62e+01]
10000     [1.61e+01, 3.32e-01]    [1.62e+01, 3.32e-01]    [1.62e+01]
11000     [1.56e+01, 2.99e-01]    [1.57e+01, 2.99e-01]    [1.57e+01]
12000     [1.56e+01, 2.80e-01]    [1.58e+01, 2.80e-01]    [1.58e+01]
13000     [1.49e+01, 2.69e-01]    [1.51e+01, 2.69e-01]    [1.51e+01]
14000     [1.68e+01, 2.63e-01]    [1.70e+01, 2.63e-01]    [1.70e+01]
15000     [1.46e+01, 2.60e-01]    [1.47e+01, 2.60e-01]    [1.47e+01]
16000     [1.68e+01, 2.58e-01]    [1.70e+01, 2.58e-01]    [1.70e+01]
17000     [1.54e+01, 2.57e-01]    [1.56e+01, 2.57e-01]    [1.56e+01]
18000     [1.47e+01, 2.55e-01]    [1.48e+01, 2.55e-01]    [1.48e+01]
19000     [1.55e+01, 2.54e-01]    [1.57e+01, 2.54e-01]    [1.57e+01]
20000     [1.45e+01, 2.52e-01]    [1.47e+01, 2.52e-01]    [1.47e+01]
21000     [1.54e+01, 2.51e-01]    [1.57e+01, 2.51e-01]    [1.57e+01]
22000     [1.47e+01, 2.49e-01]    [1.49e+01, 2.49e-01]    [1.49e+01]
23000     [1.47e+01, 2.47e-01]    [1.49e+01, 2.47e-01]    [1.49e+01]
24000     [1.40e+01, 2.44e-01]    [1.42e+01, 2.44e-01]    [1.42e+01]
25000     [1.58e+01, 2.42e-01]    [1.60e+01, 2.42e-01]    [1.60e+01]
26000     [1.77e+01, 2.38e-01]    [1.80e+01, 2.38e-01]    [1.80e+01]
27000     [1.39e+01, 2.35e-01]    [1.42e+01, 2.35e-01]    [1.42e+01]
28000     [1.56e+01, 2.33e-01]    [1.59e+01, 2.33e-01]    [1.59e+01]
29000     [1.49e+01, 2.30e-01]    [1.53e+01, 2.30e-01]    [1.53e+01]
30000     [1.32e+01, 2.29e-01]    [1.35e+01, 2.29e-01]    [1.35e+01]

Best model at step 30000:
  train loss: 1.34e+01
  test loss: 1.37e+01
  test metric: [1.35e+01]

'train' took 23.234926 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...
Estar
11.627595 6.5404
              Case          n     E (GPa)   sy (GPa)     C (GPa)    dP/dh (N/m)      Wp/Wt    E* (GPa)      sy/E*
count    97.000000  97.000000   97.000000  97.000000   97.000000      97.000000  97.000000   97.000000  97.000000
mean    279.030928   0.213917  107.163804   1.327507   92.953350  128652.067967   0.731227  102.813375   0.013835
std     411.446469   0.178797   67.175628   1.126477   66.232911   85768.846959   0.134844   60.541899   0.009753
min       1.000000   0.000000   10.000000   0.030000    2.731300    9801.900000   0.451835   10.880844   0.001399
25%      37.000000   0.100000   50.000000   0.300000   39.269000   53194.000000   0.628612   52.343315   0.005508
50%      67.000000   0.177243  100.806000   1.000000   80.813000  123181.000000   0.740598  100.685905   0.011463
75%      91.000000   0.300000  170.000000   2.000000  144.840000  194391.000000   0.830543  159.806250   0.019105
max    1023.000000   0.500000  210.000000   5.298021  261.330000  336838.000000   0.971835  190.913667   0.038209
              Case          n     E (GPa)   sy (GPa)     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  14.000000   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499   1.436338   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848   1.652156   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000   0.036792    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500   0.303588   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000   0.985823   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000   1.719617   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000   5.298021  239.235773  326727.270700   0.971982
             Case     C (GPa)    dP/dh (N/m)       Wp/Wt     hm (um)        Pm (N)  E* (GPa)      sy (GPa)  s0.008 (GPa)  s0.015 (GPa)  s0.033 (GPa)
count  144.000000  144.000000     144.000000  144.000000  144.000000  1.440000e+02    144.00  1.440000e+02  1.440000e+02      144.0000    144.000000
mean    71.500000  137.712090  202749.537112    0.725860    0.256082  9.000019e-03    109.63  1.121000e+00  1.165600e+00        1.2001      1.234167
std     41.713307    9.399233    4178.310300    0.009606    0.008684  2.479224e-08      0.00  2.228196e-16  2.228196e-16        0.0000      0.000000
min      0.000000  101.595236  192856.539700    0.696906    0.223895  8.999961e-03    109.63  1.121000e+00  1.165600e+00        1.2001      1.234167
25%     35.750000  132.175084  199461.369750    0.718985    0.251139  9.000003e-03    109.63  1.121000e+00  1.165600e+00        1.2001      1.234167
50%     71.500000  136.682184  202962.030950    0.725867    0.256606  9.000022e-03    109.63  1.121000e+00  1.165600e+00        1.2001      1.234167
75%    107.250000  142.696850  205897.992300    0.732360    0.260943  9.000038e-03    109.63  1.121000e+00  1.165600e+00        1.2001      1.234167
max    143.000000  179.537847  212396.807300    0.754513    0.297635  9.000075e-03    109.63  1.121000e+00  1.165600e+00        1.2001      1.234167

Iteration: 0
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.077494 s

'compile' took 0.383110 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.01e+02, 9.99e+01, 2.67e+00]    [0.00e+00, 9.95e+01, 2.67e+00]    [9.95e+01, 9.95e+01, 2.88e-02, 2.88e-02]
1000      [7.41e+01, 6.69e+01, 2.88e+00]    [0.00e+00, 9.00e+01, 2.88e+00]    [9.01e+01, 9.00e+01, 2.53e-01, 2.54e-01]
2000      [2.40e+01, 1.93e+01, 4.18e+00]    [0.00e+00, 6.60e+00, 4.18e+00]    [1.03e+01, 6.60e+00, 2.08e+00, 2.04e+00]
3000      [4.17e+00, 1.66e+00, 4.55e+00]    [0.00e+00, 4.33e+01, 4.55e+00]    [3.91e+01, 4.33e+01, 3.18e+00, 3.27e+00]
4000      [3.96e+00, 9.37e-01, 4.46e+00]    [0.00e+00, 4.19e+01, 4.46e+00]    [3.94e+01, 4.19e+01, 2.94e+00, 2.99e+00]
5000      [3.68e+00, 7.86e-01, 4.39e+00]    [0.00e+00, 4.17e+01, 4.39e+00]    [4.01e+01, 4.17e+01, 2.92e+00, 2.95e+00]
6000      [3.55e+00, 6.98e-01, 4.33e+00]    [0.00e+00, 4.14e+01, 4.33e+00]    [4.03e+01, 4.14e+01, 2.93e+00, 2.95e+00]
7000      [3.45e+00, 6.30e-01, 4.27e+00]    [0.00e+00, 4.11e+01, 4.27e+00]    [4.05e+01, 4.11e+01, 2.94e+00, 2.95e+00]
8000      [3.39e+00, 5.74e-01, 4.22e+00]    [0.00e+00, 4.09e+01, 4.22e+00]    [4.05e+01, 4.09e+01, 2.94e+00, 2.94e+00]
9000      [3.35e+00, 5.77e-01, 4.17e+00]    [0.00e+00, 4.06e+01, 4.17e+00]    [4.05e+01, 4.06e+01, 2.93e+00, 2.93e+00]
10000     [3.32e+00, 5.77e-01, 4.12e+00]    [0.00e+00, 4.04e+01, 4.12e+00]    [4.04e+01, 4.04e+01, 2.93e+00, 2.93e+00]
11000     [3.25e+00, 4.95e-01, 4.08e+00]    [0.00e+00, 4.01e+01, 4.08e+00]    [4.04e+01, 4.01e+01, 2.92e+00, 2.91e+00]
12000     [3.21e+00, 4.75e-01, 4.04e+00]    [0.00e+00, 4.00e+01, 4.04e+00]    [4.04e+01, 4.00e+01, 2.91e+00, 2.90e+00]
13000     [3.18e+00, 4.03e-01, 4.00e+00]    [0.00e+00, 3.98e+01, 4.00e+00]    [4.04e+01, 3.98e+01, 2.91e+00, 2.90e+00]
14000     [3.16e+00, 3.95e-01, 3.97e+00]    [0.00e+00, 3.97e+01, 3.97e+00]    [4.04e+01, 3.97e+01, 2.91e+00, 2.89e+00]
15000     [3.13e+00, 4.03e-01, 3.93e+00]    [0.00e+00, 3.96e+01, 3.93e+00]    [4.05e+01, 3.96e+01, 2.91e+00, 2.89e+00]
16000     [3.09e+00, 3.43e-01, 3.90e+00]    [0.00e+00, 3.94e+01, 3.90e+00]    [4.05e+01, 3.94e+01, 2.91e+00, 2.88e+00]
17000     [3.08e+00, 3.70e-01, 3.87e+00]    [0.00e+00, 3.93e+01, 3.87e+00]    [4.05e+01, 3.93e+01, 2.91e+00, 2.88e+00]
18000     [3.07e+00, 3.63e-01, 3.84e+00]    [0.00e+00, 3.92e+01, 3.84e+00]    [4.05e+01, 3.92e+01, 2.91e+00, 2.88e+00]
19000     [3.05e+00, 3.51e-01, 3.81e+00]    [0.00e+00, 3.91e+01, 3.81e+00]    [4.05e+01, 3.91e+01, 2.91e+00, 2.89e+00]
20000     [3.02e+00, 3.01e-01, 3.77e+00]    [0.00e+00, 3.91e+01, 3.77e+00]    [4.05e+01, 3.91e+01, 2.92e+00, 2.89e+00]
21000     [3.02e+00, 3.34e-01, 3.75e+00]    [0.00e+00, 3.90e+01, 3.75e+00]    [4.04e+01, 3.90e+01, 2.92e+00, 2.89e+00]
22000     [3.00e+00, 2.78e-01, 3.72e+00]    [0.00e+00, 3.89e+01, 3.72e+00]    [4.04e+01, 3.89e+01, 2.92e+00, 2.89e+00]
23000     [2.99e+00, 3.12e-01, 3.69e+00]    [0.00e+00, 3.88e+01, 3.69e+00]    [4.04e+01, 3.88e+01, 2.93e+00, 2.90e+00]
24000     [2.97e+00, 2.91e-01, 3.67e+00]    [0.00e+00, 3.88e+01, 3.67e+00]    [4.03e+01, 3.88e+01, 2.93e+00, 2.89e+00]
25000     [2.97e+00, 2.71e-01, 3.64e+00]    [0.00e+00, 3.87e+01, 3.64e+00]    [4.03e+01, 3.87e+01, 2.93e+00, 2.90e+00]
26000     [2.97e+00, 3.18e-01, 3.62e+00]    [0.00e+00, 3.86e+01, 3.62e+00]    [4.03e+01, 3.86e+01, 2.93e+00, 2.90e+00]
27000     [2.94e+00, 2.78e-01, 3.60e+00]    [0.00e+00, 3.86e+01, 3.60e+00]    [4.03e+01, 3.86e+01, 2.94e+00, 2.90e+00]
28000     [2.99e+00, 3.12e-01, 3.58e+00]    [0.00e+00, 3.85e+01, 3.58e+00]    [4.03e+01, 3.85e+01, 2.94e+00, 2.91e+00]
29000     [2.91e+00, 2.38e-01, 3.55e+00]    [0.00e+00, 3.84e+01, 3.55e+00]    [4.02e+01, 3.84e+01, 2.94e+00, 2.90e+00]
30000     [2.90e+00, 2.18e-01, 3.53e+00]    [0.00e+00, 3.84e+01, 3.53e+00]    [4.02e+01, 3.84e+01, 2.94e+00, 2.91e+00]

Best model at step 30000:
  train loss: 6.65e+00
  test loss: 4.19e+01
  test metric: [4.02e+01, 3.84e+01, 2.94e+00, 2.91e+00]

'train' took 38.916102 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 1
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.078020 s

'compile' took 0.379777 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [9.86e+01, 9.89e+01, 2.82e+00]    [0.00e+00, 1.00e+02, 2.82e+00]    [1.00e+02, 1.00e+02, 6.08e-02, 6.08e-02]
1000      [7.40e+01, 6.57e+01, 3.06e+00]    [0.00e+00, 8.86e+01, 3.06e+00]    [8.88e+01, 8.86e+01, 3.99e-01, 4.05e-01]
2000      [1.81e+01, 1.32e+01, 4.54e+00]    [0.00e+00, 1.50e+01, 4.54e+00]    [7.46e+00, 1.50e+01, 3.60e+00, 4.14e+00]
3000      [4.37e+00, 1.74e+00, 4.80e+00]    [0.00e+00, 4.42e+01, 4.80e+00]    [4.02e+01, 4.42e+01, 2.66e+00, 2.74e+00]
4000      [3.86e+00, 1.19e+00, 4.73e+00]    [0.00e+00, 4.33e+01, 4.73e+00]    [4.20e+01, 4.33e+01, 2.54e+00, 2.56e+00]
5000      [3.65e+00, 1.02e+00, 4.66e+00]    [0.00e+00, 4.30e+01, 4.66e+00]    [4.26e+01, 4.30e+01, 2.55e+00, 2.56e+00]
6000      [3.52e+00, 9.44e-01, 4.60e+00]    [0.00e+00, 4.29e+01, 4.60e+00]    [4.32e+01, 4.29e+01, 2.56e+00, 2.56e+00]
7000      [3.43e+00, 8.47e-01, 4.53e+00]    [0.00e+00, 4.29e+01, 4.53e+00]    [4.37e+01, 4.29e+01, 2.57e+00, 2.56e+00]
8000      [3.35e+00, 7.95e-01, 4.47e+00]    [0.00e+00, 4.29e+01, 4.47e+00]    [4.41e+01, 4.29e+01, 2.58e+00, 2.56e+00]
9000      [3.30e+00, 8.28e-01, 4.41e+00]    [0.00e+00, 4.28e+01, 4.41e+00]    [4.41e+01, 4.28e+01, 2.57e+00, 2.55e+00]
10000     [3.26e+00, 7.77e-01, 4.35e+00]    [0.00e+00, 4.25e+01, 4.35e+00]    [4.40e+01, 4.25e+01, 2.57e+00, 2.54e+00]
11000     [3.21e+00, 7.66e-01, 4.30e+00]    [0.00e+00, 4.24e+01, 4.30e+00]    [4.39e+01, 4.24e+01, 2.57e+00, 2.54e+00]
12000     [3.17e+00, 7.80e-01, 4.25e+00]    [0.00e+00, 4.22e+01, 4.25e+00]    [4.39e+01, 4.22e+01, 2.56e+00, 2.53e+00]
13000     [3.12e+00, 7.31e-01, 4.20e+00]    [0.00e+00, 4.20e+01, 4.20e+00]    [4.38e+01, 4.20e+01, 2.56e+00, 2.53e+00]
14000     [3.10e+00, 7.71e-01, 4.16e+00]    [0.00e+00, 4.19e+01, 4.16e+00]    [4.38e+01, 4.19e+01, 2.56e+00, 2.52e+00]
15000     [3.08e+00, 7.45e-01, 4.12e+00]    [0.00e+00, 4.17e+01, 4.12e+00]    [4.37e+01, 4.17e+01, 2.55e+00, 2.52e+00]
16000     [3.03e+00, 7.02e-01, 4.08e+00]    [0.00e+00, 4.15e+01, 4.08e+00]    [4.36e+01, 4.15e+01, 2.56e+00, 2.52e+00]
17000     [3.00e+00, 6.94e-01, 4.04e+00]    [0.00e+00, 4.14e+01, 4.04e+00]    [4.35e+01, 4.14e+01, 2.55e+00, 2.52e+00]
18000     [2.99e+00, 6.70e-01, 4.01e+00]    [0.00e+00, 4.12e+01, 4.01e+00]    [4.34e+01, 4.12e+01, 2.56e+00, 2.52e+00]
19000     [3.01e+00, 7.67e-01, 3.97e+00]    [0.00e+00, 4.11e+01, 3.97e+00]    [4.34e+01, 4.11e+01, 2.56e+00, 2.52e+00]
20000     [2.96e+00, 6.51e-01, 3.94e+00]    [0.00e+00, 4.11e+01, 3.94e+00]    [4.33e+01, 4.11e+01, 2.56e+00, 2.52e+00]
21000     [2.94e+00, 6.56e-01, 3.90e+00]    [0.00e+00, 4.10e+01, 3.90e+00]    [4.33e+01, 4.10e+01, 2.56e+00, 2.52e+00]
22000     [2.93e+00, 6.92e-01, 3.87e+00]    [0.00e+00, 4.09e+01, 3.87e+00]    [4.32e+01, 4.09e+01, 2.56e+00, 2.52e+00]
23000     [2.91e+00, 6.54e-01, 3.84e+00]    [0.00e+00, 4.08e+01, 3.84e+00]    [4.32e+01, 4.08e+01, 2.55e+00, 2.51e+00]
24000     [2.90e+00, 6.70e-01, 3.81e+00]    [0.00e+00, 4.08e+01, 3.81e+00]    [4.32e+01, 4.08e+01, 2.56e+00, 2.51e+00]
25000     [2.87e+00, 5.83e-01, 3.78e+00]    [0.00e+00, 4.07e+01, 3.78e+00]    [4.31e+01, 4.07e+01, 2.55e+00, 2.51e+00]
26000     [2.86e+00, 5.95e-01, 3.76e+00]    [0.00e+00, 4.07e+01, 3.76e+00]    [4.31e+01, 4.07e+01, 2.55e+00, 2.50e+00]
27000     [2.84e+00, 5.66e-01, 3.73e+00]    [0.00e+00, 4.07e+01, 3.73e+00]    [4.31e+01, 4.07e+01, 2.55e+00, 2.50e+00]
28000     [2.85e+00, 6.02e-01, 3.70e+00]    [0.00e+00, 4.06e+01, 3.70e+00]    [4.31e+01, 4.06e+01, 2.54e+00, 2.50e+00]
29000     [2.83e+00, 5.74e-01, 3.68e+00]    [0.00e+00, 4.06e+01, 3.68e+00]    [4.31e+01, 4.06e+01, 2.54e+00, 2.49e+00]
30000     [2.83e+00, 5.84e-01, 3.66e+00]    [0.00e+00, 4.06e+01, 3.66e+00]    [4.31e+01, 4.06e+01, 2.53e+00, 2.49e+00]

Best model at step 30000:
  train loss: 7.07e+00
  test loss: 4.43e+01
  test metric: [4.31e+01, 4.06e+01, 2.53e+00, 2.49e+00]

'train' took 38.047798 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 2
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.079282 s

'compile' took 0.379735 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.01e+02, 1.00e+02, 2.76e+00]    [0.00e+00, 1.00e+02, 2.76e+00]    [1.00e+02, 1.00e+02, 4.40e-02, 4.40e-02]
1000      [7.60e+01, 7.07e+01, 2.85e+00]    [0.00e+00, 9.50e+01, 2.85e+00]    [9.51e+01, 9.50e+01, 4.14e-01, 4.17e-01]
2000      [2.55e+01, 2.08e+01, 4.16e+00]    [0.00e+00, 1.58e+01, 4.16e+00]    [1.93e+01, 1.58e+01, 3.58e+00, 3.73e+00]
3000      [5.05e+00, 2.22e+00, 4.48e+00]    [0.00e+00, 3.69e+01, 4.48e+00]    [3.35e+01, 3.69e+01, 4.27e+00, 4.38e+00]
4000      [4.46e+00, 1.22e+00, 4.41e+00]    [0.00e+00, 3.73e+01, 4.41e+00]    [3.54e+01, 3.73e+01, 3.86e+00, 3.92e+00]
5000      [4.05e+00, 9.46e-01, 4.35e+00]    [0.00e+00, 3.82e+01, 4.35e+00]    [3.70e+01, 3.82e+01, 3.80e+00, 3.84e+00]
6000      [3.76e+00, 7.84e-01, 4.29e+00]    [0.00e+00, 3.83e+01, 4.29e+00]    [3.77e+01, 3.83e+01, 3.77e+00, 3.79e+00]
7000      [3.62e+00, 7.94e-01, 4.24e+00]    [0.00e+00, 3.81e+01, 4.24e+00]    [3.77e+01, 3.81e+01, 3.77e+00, 3.78e+00]
8000      [3.50e+00, 6.68e-01, 4.19e+00]    [0.00e+00, 3.80e+01, 4.19e+00]    [3.78e+01, 3.80e+01, 3.77e+00, 3.78e+00]
9000      [3.40e+00, 5.87e-01, 4.14e+00]    [0.00e+00, 3.78e+01, 4.14e+00]    [3.78e+01, 3.78e+01, 3.78e+00, 3.78e+00]
10000     [3.35e+00, 5.52e-01, 4.10e+00]    [0.00e+00, 3.78e+01, 4.10e+00]    [3.78e+01, 3.78e+01, 3.78e+00, 3.78e+00]
11000     [3.33e+00, 5.38e-01, 4.07e+00]    [0.00e+00, 3.76e+01, 4.07e+00]    [3.78e+01, 3.76e+01, 3.78e+00, 3.78e+00]
12000     [3.26e+00, 5.24e-01, 4.03e+00]    [0.00e+00, 3.74e+01, 4.03e+00]    [3.77e+01, 3.74e+01, 3.79e+00, 3.79e+00]
13000     [3.24e+00, 5.62e-01, 4.00e+00]    [0.00e+00, 3.73e+01, 4.00e+00]    [3.76e+01, 3.73e+01, 3.80e+00, 3.79e+00]
14000     [3.17e+00, 4.74e-01, 3.97e+00]    [0.00e+00, 3.71e+01, 3.97e+00]    [3.75e+01, 3.71e+01, 3.81e+00, 3.80e+00]
15000     [3.14e+00, 4.86e-01, 3.94e+00]    [0.00e+00, 3.70e+01, 3.94e+00]    [3.75e+01, 3.70e+01, 3.82e+00, 3.81e+00]
16000     [3.11e+00, 4.36e-01, 3.92e+00]    [0.00e+00, 3.69e+01, 3.92e+00]    [3.74e+01, 3.69e+01, 3.82e+00, 3.81e+00]
17000     [3.11e+00, 4.68e-01, 3.89e+00]    [0.00e+00, 3.68e+01, 3.89e+00]    [3.73e+01, 3.68e+01, 3.82e+00, 3.81e+00]
18000     [3.04e+00, 3.82e-01, 3.86e+00]    [0.00e+00, 3.67e+01, 3.86e+00]    [3.73e+01, 3.67e+01, 3.83e+00, 3.82e+00]
19000     [3.05e+00, 4.39e-01, 3.84e+00]    [0.00e+00, 3.66e+01, 3.84e+00]    [3.72e+01, 3.66e+01, 3.84e+00, 3.82e+00]
20000     [3.00e+00, 3.55e-01, 3.82e+00]    [0.00e+00, 3.65e+01, 3.82e+00]    [3.72e+01, 3.65e+01, 3.85e+00, 3.83e+00]
21000     [3.02e+00, 4.44e-01, 3.80e+00]    [0.00e+00, 3.64e+01, 3.80e+00]    [3.72e+01, 3.64e+01, 3.86e+00, 3.84e+00]
22000     [2.98e+00, 3.55e-01, 3.77e+00]    [0.00e+00, 3.63e+01, 3.77e+00]    [3.71e+01, 3.63e+01, 3.86e+00, 3.84e+00]
23000     [2.96e+00, 3.75e-01, 3.75e+00]    [0.00e+00, 3.62e+01, 3.75e+00]    [3.71e+01, 3.62e+01, 3.87e+00, 3.85e+00]
24000     [2.95e+00, 3.44e-01, 3.73e+00]    [0.00e+00, 3.61e+01, 3.73e+00]    [3.71e+01, 3.61e+01, 3.88e+00, 3.86e+00]
25000     [2.91e+00, 3.18e-01, 3.71e+00]    [0.00e+00, 3.60e+01, 3.71e+00]    [3.70e+01, 3.60e+01, 3.89e+00, 3.86e+00]
26000     [2.88e+00, 2.66e-01, 3.70e+00]    [0.00e+00, 3.59e+01, 3.70e+00]    [3.70e+01, 3.59e+01, 3.90e+00, 3.87e+00]
27000     [2.89e+00, 2.93e-01, 3.68e+00]    [0.00e+00, 3.58e+01, 3.68e+00]    [3.69e+01, 3.58e+01, 3.91e+00, 3.88e+00]
28000     [2.86e+00, 2.92e-01, 3.66e+00]    [0.00e+00, 3.58e+01, 3.66e+00]    [3.69e+01, 3.58e+01, 3.92e+00, 3.89e+00]
29000     [2.83e+00, 2.62e-01, 3.64e+00]    [0.00e+00, 3.57e+01, 3.64e+00]    [3.69e+01, 3.57e+01, 3.93e+00, 3.90e+00]
30000     [2.83e+00, 2.66e-01, 3.62e+00]    [0.00e+00, 3.56e+01, 3.62e+00]    [3.69e+01, 3.56e+01, 3.94e+00, 3.91e+00]

Best model at step 30000:
  train loss: 6.72e+00
  test loss: 3.92e+01
  test metric: [3.69e+01, 3.56e+01, 3.94e+00, 3.91e+00]

'train' took 38.143710 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 3
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.077479 s

'compile' took 0.372017 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [9.92e+01, 9.94e+01, 2.72e+00]    [0.00e+00, 1.00e+02, 2.72e+00]    [1.00e+02, 1.00e+02, 4.83e-02, 4.83e-02]
1000      [7.53e+01, 7.08e+01, 2.84e+00]    [0.00e+00, 9.16e+01, 2.84e+00]    [9.17e+01, 9.16e+01, 3.28e-01, 3.32e-01]
2000      [2.55e+01, 1.91e+01, 4.13e+00]    [0.00e+00, 5.55e+00, 4.13e+00]    [1.16e+01, 5.55e+00, 2.81e+00, 2.71e+00]
3000      [4.88e+00, 1.69e+00, 4.52e+00]    [0.00e+00, 4.30e+01, 4.52e+00]    [3.75e+01, 4.30e+01, 3.27e+00, 3.40e+00]
4000      [3.95e+00, 1.18e+00, 4.46e+00]    [0.00e+00, 4.24e+01, 4.46e+00]    [4.01e+01, 4.24e+01, 3.16e+00, 3.21e+00]
5000      [3.70e+00, 1.03e+00, 4.40e+00]    [0.00e+00, 4.11e+01, 4.40e+00]    [4.04e+01, 4.11e+01, 3.07e+00, 3.09e+00]
6000      [3.58e+00, 9.70e-01, 4.34e+00]    [0.00e+00, 4.04e+01, 4.34e+00]    [4.05e+01, 4.04e+01, 3.04e+00, 3.04e+00]
7000      [3.48e+00, 9.44e-01, 4.28e+00]    [0.00e+00, 4.01e+01, 4.28e+00]    [4.07e+01, 4.01e+01, 3.06e+00, 3.04e+00]
8000      [3.40e+00, 9.19e-01, 4.23e+00]    [0.00e+00, 3.98e+01, 4.23e+00]    [4.09e+01, 3.98e+01, 3.08e+00, 3.05e+00]
9000      [3.34e+00, 8.65e-01, 4.18e+00]    [0.00e+00, 3.97e+01, 4.18e+00]    [4.10e+01, 3.97e+01, 3.09e+00, 3.06e+00]
10000     [3.29e+00, 8.68e-01, 4.14e+00]    [0.00e+00, 3.95e+01, 4.14e+00]    [4.10e+01, 3.95e+01, 3.10e+00, 3.07e+00]
11000     [3.29e+00, 9.18e-01, 4.09e+00]    [0.00e+00, 3.93e+01, 4.09e+00]    [4.10e+01, 3.93e+01, 3.12e+00, 3.08e+00]
12000     [3.23e+00, 8.29e-01, 4.05e+00]    [0.00e+00, 3.90e+01, 4.05e+00]    [4.08e+01, 3.90e+01, 3.13e+00, 3.09e+00]
13000     [3.20e+00, 8.32e-01, 4.01e+00]    [0.00e+00, 3.87e+01, 4.01e+00]    [4.06e+01, 3.87e+01, 3.14e+00, 3.10e+00]
14000     [3.16e+00, 8.11e-01, 3.97e+00]    [0.00e+00, 3.85e+01, 3.97e+00]    [4.04e+01, 3.85e+01, 3.15e+00, 3.11e+00]
15000     [3.18e+00, 8.08e-01, 3.94e+00]    [0.00e+00, 3.83e+01, 3.94e+00]    [4.02e+01, 3.83e+01, 3.15e+00, 3.11e+00]
16000     [3.12e+00, 7.63e-01, 3.90e+00]    [0.00e+00, 3.81e+01, 3.90e+00]    [4.00e+01, 3.81e+01, 3.15e+00, 3.11e+00]
17000     [3.09e+00, 7.46e-01, 3.87e+00]    [0.00e+00, 3.79e+01, 3.87e+00]    [3.98e+01, 3.79e+01, 3.16e+00, 3.11e+00]
18000     [3.08e+00, 7.46e-01, 3.83e+00]    [0.00e+00, 3.77e+01, 3.83e+00]    [3.96e+01, 3.77e+01, 3.16e+00, 3.11e+00]
19000     [3.06e+00, 7.58e-01, 3.80e+00]    [0.00e+00, 3.76e+01, 3.80e+00]    [3.95e+01, 3.76e+01, 3.16e+00, 3.12e+00]
20000     [3.05e+00, 7.10e-01, 3.77e+00]    [0.00e+00, 3.75e+01, 3.77e+00]    [3.95e+01, 3.75e+01, 3.16e+00, 3.12e+00]
21000     [3.01e+00, 7.11e-01, 3.74e+00]    [0.00e+00, 3.74e+01, 3.74e+00]    [3.93e+01, 3.74e+01, 3.16e+00, 3.12e+00]
22000     [3.02e+00, 6.90e-01, 3.72e+00]    [0.00e+00, 3.73e+01, 3.72e+00]    [3.93e+01, 3.73e+01, 3.17e+00, 3.12e+00]
23000     [2.99e+00, 6.85e-01, 3.69e+00]    [0.00e+00, 3.72e+01, 3.69e+00]    [3.92e+01, 3.72e+01, 3.17e+00, 3.13e+00]
24000     [2.97e+00, 6.46e-01, 3.66e+00]    [0.00e+00, 3.72e+01, 3.66e+00]    [3.92e+01, 3.72e+01, 3.18e+00, 3.14e+00]
25000     [2.99e+00, 6.92e-01, 3.64e+00]    [0.00e+00, 3.70e+01, 3.64e+00]    [3.90e+01, 3.70e+01, 3.19e+00, 3.14e+00]
26000     [2.96e+00, 6.71e-01, 3.61e+00]    [0.00e+00, 3.70e+01, 3.61e+00]    [3.90e+01, 3.70e+01, 3.20e+00, 3.15e+00]
27000     [2.94e+00, 6.25e-01, 3.59e+00]    [0.00e+00, 3.68e+01, 3.59e+00]    [3.89e+01, 3.68e+01, 3.21e+00, 3.16e+00]
28000     [2.94e+00, 6.36e-01, 3.57e+00]    [0.00e+00, 3.67e+01, 3.57e+00]    [3.88e+01, 3.67e+01, 3.22e+00, 3.17e+00]
29000     [2.94e+00, 6.07e-01, 3.54e+00]    [0.00e+00, 3.67e+01, 3.54e+00]    [3.87e+01, 3.67e+01, 3.23e+00, 3.18e+00]
30000     [2.90e+00, 5.84e-01, 3.52e+00]    [0.00e+00, 3.66e+01, 3.52e+00]    [3.86e+01, 3.66e+01, 3.24e+00, 3.19e+00]

Best model at step 30000:
  train loss: 7.01e+00
  test loss: 4.01e+01
  test metric: [3.86e+01, 3.66e+01, 3.24e+00, 3.19e+00]

'train' took 38.810350 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 4
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.079267 s

'compile' took 0.376252 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.01e+02, 9.98e+01, 2.77e+00]    [0.00e+00, 9.97e+01, 2.77e+00]    [9.97e+01, 9.97e+01, 6.38e-02, 6.38e-02]
1000      [7.50e+01, 6.73e+01, 2.96e+00]    [0.00e+00, 9.22e+01, 2.96e+00]    [9.23e+01, 9.22e+01, 8.09e-01, 8.14e-01]
2000      [2.58e+01, 2.08e+01, 4.10e+00]    [0.00e+00, 8.26e+00, 4.10e+00]    [1.33e+01, 8.26e+00, 3.91e+00, 3.86e+00]
3000      [5.25e+00, 2.11e+00, 4.45e+00]    [0.00e+00, 3.95e+01, 4.45e+00]    [3.50e+01, 3.95e+01, 3.49e+00, 3.61e+00]
4000      [4.42e+00, 1.21e+00, 4.40e+00]    [0.00e+00, 3.95e+01, 4.40e+00]    [3.76e+01, 3.95e+01, 3.16e+00, 3.20e+00]
5000      [3.98e+00, 9.61e-01, 4.36e+00]    [0.00e+00, 3.97e+01, 4.36e+00]    [3.89e+01, 3.97e+01, 3.14e+00, 3.15e+00]
6000      [3.68e+00, 8.42e-01, 4.31e+00]    [0.00e+00, 3.95e+01, 4.31e+00]    [3.92e+01, 3.95e+01, 3.14e+00, 3.14e+00]
7000      [3.51e+00, 8.19e-01, 4.27e+00]    [0.00e+00, 3.93e+01, 4.27e+00]    [3.92e+01, 3.93e+01, 3.15e+00, 3.15e+00]
8000      [3.47e+00, 8.05e-01, 4.23e+00]    [0.00e+00, 3.91e+01, 4.23e+00]    [3.92e+01, 3.91e+01, 3.19e+00, 3.18e+00]
9000      [3.35e+00, 6.50e-01, 4.20e+00]    [0.00e+00, 3.90e+01, 4.20e+00]    [3.92e+01, 3.90e+01, 3.23e+00, 3.22e+00]
10000     [3.30e+00, 6.11e-01, 4.16e+00]    [0.00e+00, 3.89e+01, 4.16e+00]    [3.93e+01, 3.89e+01, 3.26e+00, 3.25e+00]
11000     [3.28e+00, 6.26e-01, 4.13e+00]    [0.00e+00, 3.88e+01, 4.13e+00]    [3.92e+01, 3.88e+01, 3.28e+00, 3.27e+00]
12000     [3.22e+00, 5.31e-01, 4.10e+00]    [0.00e+00, 3.87e+01, 4.10e+00]    [3.92e+01, 3.87e+01, 3.29e+00, 3.28e+00]
13000     [3.20e+00, 5.28e-01, 4.07e+00]    [0.00e+00, 3.86e+01, 4.07e+00]    [3.92e+01, 3.86e+01, 3.31e+00, 3.29e+00]
14000     [3.18e+00, 5.39e-01, 4.03e+00]    [0.00e+00, 3.85e+01, 4.03e+00]    [3.92e+01, 3.85e+01, 3.32e+00, 3.31e+00]
15000     [3.12e+00, 4.78e-01, 4.01e+00]    [0.00e+00, 3.85e+01, 4.01e+00]    [3.92e+01, 3.85e+01, 3.33e+00, 3.32e+00]
16000     [3.09e+00, 4.51e-01, 3.98e+00]    [0.00e+00, 3.84e+01, 3.98e+00]    [3.92e+01, 3.84e+01, 3.35e+00, 3.33e+00]
17000     [3.10e+00, 4.82e-01, 3.95e+00]    [0.00e+00, 3.83e+01, 3.95e+00]    [3.91e+01, 3.83e+01, 3.36e+00, 3.34e+00]
18000     [3.06e+00, 4.53e-01, 3.93e+00]    [0.00e+00, 3.81e+01, 3.93e+00]    [3.90e+01, 3.81e+01, 3.37e+00, 3.35e+00]
19000     [3.02e+00, 4.06e-01, 3.90e+00]    [0.00e+00, 3.80e+01, 3.90e+00]    [3.89e+01, 3.80e+01, 3.38e+00, 3.36e+00]
20000     [2.98e+00, 4.03e-01, 3.88e+00]    [0.00e+00, 3.79e+01, 3.88e+00]    [3.89e+01, 3.79e+01, 3.40e+00, 3.37e+00]
21000     [2.99e+00, 3.99e-01, 3.86e+00]    [0.00e+00, 3.78e+01, 3.86e+00]    [3.89e+01, 3.78e+01, 3.41e+00, 3.38e+00]
22000     [3.00e+00, 4.69e-01, 3.84e+00]    [0.00e+00, 3.76e+01, 3.84e+00]    [3.88e+01, 3.76e+01, 3.42e+00, 3.39e+00]
23000     [2.91e+00, 3.30e-01, 3.82e+00]    [0.00e+00, 3.75e+01, 3.82e+00]    [3.88e+01, 3.75e+01, 3.44e+00, 3.40e+00]
24000     [2.90e+00, 3.21e-01, 3.80e+00]    [0.00e+00, 3.74e+01, 3.80e+00]    [3.88e+01, 3.74e+01, 3.45e+00, 3.42e+00]
25000     [2.87e+00, 3.09e-01, 3.78e+00]    [0.00e+00, 3.73e+01, 3.78e+00]    [3.87e+01, 3.73e+01, 3.47e+00, 3.43e+00]
26000     [2.87e+00, 3.50e-01, 3.76e+00]    [0.00e+00, 3.72e+01, 3.76e+00]    [3.86e+01, 3.72e+01, 3.48e+00, 3.44e+00]
27000     [2.84e+00, 2.97e-01, 3.74e+00]    [0.00e+00, 3.70e+01, 3.74e+00]    [3.86e+01, 3.70e+01, 3.49e+00, 3.45e+00]
28000     [2.88e+00, 3.71e-01, 3.72e+00]    [0.00e+00, 3.69e+01, 3.72e+00]    [3.85e+01, 3.69e+01, 3.50e+00, 3.46e+00]
29000     [2.79e+00, 2.57e-01, 3.70e+00]    [0.00e+00, 3.68e+01, 3.70e+00]    [3.85e+01, 3.68e+01, 3.52e+00, 3.48e+00]
30000     [2.82e+00, 2.51e-01, 3.68e+00]    [0.00e+00, 3.67e+01, 3.68e+00]    [3.84e+01, 3.67e+01, 3.53e+00, 3.49e+00]

Best model at step 29000:
  train loss: 6.75e+00
  test loss: 4.05e+01
  test metric: [3.85e+01, 3.68e+01, 3.52e+00, 3.48e+00]

'train' took 37.443015 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 5
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.077992 s

'compile' took 0.374286 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.00e+02, 1.00e+02, 2.83e+00]    [0.00e+00, 9.95e+01, 2.83e+00]    [9.95e+01, 9.95e+01, 6.62e-02, 6.62e-02]
1000      [7.47e+01, 6.75e+01, 2.98e+00]    [0.00e+00, 9.13e+01, 2.98e+00]    [9.13e+01, 9.13e+01, 3.22e-01, 3.22e-01]
2000      [2.75e+01, 2.27e+01, 4.12e+00]    [0.00e+00, 1.76e+01, 4.12e+00]    [1.96e+01, 1.76e+01, 3.85e+00, 3.94e+00]
3000      [4.94e+00, 1.87e+00, 4.57e+00]    [0.00e+00, 3.67e+01, 4.57e+00]    [3.45e+01, 3.67e+01, 3.55e+00, 3.60e+00]
4000      [4.10e+00, 1.39e+00, 4.49e+00]    [0.00e+00, 3.79e+01, 4.49e+00]    [3.72e+01, 3.79e+01, 3.41e+00, 3.42e+00]
5000      [3.67e+00, 1.12e+00, 4.42e+00]    [0.00e+00, 3.86e+01, 4.42e+00]    [3.86e+01, 3.86e+01, 3.36e+00, 3.36e+00]
6000      [3.46e+00, 1.05e+00, 4.37e+00]    [0.00e+00, 3.89e+01, 4.37e+00]    [3.94e+01, 3.89e+01, 3.33e+00, 3.32e+00]
7000      [3.35e+00, 9.12e-01, 4.32e+00]    [0.00e+00, 3.85e+01, 4.32e+00]    [3.94e+01, 3.85e+01, 3.29e+00, 3.27e+00]
8000      [3.28e+00, 8.69e-01, 4.27e+00]    [0.00e+00, 3.79e+01, 4.27e+00]    [3.92e+01, 3.79e+01, 3.26e+00, 3.23e+00]
9000      [3.19e+00, 7.35e-01, 4.23e+00]    [0.00e+00, 3.75e+01, 4.23e+00]    [3.91e+01, 3.75e+01, 3.25e+00, 3.21e+00]
10000     [3.14e+00, 7.22e-01, 4.19e+00]    [0.00e+00, 3.73e+01, 4.19e+00]    [3.92e+01, 3.73e+01, 3.26e+00, 3.21e+00]
11000     [3.09e+00, 6.82e-01, 4.15e+00]    [0.00e+00, 3.70e+01, 4.15e+00]    [3.90e+01, 3.70e+01, 3.26e+00, 3.21e+00]
12000     [3.06e+00, 6.45e-01, 4.11e+00]    [0.00e+00, 3.68e+01, 4.11e+00]    [3.90e+01, 3.68e+01, 3.26e+00, 3.21e+00]
13000     [3.08e+00, 7.23e-01, 4.07e+00]    [0.00e+00, 3.68e+01, 4.07e+00]    [3.90e+01, 3.68e+01, 3.26e+00, 3.21e+00]
14000     [2.98e+00, 6.07e-01, 4.04e+00]    [0.00e+00, 3.67e+01, 4.04e+00]    [3.90e+01, 3.67e+01, 3.27e+00, 3.21e+00]
15000     [2.99e+00, 7.34e-01, 4.00e+00]    [0.00e+00, 3.66e+01, 4.00e+00]    [3.91e+01, 3.66e+01, 3.27e+00, 3.22e+00]
16000     [2.95e+00, 6.21e-01, 3.97e+00]    [0.00e+00, 3.65e+01, 3.97e+00]    [3.91e+01, 3.65e+01, 3.28e+00, 3.22e+00]
17000     [2.95e+00, 6.32e-01, 3.94e+00]    [0.00e+00, 3.65e+01, 3.94e+00]    [3.92e+01, 3.65e+01, 3.28e+00, 3.22e+00]
18000     [2.89e+00, 5.53e-01, 3.91e+00]    [0.00e+00, 3.65e+01, 3.91e+00]    [3.92e+01, 3.65e+01, 3.29e+00, 3.22e+00]
19000     [2.88e+00, 5.91e-01, 3.88e+00]    [0.00e+00, 3.65e+01, 3.88e+00]    [3.92e+01, 3.65e+01, 3.29e+00, 3.23e+00]
20000     [2.85e+00, 5.36e-01, 3.86e+00]    [0.00e+00, 3.65e+01, 3.86e+00]    [3.93e+01, 3.65e+01, 3.30e+00, 3.23e+00]
21000     [2.83e+00, 5.26e-01, 3.83e+00]    [0.00e+00, 3.65e+01, 3.83e+00]    [3.93e+01, 3.65e+01, 3.31e+00, 3.24e+00]
22000     [2.85e+00, 5.53e-01, 3.80e+00]    [0.00e+00, 3.65e+01, 3.80e+00]    [3.93e+01, 3.65e+01, 3.32e+00, 3.25e+00]
23000     [2.80e+00, 5.03e-01, 3.78e+00]    [0.00e+00, 3.64e+01, 3.78e+00]    [3.93e+01, 3.64e+01, 3.33e+00, 3.26e+00]
24000     [2.79e+00, 5.05e-01, 3.75e+00]    [0.00e+00, 3.64e+01, 3.75e+00]    [3.93e+01, 3.64e+01, 3.33e+00, 3.26e+00]
25000     [2.79e+00, 4.90e-01, 3.73e+00]    [0.00e+00, 3.64e+01, 3.73e+00]    [3.93e+01, 3.64e+01, 3.34e+00, 3.27e+00]
26000     [2.80e+00, 5.08e-01, 3.71e+00]    [0.00e+00, 3.64e+01, 3.71e+00]    [3.93e+01, 3.64e+01, 3.34e+00, 3.27e+00]
27000     [2.77e+00, 4.80e-01, 3.68e+00]    [0.00e+00, 3.64e+01, 3.68e+00]    [3.93e+01, 3.64e+01, 3.35e+00, 3.28e+00]
28000     [2.76e+00, 4.79e-01, 3.66e+00]    [0.00e+00, 3.63e+01, 3.66e+00]    [3.93e+01, 3.63e+01, 3.36e+00, 3.29e+00]
29000     [2.71e+00, 4.03e-01, 3.64e+00]    [0.00e+00, 3.63e+01, 3.64e+00]    [3.92e+01, 3.63e+01, 3.37e+00, 3.30e+00]
30000     [2.74e+00, 4.20e-01, 3.62e+00]    [0.00e+00, 3.63e+01, 3.62e+00]    [3.92e+01, 3.63e+01, 3.37e+00, 3.30e+00]

Best model at step 29000:
  train loss: 6.76e+00
  test loss: 4.00e+01
  test metric: [3.92e+01, 3.63e+01, 3.37e+00, 3.30e+00]

'train' took 38.515089 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 6
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.079643 s

'compile' took 0.380334 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [9.92e+01, 9.91e+01, 2.75e+00]    [0.00e+00, 1.00e+02, 2.75e+00]    [1.00e+02, 1.00e+02, 7.07e-02, 7.07e-02]
1000      [7.37e+01, 6.54e+01, 2.96e+00]    [0.00e+00, 8.97e+01, 2.96e+00]    [8.98e+01, 8.97e+01, 5.58e-01, 5.65e-01]
2000      [2.15e+01, 1.54e+01, 4.28e+00]    [0.00e+00, 4.98e+00, 4.28e+00]    [4.64e+00, 4.98e+00, 3.47e+00, 3.81e+00]
3000      [4.84e+00, 2.17e+00, 4.57e+00]    [0.00e+00, 4.06e+01, 4.57e+00]    [3.59e+01, 4.06e+01, 3.54e+00, 3.67e+00]
4000      [4.15e+00, 1.21e+00, 4.52e+00]    [0.00e+00, 4.06e+01, 4.52e+00]    [3.87e+01, 4.06e+01, 3.27e+00, 3.32e+00]
5000      [3.79e+00, 1.06e+00, 4.45e+00]    [0.00e+00, 4.07e+01, 4.45e+00]    [4.00e+01, 4.07e+01, 3.24e+00, 3.25e+00]
6000      [3.62e+00, 9.91e-01, 4.39e+00]    [0.00e+00, 4.06e+01, 4.39e+00]    [4.06e+01, 4.06e+01, 3.21e+00, 3.21e+00]
7000      [3.50e+00, 9.50e-01, 4.33e+00]    [0.00e+00, 4.03e+01, 4.33e+00]    [4.07e+01, 4.03e+01, 3.17e+00, 3.16e+00]
8000      [3.47e+00, 9.49e-01, 4.27e+00]    [0.00e+00, 4.00e+01, 4.27e+00]    [4.07e+01, 4.00e+01, 3.16e+00, 3.14e+00]
9000      [3.33e+00, 8.67e-01, 4.22e+00]    [0.00e+00, 3.96e+01, 4.22e+00]    [4.06e+01, 3.96e+01, 3.15e+00, 3.13e+00]
10000     [3.32e+00, 8.58e-01, 4.18e+00]    [0.00e+00, 3.92e+01, 4.18e+00]    [4.05e+01, 3.92e+01, 3.14e+00, 3.11e+00]
11000     [3.20e+00, 7.92e-01, 4.14e+00]    [0.00e+00, 3.88e+01, 4.14e+00]    [4.04e+01, 3.88e+01, 3.13e+00, 3.10e+00]
12000     [3.15e+00, 7.54e-01, 4.10e+00]    [0.00e+00, 3.85e+01, 4.10e+00]    [4.03e+01, 3.85e+01, 3.13e+00, 3.09e+00]
13000     [3.15e+00, 7.76e-01, 4.06e+00]    [0.00e+00, 3.83e+01, 4.06e+00]    [4.03e+01, 3.83e+01, 3.12e+00, 3.07e+00]
14000     [3.09e+00, 7.66e-01, 4.02e+00]    [0.00e+00, 3.81e+01, 4.02e+00]    [4.02e+01, 3.81e+01, 3.11e+00, 3.07e+00]
15000     [3.08e+00, 7.40e-01, 3.99e+00]    [0.00e+00, 3.79e+01, 3.99e+00]    [4.02e+01, 3.79e+01, 3.11e+00, 3.06e+00]
16000     [3.04e+00, 7.47e-01, 3.95e+00]    [0.00e+00, 3.78e+01, 3.95e+00]    [4.02e+01, 3.78e+01, 3.10e+00, 3.05e+00]
17000     [3.01e+00, 6.86e-01, 3.91e+00]    [0.00e+00, 3.77e+01, 3.91e+00]    [4.01e+01, 3.77e+01, 3.09e+00, 3.04e+00]
18000     [2.98e+00, 6.94e-01, 3.88e+00]    [0.00e+00, 3.77e+01, 3.88e+00]    [4.01e+01, 3.77e+01, 3.09e+00, 3.04e+00]
19000     [2.98e+00, 7.02e-01, 3.84e+00]    [0.00e+00, 3.76e+01, 3.84e+00]    [4.01e+01, 3.76e+01, 3.09e+00, 3.03e+00]
20000     [2.95e+00, 6.36e-01, 3.81e+00]    [0.00e+00, 3.76e+01, 3.81e+00]    [4.01e+01, 3.76e+01, 3.08e+00, 3.02e+00]
21000     [2.94e+00, 6.56e-01, 3.78e+00]    [0.00e+00, 3.76e+01, 3.78e+00]    [4.01e+01, 3.76e+01, 3.07e+00, 3.02e+00]
22000     [2.92e+00, 6.34e-01, 3.75e+00]    [0.00e+00, 3.75e+01, 3.75e+00]    [4.01e+01, 3.75e+01, 3.07e+00, 3.01e+00]
23000     [2.95e+00, 6.95e-01, 3.73e+00]    [0.00e+00, 3.75e+01, 3.73e+00]    [4.01e+01, 3.75e+01, 3.07e+00, 3.01e+00]
24000     [2.88e+00, 5.79e-01, 3.70e+00]    [0.00e+00, 3.75e+01, 3.70e+00]    [4.00e+01, 3.75e+01, 3.06e+00, 3.00e+00]
25000     [2.89e+00, 5.99e-01, 3.67e+00]    [0.00e+00, 3.75e+01, 3.67e+00]    [4.01e+01, 3.75e+01, 3.06e+00, 3.00e+00]
26000     [2.87e+00, 5.78e-01, 3.65e+00]    [0.00e+00, 3.75e+01, 3.65e+00]    [4.01e+01, 3.75e+01, 3.05e+00, 3.00e+00]
27000     [2.88e+00, 5.86e-01, 3.62e+00]    [0.00e+00, 3.75e+01, 3.62e+00]    [4.01e+01, 3.75e+01, 3.05e+00, 2.99e+00]
28000     [2.87e+00, 6.10e-01, 3.60e+00]    [0.00e+00, 3.75e+01, 3.60e+00]    [4.01e+01, 3.75e+01, 3.05e+00, 2.99e+00]
29000     [2.86e+00, 5.72e-01, 3.58e+00]    [0.00e+00, 3.75e+01, 3.58e+00]    [4.01e+01, 3.75e+01, 3.05e+00, 2.99e+00]
30000     [2.83e+00, 5.25e-01, 3.55e+00]    [0.00e+00, 3.75e+01, 3.55e+00]    [4.01e+01, 3.75e+01, 3.04e+00, 2.99e+00]

Best model at step 30000:
  train loss: 6.91e+00
  test loss: 4.11e+01
  test metric: [4.01e+01, 3.75e+01, 3.04e+00, 2.99e+00]

'train' took 38.014027 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 7
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.082868 s

'compile' took 0.380315 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.01e+02, 1.01e+02, 2.92e+00]    [0.00e+00, 9.94e+01, 2.92e+00]    [9.94e+01, 9.94e+01, 1.39e-01, 1.39e-01]
1000      [7.44e+01, 6.52e+01, 3.15e+00]    [0.00e+00, 9.30e+01, 3.15e+00]    [9.32e+01, 9.30e+01, 1.10e+00, 1.12e+00]
2000      [2.91e+01, 2.32e+01, 4.29e+00]    [0.00e+00, 1.96e+01, 4.29e+00]    [2.40e+01, 1.96e+01, 5.47e+00, 5.54e+00]
3000      [6.12e+00, 1.88e+00, 4.68e+00]    [0.00e+00, 3.61e+01, 4.68e+00]    [3.14e+01, 3.61e+01, 4.89e+00, 5.07e+00]
4000      [5.08e+00, 1.15e+00, 4.60e+00]    [0.00e+00, 3.56e+01, 4.60e+00]    [3.33e+01, 3.56e+01, 4.71e+00, 4.79e+00]
5000      [4.56e+00, 1.07e+00, 4.53e+00]    [0.00e+00, 3.51e+01, 4.53e+00]    [3.41e+01, 3.51e+01, 4.60e+00, 4.64e+00]
6000      [4.21e+00, 1.00e+00, 4.47e+00]    [0.00e+00, 3.48e+01, 4.47e+00]    [3.45e+01, 3.48e+01, 4.55e+00, 4.56e+00]
7000      [4.00e+00, 9.22e-01, 4.42e+00]    [0.00e+00, 3.41e+01, 4.42e+00]    [3.40e+01, 3.41e+01, 4.52e+00, 4.52e+00]
8000      [3.79e+00, 8.69e-01, 4.38e+00]    [0.00e+00, 3.38e+01, 4.38e+00]    [3.41e+01, 3.38e+01, 4.47e+00, 4.46e+00]
9000      [3.60e+00, 7.39e-01, 4.34e+00]    [0.00e+00, 3.38e+01, 4.34e+00]    [3.43e+01, 3.38e+01, 4.43e+00, 4.42e+00]
10000     [3.45e+00, 7.06e-01, 4.30e+00]    [0.00e+00, 3.38e+01, 4.30e+00]    [3.46e+01, 3.38e+01, 4.39e+00, 4.37e+00]
11000     [3.34e+00, 6.47e-01, 4.27e+00]    [0.00e+00, 3.36e+01, 4.27e+00]    [3.47e+01, 3.36e+01, 4.35e+00, 4.32e+00]
12000     [3.27e+00, 6.59e-01, 4.23e+00]    [0.00e+00, 3.35e+01, 4.23e+00]    [3.49e+01, 3.35e+01, 4.33e+00, 4.29e+00]
13000     [3.19e+00, 5.94e-01, 4.20e+00]    [0.00e+00, 3.34e+01, 4.20e+00]    [3.50e+01, 3.34e+01, 4.32e+00, 4.27e+00]
14000     [3.15e+00, 5.72e-01, 4.17e+00]    [0.00e+00, 3.33e+01, 4.17e+00]    [3.50e+01, 3.33e+01, 4.31e+00, 4.25e+00]
15000     [3.14e+00, 6.52e-01, 4.14e+00]    [0.00e+00, 3.32e+01, 4.14e+00]    [3.50e+01, 3.32e+01, 4.30e+00, 4.25e+00]
16000     [3.08e+00, 5.15e-01, 4.11e+00]    [0.00e+00, 3.30e+01, 4.11e+00]    [3.49e+01, 3.30e+01, 4.30e+00, 4.24e+00]
17000     [3.05e+00, 5.04e-01, 4.08e+00]    [0.00e+00, 3.28e+01, 4.08e+00]    [3.48e+01, 3.28e+01, 4.30e+00, 4.24e+00]
18000     [3.04e+00, 4.89e-01, 4.05e+00]    [0.00e+00, 3.26e+01, 4.05e+00]    [3.46e+01, 3.26e+01, 4.30e+00, 4.24e+00]
19000     [3.02e+00, 5.52e-01, 4.02e+00]    [0.00e+00, 3.25e+01, 4.02e+00]    [3.45e+01, 3.25e+01, 4.31e+00, 4.25e+00]
20000     [3.06e+00, 5.55e-01, 3.99e+00]    [0.00e+00, 3.24e+01, 3.99e+00]    [3.45e+01, 3.24e+01, 4.32e+00, 4.26e+00]
21000     [3.00e+00, 4.40e-01, 3.97e+00]    [0.00e+00, 3.23e+01, 3.97e+00]    [3.44e+01, 3.23e+01, 4.33e+00, 4.26e+00]
22000     [2.98e+00, 4.11e-01, 3.94e+00]    [0.00e+00, 3.22e+01, 3.94e+00]    [3.43e+01, 3.22e+01, 4.33e+00, 4.27e+00]
23000     [2.95e+00, 3.78e-01, 3.91e+00]    [0.00e+00, 3.21e+01, 3.91e+00]    [3.42e+01, 3.21e+01, 4.34e+00, 4.27e+00]
24000     [2.95e+00, 3.49e-01, 3.89e+00]    [0.00e+00, 3.20e+01, 3.89e+00]    [3.42e+01, 3.20e+01, 4.35e+00, 4.28e+00]
25000     [2.94e+00, 3.12e-01, 3.87e+00]    [0.00e+00, 3.19e+01, 3.87e+00]    [3.41e+01, 3.19e+01, 4.36e+00, 4.29e+00]
26000     [2.91e+00, 3.18e-01, 3.84e+00]    [0.00e+00, 3.18e+01, 3.84e+00]    [3.40e+01, 3.18e+01, 4.37e+00, 4.30e+00]
27000     [2.91e+00, 3.02e-01, 3.82e+00]    [0.00e+00, 3.17e+01, 3.82e+00]    [3.39e+01, 3.17e+01, 4.39e+00, 4.32e+00]
28000     [2.91e+00, 2.91e-01, 3.80e+00]    [0.00e+00, 3.16e+01, 3.80e+00]    [3.38e+01, 3.16e+01, 4.40e+00, 4.33e+00]
29000     [2.95e+00, 3.40e-01, 3.78e+00]    [0.00e+00, 3.15e+01, 3.78e+00]    [3.38e+01, 3.15e+01, 4.42e+00, 4.34e+00]
30000     [2.89e+00, 2.76e-01, 3.75e+00]    [0.00e+00, 3.14e+01, 3.75e+00]    [3.37e+01, 3.14e+01, 4.43e+00, 4.36e+00]

Best model at step 30000:
  train loss: 6.93e+00
  test loss: 3.51e+01
  test metric: [3.37e+01, 3.14e+01, 4.43e+00, 4.36e+00]

'train' took 38.375523 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 8
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.082772 s

'compile' took 0.385603 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.01e+02, 1.01e+02, 2.78e+00]    [0.00e+00, 9.96e+01, 2.78e+00]    [9.96e+01, 9.96e+01, 1.31e-01, 1.31e-01]
1000      [7.61e+01, 6.97e+01, 2.90e+00]    [0.00e+00, 9.41e+01, 2.90e+00]    [9.42e+01, 9.41e+01, 4.77e-01, 4.83e-01]
2000      [2.98e+01, 2.24e+01, 4.07e+00]    [0.00e+00, 1.81e+01, 4.07e+00]    [2.45e+01, 1.81e+01, 4.23e+00, 4.50e+00]
3000      [5.71e+00, 2.67e+00, 4.48e+00]    [0.00e+00, 3.77e+01, 4.48e+00]    [3.11e+01, 3.77e+01, 3.80e+00, 3.99e+00]
4000      [4.46e+00, 1.32e+00, 4.42e+00]    [0.00e+00, 3.82e+01, 4.42e+00]    [3.51e+01, 3.82e+01, 3.43e+00, 3.51e+00]
5000      [3.94e+00, 1.09e+00, 4.36e+00]    [0.00e+00, 3.82e+01, 4.36e+00]    [3.64e+01, 3.82e+01, 3.31e+00, 3.36e+00]
6000      [3.66e+00, 9.57e-01, 4.31e+00]    [0.00e+00, 3.80e+01, 4.31e+00]    [3.70e+01, 3.80e+01, 3.28e+00, 3.31e+00]
7000      [3.55e+00, 9.42e-01, 4.26e+00]    [0.00e+00, 3.77e+01, 4.26e+00]    [3.71e+01, 3.77e+01, 3.27e+00, 3.29e+00]
8000      [3.43e+00, 8.30e-01, 4.22e+00]    [0.00e+00, 3.74e+01, 4.22e+00]    [3.72e+01, 3.74e+01, 3.26e+00, 3.27e+00]
9000      [3.35e+00, 7.66e-01, 4.17e+00]    [0.00e+00, 3.72e+01, 4.17e+00]    [3.73e+01, 3.72e+01, 3.26e+00, 3.26e+00]
10000     [3.27e+00, 7.24e-01, 4.13e+00]    [0.00e+00, 3.72e+01, 4.13e+00]    [3.76e+01, 3.72e+01, 3.27e+00, 3.25e+00]
11000     [3.23e+00, 6.82e-01, 4.09e+00]    [0.00e+00, 3.68e+01, 4.09e+00]    [3.75e+01, 3.68e+01, 3.24e+00, 3.22e+00]
12000     [3.17e+00, 6.25e-01, 4.05e+00]    [0.00e+00, 3.65e+01, 4.05e+00]    [3.75e+01, 3.65e+01, 3.22e+00, 3.19e+00]
13000     [3.14e+00, 6.13e-01, 4.02e+00]    [0.00e+00, 3.63e+01, 4.02e+00]    [3.75e+01, 3.63e+01, 3.20e+00, 3.18e+00]
14000     [3.13e+00, 5.97e-01, 3.98e+00]    [0.00e+00, 3.62e+01, 3.98e+00]    [3.75e+01, 3.62e+01, 3.20e+00, 3.16e+00]
15000     [3.08e+00, 5.66e-01, 3.95e+00]    [0.00e+00, 3.62e+01, 3.95e+00]    [3.77e+01, 3.62e+01, 3.21e+00, 3.17e+00]
16000     [3.05e+00, 5.59e-01, 3.92e+00]    [0.00e+00, 3.62e+01, 3.92e+00]    [3.78e+01, 3.62e+01, 3.21e+00, 3.17e+00]
17000     [3.02e+00, 5.25e-01, 3.89e+00]    [0.00e+00, 3.61e+01, 3.89e+00]    [3.79e+01, 3.61e+01, 3.21e+00, 3.17e+00]
18000     [2.97e+00, 5.06e-01, 3.86e+00]    [0.00e+00, 3.61e+01, 3.86e+00]    [3.80e+01, 3.61e+01, 3.22e+00, 3.17e+00]
19000     [2.93e+00, 4.88e-01, 3.83e+00]    [0.00e+00, 3.60e+01, 3.83e+00]    [3.81e+01, 3.60e+01, 3.22e+00, 3.17e+00]
20000     [2.93e+00, 4.95e-01, 3.81e+00]    [0.00e+00, 3.60e+01, 3.81e+00]    [3.81e+01, 3.60e+01, 3.21e+00, 3.16e+00]
21000     [2.91e+00, 5.07e-01, 3.78e+00]    [0.00e+00, 3.61e+01, 3.78e+00]    [3.83e+01, 3.61e+01, 3.21e+00, 3.16e+00]
22000     [2.86e+00, 4.45e-01, 3.76e+00]    [0.00e+00, 3.61e+01, 3.76e+00]    [3.83e+01, 3.61e+01, 3.22e+00, 3.16e+00]
23000     [2.91e+00, 5.65e-01, 3.73e+00]    [0.00e+00, 3.61e+01, 3.73e+00]    [3.84e+01, 3.61e+01, 3.22e+00, 3.16e+00]
24000     [2.83e+00, 4.43e-01, 3.71e+00]    [0.00e+00, 3.61e+01, 3.71e+00]    [3.85e+01, 3.61e+01, 3.22e+00, 3.17e+00]
25000     [2.83e+00, 4.83e-01, 3.68e+00]    [0.00e+00, 3.61e+01, 3.68e+00]    [3.85e+01, 3.61e+01, 3.23e+00, 3.17e+00]
26000     [2.79e+00, 4.04e-01, 3.66e+00]    [0.00e+00, 3.61e+01, 3.66e+00]    [3.85e+01, 3.61e+01, 3.23e+00, 3.17e+00]
27000     [2.80e+00, 4.58e-01, 3.64e+00]    [0.00e+00, 3.61e+01, 3.64e+00]    [3.86e+01, 3.61e+01, 3.23e+00, 3.17e+00]
28000     [2.77e+00, 4.03e-01, 3.62e+00]    [0.00e+00, 3.60e+01, 3.62e+00]    [3.86e+01, 3.60e+01, 3.24e+00, 3.17e+00]
29000     [2.77e+00, 4.28e-01, 3.60e+00]    [0.00e+00, 3.61e+01, 3.60e+00]    [3.87e+01, 3.61e+01, 3.24e+00, 3.18e+00]
30000     [2.74e+00, 3.68e-01, 3.58e+00]    [0.00e+00, 3.60e+01, 3.58e+00]    [3.87e+01, 3.60e+01, 3.24e+00, 3.18e+00]

Best model at step 30000:
  train loss: 6.68e+00
  test loss: 3.96e+01
  test metric: [3.87e+01, 3.60e+01, 3.24e+00, 3.18e+00]

'train' took 39.252515 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 9
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.077435 s

'compile' took 0.387462 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [9.98e+01, 9.95e+01, 2.79e+00]    [0.00e+00, 1.00e+02, 2.79e+00]    [1.00e+02, 1.00e+02, 1.61e-01, 1.61e-01]
1000      [7.48e+01, 6.83e+01, 2.95e+00]    [0.00e+00, 9.08e+01, 2.95e+00]    [9.09e+01, 9.08e+01, 2.02e-01, 2.04e-01]
2000      [2.75e+01, 2.05e+01, 4.22e+00]    [0.00e+00, 1.27e+01, 4.22e+00]    [1.80e+01, 1.27e+01, 1.25e+00, 1.33e+00]
3000      [4.97e+00, 2.64e+00, 4.65e+00]    [0.00e+00, 4.31e+01, 4.65e+00]    [3.70e+01, 4.31e+01, 2.81e+00, 2.93e+00]
4000      [4.27e+00, 1.52e+00, 4.59e+00]    [0.00e+00, 4.33e+01, 4.59e+00]    [4.01e+01, 4.33e+01, 2.68e+00, 2.74e+00]
5000      [3.91e+00, 1.14e+00, 4.52e+00]    [0.00e+00, 4.24e+01, 4.52e+00]    [4.07e+01, 4.24e+01, 2.59e+00, 2.62e+00]
6000      [3.64e+00, 8.80e-01, 4.45e+00]    [0.00e+00, 4.21e+01, 4.45e+00]    [4.13e+01, 4.21e+01, 2.57e+00, 2.58e+00]
7000      [3.52e+00, 8.06e-01, 4.39e+00]    [0.00e+00, 4.19e+01, 4.39e+00]    [4.15e+01, 4.19e+01, 2.58e+00, 2.58e+00]
8000      [3.46e+00, 8.67e-01, 4.33e+00]    [0.00e+00, 4.16e+01, 4.33e+00]    [4.14e+01, 4.16e+01, 2.58e+00, 2.59e+00]
9000      [3.39e+00, 8.21e-01, 4.27e+00]    [0.00e+00, 4.13e+01, 4.27e+00]    [4.14e+01, 4.13e+01, 2.59e+00, 2.59e+00]
10000     [3.32e+00, 7.61e-01, 4.22e+00]    [0.00e+00, 4.12e+01, 4.22e+00]    [4.15e+01, 4.12e+01, 2.59e+00, 2.59e+00]
11000     [3.26e+00, 6.77e-01, 4.17e+00]    [0.00e+00, 4.11e+01, 4.17e+00]    [4.16e+01, 4.11e+01, 2.60e+00, 2.59e+00]
12000     [3.21e+00, 6.41e-01, 4.13e+00]    [0.00e+00, 4.10e+01, 4.13e+00]    [4.17e+01, 4.10e+01, 2.61e+00, 2.59e+00]
13000     [3.15e+00, 6.60e-01, 4.10e+00]    [0.00e+00, 4.09e+01, 4.10e+00]    [4.18e+01, 4.09e+01, 2.62e+00, 2.60e+00]
14000     [3.12e+00, 6.03e-01, 4.06e+00]    [0.00e+00, 4.08e+01, 4.06e+00]    [4.19e+01, 4.08e+01, 2.63e+00, 2.61e+00]
15000     [3.09e+00, 6.00e-01, 4.03e+00]    [0.00e+00, 4.07e+01, 4.03e+00]    [4.19e+01, 4.07e+01, 2.63e+00, 2.61e+00]
16000     [3.05e+00, 5.54e-01, 4.00e+00]    [0.00e+00, 4.06e+01, 4.00e+00]    [4.18e+01, 4.06e+01, 2.63e+00, 2.61e+00]
17000     [3.01e+00, 5.29e-01, 3.97e+00]    [0.00e+00, 4.04e+01, 3.97e+00]    [4.17e+01, 4.04e+01, 2.63e+00, 2.61e+00]
18000     [2.99e+00, 5.67e-01, 3.94e+00]    [0.00e+00, 4.02e+01, 3.94e+00]    [4.16e+01, 4.02e+01, 2.64e+00, 2.61e+00]
19000     [3.01e+00, 5.48e-01, 3.91e+00]    [0.00e+00, 4.01e+01, 3.91e+00]    [4.15e+01, 4.01e+01, 2.64e+00, 2.61e+00]
20000     [2.95e+00, 4.89e-01, 3.88e+00]    [0.00e+00, 4.01e+01, 3.88e+00]    [4.16e+01, 4.01e+01, 2.64e+00, 2.62e+00]
21000     [2.93e+00, 4.96e-01, 3.86e+00]    [0.00e+00, 4.01e+01, 3.86e+00]    [4.16e+01, 4.01e+01, 2.65e+00, 2.62e+00]
22000     [2.96e+00, 5.69e-01, 3.83e+00]    [0.00e+00, 4.01e+01, 3.83e+00]    [4.17e+01, 4.01e+01, 2.66e+00, 2.63e+00]
23000     [2.90e+00, 4.34e-01, 3.80e+00]    [0.00e+00, 4.01e+01, 3.80e+00]    [4.17e+01, 4.01e+01, 2.66e+00, 2.63e+00]
24000     [2.88e+00, 4.34e-01, 3.78e+00]    [0.00e+00, 4.01e+01, 3.78e+00]    [4.17e+01, 4.01e+01, 2.67e+00, 2.64e+00]
25000     [2.88e+00, 4.65e-01, 3.75e+00]    [0.00e+00, 4.01e+01, 3.75e+00]    [4.17e+01, 4.01e+01, 2.68e+00, 2.65e+00]
26000     [2.88e+00, 4.56e-01, 3.73e+00]    [0.00e+00, 4.01e+01, 3.73e+00]    [4.17e+01, 4.01e+01, 2.68e+00, 2.65e+00]
27000     [2.86e+00, 4.35e-01, 3.71e+00]    [0.00e+00, 4.02e+01, 3.71e+00]    [4.18e+01, 4.02e+01, 2.69e+00, 2.66e+00]
28000     [2.86e+00, 3.96e-01, 3.68e+00]    [0.00e+00, 4.02e+01, 3.68e+00]    [4.17e+01, 4.02e+01, 2.70e+00, 2.67e+00]
29000     [2.84e+00, 4.26e-01, 3.66e+00]    [0.00e+00, 4.00e+01, 3.66e+00]    [4.16e+01, 4.00e+01, 2.70e+00, 2.67e+00]
30000     [2.79e+00, 3.74e-01, 3.64e+00]    [0.00e+00, 4.00e+01, 3.64e+00]    [4.16e+01, 4.00e+01, 2.71e+00, 2.68e+00]

Best model at step 30000:
  train loss: 6.80e+00
  test loss: 4.36e+01
  test metric: [4.16e+01, 4.00e+01, 2.71e+00, 2.68e+00]

'train' took 38.269988 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...
Estar
[36.92176138  3.2472409 ] [2.44069091 0.52985835]
              Case          n     E (GPa)   sy (GPa)     C (GPa)    dP/dh (N/m)      Wp/Wt    E* (GPa)      sy/E*
count    97.000000  97.000000   97.000000  97.000000   97.000000      97.000000  97.000000   97.000000  97.000000
mean    279.030928   0.213917  107.163804   1.327507   92.953350  128652.067967   0.731227  102.813375   0.013835
std     411.446469   0.178797   67.175628   1.126477   66.232911   85768.846959   0.134844   60.541899   0.009753
min       1.000000   0.000000   10.000000   0.030000    2.731300    9801.900000   0.451835   10.880844   0.001399
25%      37.000000   0.100000   50.000000   0.300000   39.269000   53194.000000   0.628612   52.343315   0.005508
50%      67.000000   0.177243  100.806000   1.000000   80.813000  123181.000000   0.740598  100.685905   0.011463
75%      91.000000   0.300000  170.000000   2.000000  144.840000  194391.000000   0.830543  159.806250   0.019105
max    1023.000000   0.500000  210.000000   5.298021  261.330000  336838.000000   0.971835  190.913667   0.038209
              Case          n     E (GPa)   sy (GPa)     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  14.000000   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499   1.436338   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848   1.652156   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000   0.036792    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500   0.303588   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000   0.985823   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000   1.719617   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000   5.298021  239.235773  326727.270700   0.971982
             Case     C (GPa)    dP/dh (N/m)       Wp/Wt     hm (um)        Pm (N)  E* (GPa)      sy (GPa)  s0.008 (GPa)  s0.015 (GPa)  s0.033 (GPa)
count  144.000000  144.000000     144.000000  144.000000  144.000000  1.440000e+02    144.00  1.440000e+02  1.440000e+02      144.0000    144.000000
mean    71.500000  137.712090  202749.537112    0.725860    0.256082  9.000019e-03    109.63  1.121000e+00  1.165600e+00        1.2001      1.234167
std     41.713307    9.399233    4178.310300    0.009606    0.008684  2.479224e-08      0.00  2.228196e-16  2.228196e-16        0.0000      0.000000
min      0.000000  101.595236  192856.539700    0.696906    0.223895  8.999961e-03    109.63  1.121000e+00  1.165600e+00        1.2001      1.234167
25%     35.750000  132.175084  199461.369750    0.718985    0.251139  9.000003e-03    109.63  1.121000e+00  1.165600e+00        1.2001      1.234167
50%     71.500000  136.682184  202962.030950    0.725867    0.256606  9.000022e-03    109.63  1.121000e+00  1.165600e+00        1.2001      1.234167
75%    107.250000  142.696850  205897.992300    0.732360    0.260943  9.000038e-03    109.63  1.121000e+00  1.165600e+00        1.2001      1.234167
max    143.000000  179.537847  212396.807300    0.754513    0.297635  9.000075e-03    109.63  1.121000e+00  1.165600e+00        1.2001      1.234167

Iteration: 0
[ 88  70  87  36  21   9 103  67 117  47] ==> [  7  89  97  26 110 128  59  22 129  16 126 120  40  45  54  33  24   8
 127  51  86 116  37  96  43 114  63 140 101  18  71  27 141   2  60  10
  76 105  56 108  61  44  66 112  95  92  50  30 131  83  98  62 122  90
 113  48 138  73  13 142 100  84  78 124  15  52   3 118 135   6  68  85
  12  91 109  93  46  11 121 104  41 106   1 102 133  42   4 119  17  38
   5  53 137  94   0  34  28  55  75  35  23  74  31 107  57 125  65  32
 132  14 111  19  29  49 130  99  82  64 143  79  69 123  80 115  20 139
  72  77  25  81 134 136  39  58]
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.078053 s

'compile' took 0.377519 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.02e+02, 1.01e+02, 2.77e+00]    [0.00e+00, 9.88e+01, 2.77e+00]    [9.88e+01, 9.88e+01, 5.40e-02, 5.40e-02]
1000      [7.06e+01, 7.01e+01, 3.11e+00]    [0.00e+00, 7.84e+01, 3.11e+00]    [7.88e+01, 7.84e+01, 1.00e+00, 1.02e+00]
2000      [1.99e+01, 1.40e+01, 4.40e+00]    [0.00e+00, 1.03e+01, 4.40e+00]    [5.94e+00, 1.03e+01, 2.03e+00, 2.28e+00]
3000      [9.54e+00, 6.43e+00, 4.56e+00]    [0.00e+00, 7.98e+00, 4.56e+00]    [6.78e+00, 7.98e+00, 2.41e+00, 2.48e+00]
4000      [8.64e+00, 3.18e+00, 4.56e+00]    [0.00e+00, 2.31e+00, 4.56e+00]    [2.25e+00, 2.31e+00, 2.69e+00, 2.78e+00]
5000      [7.45e+00, 3.02e+00, 4.55e+00]    [0.00e+00, 2.93e+00, 4.55e+00]    [2.95e+00, 2.93e+00, 3.50e+00, 3.47e+00]
6000      [6.70e+00, 2.84e+00, 4.54e+00]    [0.00e+00, 3.28e+00, 4.54e+00]    [3.46e+00, 3.28e+00, 4.01e+00, 3.85e+00]
7000      [5.94e+00, 2.72e+00, 4.53e+00]    [0.00e+00, 3.64e+00, 4.53e+00]    [4.09e+00, 3.64e+00, 4.58e+00, 4.23e+00]
8000      [5.44e+00, 2.64e+00, 4.51e+00]    [0.00e+00, 3.83e+00, 4.51e+00]    [4.85e+00, 3.83e+00, 4.97e+00, 4.40e+00]
9000      [5.05e+00, 2.45e+00, 4.50e+00]    [0.00e+00, 3.97e+00, 4.50e+00]    [5.68e+00, 3.97e+00, 5.29e+00, 4.52e+00]
10000     [4.85e+00, 2.38e+00, 4.48e+00]    [0.00e+00, 4.08e+00, 4.48e+00]    [6.25e+00, 4.08e+00, 5.49e+00, 4.62e+00]
11000     [4.70e+00, 2.33e+00, 4.46e+00]    [0.00e+00, 4.21e+00, 4.46e+00]    [6.43e+00, 4.21e+00, 5.65e+00, 4.74e+00]
12000     [4.66e+00, 2.28e+00, 4.43e+00]    [0.00e+00, 4.34e+00, 4.43e+00]    [6.44e+00, 4.34e+00, 5.78e+00, 4.85e+00]
13000     [4.55e+00, 2.16e+00, 4.41e+00]    [0.00e+00, 4.45e+00, 4.41e+00]    [6.45e+00, 4.45e+00, 5.90e+00, 4.94e+00]
14000     [4.38e+00, 2.05e+00, 4.39e+00]    [0.00e+00, 4.55e+00, 4.39e+00]    [6.57e+00, 4.55e+00, 6.02e+00, 5.04e+00]
15000     [4.31e+00, 1.98e+00, 4.38e+00]    [0.00e+00, 4.64e+00, 4.38e+00]    [6.66e+00, 4.64e+00, 6.14e+00, 5.12e+00]
16000     [4.21e+00, 1.93e+00, 4.37e+00]    [0.00e+00, 4.73e+00, 4.37e+00]    [6.71e+00, 4.73e+00, 6.26e+00, 5.20e+00]
17000     [4.11e+00, 1.81e+00, 4.35e+00]    [0.00e+00, 4.80e+00, 4.35e+00]    [6.58e+00, 4.80e+00, 6.33e+00, 5.25e+00]
18000     [4.05e+00, 1.76e+00, 4.33e+00]    [0.00e+00, 4.90e+00, 4.33e+00]    [6.55e+00, 4.90e+00, 6.41e+00, 5.32e+00]
19000     [4.02e+00, 1.71e+00, 4.31e+00]    [0.00e+00, 5.01e+00, 4.31e+00]    [6.53e+00, 5.01e+00, 6.47e+00, 5.38e+00]
20000     [4.05e+00, 1.70e+00, 4.29e+00]    [0.00e+00, 5.11e+00, 4.29e+00]    [6.53e+00, 5.11e+00, 6.53e+00, 5.44e+00]
21000     [3.99e+00, 1.70e+00, 4.27e+00]    [0.00e+00, 5.21e+00, 4.27e+00]    [6.56e+00, 5.21e+00, 6.59e+00, 5.51e+00]
22000     [3.94e+00, 1.67e+00, 4.25e+00]    [0.00e+00, 5.30e+00, 4.25e+00]    [6.57e+00, 5.30e+00, 6.64e+00, 5.56e+00]
23000     [3.91e+00, 1.60e+00, 4.24e+00]    [0.00e+00, 5.38e+00, 4.24e+00]    [6.55e+00, 5.38e+00, 6.67e+00, 5.60e+00]
24000     [3.81e+00, 1.57e+00, 4.22e+00]    [0.00e+00, 5.47e+00, 4.22e+00]    [6.58e+00, 5.47e+00, 6.72e+00, 5.66e+00]
25000     [3.78e+00, 1.55e+00, 4.20e+00]    [0.00e+00, 5.55e+00, 4.20e+00]    [6.60e+00, 5.55e+00, 6.76e+00, 5.73e+00]
26000     [3.69e+00, 1.52e+00, 4.19e+00]    [0.00e+00, 5.63e+00, 4.19e+00]    [6.61e+00, 5.63e+00, 6.79e+00, 5.79e+00]
27000     [3.66e+00, 1.55e+00, 4.17e+00]    [0.00e+00, 5.72e+00, 4.17e+00]    [6.62e+00, 5.72e+00, 6.83e+00, 5.86e+00]
28000     [3.60e+00, 1.50e+00, 4.15e+00]    [0.00e+00, 5.78e+00, 4.15e+00]    [6.62e+00, 5.78e+00, 6.86e+00, 5.91e+00]
29000     [3.59e+00, 1.53e+00, 4.13e+00]    [0.00e+00, 5.82e+00, 4.13e+00]    [6.62e+00, 5.82e+00, 6.87e+00, 5.95e+00]
30000     [3.59e+00, 1.52e+00, 4.12e+00]    [0.00e+00, 5.85e+00, 4.12e+00]    [6.64e+00, 5.85e+00, 6.89e+00, 5.99e+00]

Best model at step 30000:
  train loss: 9.22e+00
  test loss: 9.96e+00
  test metric: [6.64e+00, 5.85e+00, 6.89e+00, 5.99e+00]

'train' took 38.535166 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 1
[ 40 112  13 107  94   3   2 125  24  30] ==> [119 127  12   6  93  45 120  55  47  63 130  15  42  99  22 102  98 132
  66 111  52 141  64  48  88  49  81  17  87  62   8 143 116  36 118   0
   5  97 101  92  34 106 113  59   7  78  74  57 103  73 121  50  26  65
  90  68 138  58   9  39 135 137 114  84  20  46  51  53  23  27 140  28
  37  91  10  89 105  43  69 108  35  76 139  82 124 126  44   1  38 122
 131  54 134  18  41  79 115 117  71 136  31  85  70 100  32 104  33  83
 128  60  80  25 110  21  29 142  16  56  75 123  77  86  11 109 129   4
  96  14  61  67 133  95  19  72]
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.079184 s

'compile' took 0.382004 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [9.94e+01, 9.98e+01, 2.70e+00]    [0.00e+00, 9.99e+01, 2.70e+00]    [9.99e+01, 9.99e+01, 7.30e-02, 7.30e-02]
1000      [7.03e+01, 7.24e+01, 2.96e+00]    [0.00e+00, 7.84e+01, 2.96e+00]    [7.87e+01, 7.84e+01, 9.09e-01, 9.16e-01]
2000      [2.28e+01, 1.41e+01, 4.18e+00]    [0.00e+00, 6.99e+00, 4.18e+00]    [3.83e+00, 6.99e+00, 3.02e+00, 3.39e+00]
3000      [7.30e+00, 9.79e+00, 4.47e+00]    [0.00e+00, 2.00e+01, 4.47e+00]    [1.88e+01, 2.00e+01, 2.56e+00, 2.59e+00]
4000      [7.73e+00, 5.85e+00, 4.43e+00]    [0.00e+00, 9.76e+00, 4.43e+00]    [9.87e+00, 9.76e+00, 2.79e+00, 2.78e+00]
5000      [7.69e+00, 3.76e+00, 4.41e+00]    [0.00e+00, 4.24e+00, 4.41e+00]    [5.41e+00, 4.24e+00, 3.00e+00, 2.91e+00]
6000      [7.02e+00, 3.31e+00, 4.39e+00]    [0.00e+00, 3.10e+00, 4.39e+00]    [5.92e+00, 3.10e+00, 3.29e+00, 2.97e+00]
7000      [6.08e+00, 3.25e+00, 4.39e+00]    [0.00e+00, 3.42e+00, 4.39e+00]    [8.10e+00, 3.42e+00, 3.75e+00, 3.31e+00]
8000      [5.42e+00, 3.25e+00, 4.39e+00]    [0.00e+00, 3.79e+00, 4.39e+00]    [9.08e+00, 3.79e+00, 4.16e+00, 3.64e+00]
9000      [4.97e+00, 3.19e+00, 4.37e+00]    [0.00e+00, 4.07e+00, 4.37e+00]    [9.35e+00, 4.07e+00, 4.48e+00, 3.92e+00]
10000     [4.77e+00, 3.14e+00, 4.35e+00]    [0.00e+00, 4.28e+00, 4.35e+00]    [9.39e+00, 4.28e+00, 4.70e+00, 4.12e+00]
11000     [4.56e+00, 3.11e+00, 4.32e+00]    [0.00e+00, 4.51e+00, 4.32e+00]    [9.52e+00, 4.51e+00, 4.90e+00, 4.31e+00]
12000     [4.42e+00, 3.08e+00, 4.30e+00]    [0.00e+00, 4.66e+00, 4.30e+00]    [9.68e+00, 4.66e+00, 5.07e+00, 4.45e+00]
13000     [4.29e+00, 3.02e+00, 4.28e+00]    [0.00e+00, 4.68e+00, 4.28e+00]    [9.85e+00, 4.68e+00, 5.11e+00, 4.48e+00]
14000     [4.25e+00, 2.91e+00, 4.25e+00]    [0.00e+00, 4.62e+00, 4.25e+00]    [9.77e+00, 4.62e+00, 5.13e+00, 4.47e+00]
15000     [4.25e+00, 2.90e+00, 4.22e+00]    [0.00e+00, 4.58e+00, 4.22e+00]    [9.70e+00, 4.58e+00, 5.12e+00, 4.46e+00]
16000     [4.16e+00, 2.74e+00, 4.20e+00]    [0.00e+00, 4.48e+00, 4.20e+00]    [9.51e+00, 4.48e+00, 5.11e+00, 4.44e+00]
17000     [4.11e+00, 2.68e+00, 4.17e+00]    [0.00e+00, 4.40e+00, 4.17e+00]    [9.35e+00, 4.40e+00, 5.09e+00, 4.41e+00]
18000     [4.10e+00, 2.60e+00, 4.15e+00]    [0.00e+00, 4.33e+00, 4.15e+00]    [9.20e+00, 4.33e+00, 5.08e+00, 4.40e+00]
19000     [4.05e+00, 2.54e+00, 4.13e+00]    [0.00e+00, 4.29e+00, 4.13e+00]    [9.09e+00, 4.29e+00, 5.08e+00, 4.39e+00]
20000     [4.03e+00, 2.51e+00, 4.11e+00]    [0.00e+00, 4.26e+00, 4.11e+00]    [8.98e+00, 4.26e+00, 5.08e+00, 4.38e+00]
21000     [3.99e+00, 2.48e+00, 4.09e+00]    [0.00e+00, 4.24e+00, 4.09e+00]    [8.88e+00, 4.24e+00, 5.08e+00, 4.37e+00]
22000     [3.93e+00, 2.42e+00, 4.07e+00]    [0.00e+00, 4.25e+00, 4.07e+00]    [8.84e+00, 4.25e+00, 5.08e+00, 4.37e+00]
23000     [3.97e+00, 2.42e+00, 4.06e+00]    [0.00e+00, 4.18e+00, 4.06e+00]    [8.64e+00, 4.18e+00, 5.09e+00, 4.35e+00]
24000     [3.99e+00, 2.40e+00, 4.04e+00]    [0.00e+00, 4.07e+00, 4.04e+00]    [8.33e+00, 4.07e+00, 5.10e+00, 4.33e+00]
25000     [3.90e+00, 2.29e+00, 4.02e+00]    [0.00e+00, 3.95e+00, 4.02e+00]    [7.98e+00, 3.95e+00, 5.10e+00, 4.28e+00]
26000     [3.92e+00, 2.27e+00, 4.01e+00]    [0.00e+00, 3.89e+00, 4.01e+00]    [7.75e+00, 3.89e+00, 5.10e+00, 4.26e+00]
27000     [3.85e+00, 2.20e+00, 3.99e+00]    [0.00e+00, 3.83e+00, 3.99e+00]    [7.50e+00, 3.83e+00, 5.10e+00, 4.22e+00]
28000     [3.86e+00, 2.17e+00, 3.98e+00]    [0.00e+00, 3.78e+00, 3.98e+00]    [7.24e+00, 3.78e+00, 5.12e+00, 4.20e+00]
29000     [3.87e+00, 2.18e+00, 3.96e+00]    [0.00e+00, 3.76e+00, 3.96e+00]    [7.04e+00, 3.76e+00, 5.12e+00, 4.18e+00]
30000     [3.83e+00, 2.13e+00, 3.94e+00]    [0.00e+00, 3.76e+00, 3.94e+00]    [6.86e+00, 3.76e+00, 5.12e+00, 4.17e+00]

Best model at step 30000:
  train loss: 9.90e+00
  test loss: 7.70e+00
  test metric: [6.86e+00, 3.76e+00, 5.12e+00, 4.17e+00]

'train' took 39.649340 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 2
[105  28  73  89  93  41  83  43  92  18] ==> [136  29  54  66  10 139 126 101 121 129  21  62 122   3  88  40  99 125
 130 103 127  27 137   1  37 134  65  33  50  12  31  97 133  23  46   2
  96  98  47  17  77  69  57 104  15  95  60 120 108 113 119  38   6  90
  44   9 102  76  35  85 131  59 114  48  70  80  78 109 141 111  84  30
 107  34 143 124  49  52  74  26  45  39   4  11  53 100  79 123   0   5
 112  61 115   7 140  22  68  82  20 135  86  14  42 117  25  36 142  75
  64  55  81  58  13 118  71  72  19  32  87 110 138  91  56   8  51  67
 132 116  24  94 106  16  63 128]
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.075694 s

'compile' took 0.379543 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.00e+02, 1.00e+02, 2.75e+00]    [0.00e+00, 1.00e+02, 2.75e+00]    [1.00e+02, 1.00e+02, 1.56e-02, 1.56e-02]
1000      [7.08e+01, 7.31e+01, 3.01e+00]    [0.00e+00, 7.99e+01, 3.01e+00]    [8.01e+01, 7.99e+01, 8.71e-01, 8.78e-01]
2000      [2.39e+01, 1.34e+01, 4.19e+00]    [0.00e+00, 4.15e+00, 4.19e+00]    [2.27e+00, 4.15e+00, 2.26e+00, 2.94e+00]
3000      [7.79e+00, 9.03e+00, 4.51e+00]    [0.00e+00, 1.65e+01, 4.51e+00]    [1.51e+01, 1.65e+01, 2.60e+00, 2.63e+00]
4000      [8.40e+00, 4.08e+00, 4.50e+00]    [0.00e+00, 4.57e+00, 4.50e+00]    [4.59e+00, 4.57e+00, 3.09e+00, 3.09e+00]
5000      [7.57e+00, 2.95e+00, 4.50e+00]    [0.00e+00, 2.85e+00, 4.50e+00]    [3.43e+00, 2.85e+00, 3.51e+00, 3.25e+00]
6000      [6.33e+00, 2.71e+00, 4.51e+00]    [0.00e+00, 3.15e+00, 4.51e+00]    [4.85e+00, 3.15e+00, 4.22e+00, 3.68e+00]
7000      [5.63e+00, 2.37e+00, 4.51e+00]    [0.00e+00, 3.22e+00, 4.51e+00]    [5.87e+00, 3.22e+00, 4.59e+00, 3.76e+00]
8000      [5.26e+00, 2.12e+00, 4.49e+00]    [0.00e+00, 3.36e+00, 4.49e+00]    [5.94e+00, 3.36e+00, 4.84e+00, 3.89e+00]
9000      [4.99e+00, 1.95e+00, 4.47e+00]    [0.00e+00, 3.51e+00, 4.47e+00]    [6.06e+00, 3.51e+00, 5.02e+00, 4.01e+00]
10000     [4.85e+00, 1.87e+00, 4.45e+00]    [0.00e+00, 3.56e+00, 4.45e+00]    [6.32e+00, 3.56e+00, 5.07e+00, 4.03e+00]
11000     [4.67e+00, 1.77e+00, 4.43e+00]    [0.00e+00, 3.66e+00, 4.43e+00]    [6.50e+00, 3.66e+00, 5.19e+00, 4.10e+00]
12000     [4.55e+00, 1.70e+00, 4.40e+00]    [0.00e+00, 3.77e+00, 4.40e+00]    [6.32e+00, 3.77e+00, 5.28e+00, 4.19e+00]
13000     [4.44e+00, 1.63e+00, 4.38e+00]    [0.00e+00, 3.88e+00, 4.38e+00]    [6.31e+00, 3.88e+00, 5.37e+00, 4.28e+00]
14000     [4.33e+00, 1.59e+00, 4.36e+00]    [0.00e+00, 3.97e+00, 4.36e+00]    [6.32e+00, 3.97e+00, 5.44e+00, 4.35e+00]
15000     [4.26e+00, 1.56e+00, 4.33e+00]    [0.00e+00, 4.05e+00, 4.33e+00]    [6.35e+00, 4.05e+00, 5.52e+00, 4.40e+00]
16000     [4.21e+00, 1.52e+00, 4.30e+00]    [0.00e+00, 4.12e+00, 4.30e+00]    [6.37e+00, 4.12e+00, 5.58e+00, 4.45e+00]
17000     [4.12e+00, 1.49e+00, 4.28e+00]    [0.00e+00, 4.19e+00, 4.28e+00]    [6.42e+00, 4.19e+00, 5.66e+00, 4.50e+00]
18000     [4.09e+00, 1.48e+00, 4.26e+00]    [0.00e+00, 4.26e+00, 4.26e+00]    [6.40e+00, 4.26e+00, 5.72e+00, 4.55e+00]
19000     [4.01e+00, 1.44e+00, 4.23e+00]    [0.00e+00, 4.32e+00, 4.23e+00]    [6.39e+00, 4.32e+00, 5.78e+00, 4.60e+00]
20000     [3.99e+00, 1.46e+00, 4.21e+00]    [0.00e+00, 4.39e+00, 4.21e+00]    [6.33e+00, 4.39e+00, 5.83e+00, 4.65e+00]
21000     [3.91e+00, 1.42e+00, 4.18e+00]    [0.00e+00, 4.45e+00, 4.18e+00]    [6.28e+00, 4.45e+00, 5.87e+00, 4.70e+00]
22000     [3.87e+00, 1.38e+00, 4.16e+00]    [0.00e+00, 4.51e+00, 4.16e+00]    [6.28e+00, 4.51e+00, 5.92e+00, 4.74e+00]
23000     [3.85e+00, 1.37e+00, 4.13e+00]    [0.00e+00, 4.57e+00, 4.13e+00]    [6.27e+00, 4.57e+00, 5.95e+00, 4.77e+00]
24000     [3.84e+00, 1.37e+00, 4.11e+00]    [0.00e+00, 4.63e+00, 4.11e+00]    [6.28e+00, 4.63e+00, 5.98e+00, 4.81e+00]
25000     [3.78e+00, 1.36e+00, 4.09e+00]    [0.00e+00, 4.69e+00, 4.09e+00]    [6.31e+00, 4.69e+00, 6.03e+00, 4.86e+00]
26000     [3.71e+00, 1.33e+00, 4.07e+00]    [0.00e+00, 4.76e+00, 4.07e+00]    [6.32e+00, 4.76e+00, 6.08e+00, 4.91e+00]
27000     [3.69e+00, 1.32e+00, 4.05e+00]    [0.00e+00, 4.82e+00, 4.05e+00]    [6.31e+00, 4.82e+00, 6.12e+00, 4.96e+00]
28000     [3.62e+00, 1.30e+00, 4.04e+00]    [0.00e+00, 4.89e+00, 4.04e+00]    [6.33e+00, 4.89e+00, 6.18e+00, 5.02e+00]
29000     [3.56e+00, 1.30e+00, 4.02e+00]    [0.00e+00, 4.96e+00, 4.02e+00]    [6.35e+00, 4.96e+00, 6.23e+00, 5.08e+00]
30000     [3.54e+00, 1.32e+00, 4.00e+00]    [0.00e+00, 5.03e+00, 4.00e+00]    [6.38e+00, 5.03e+00, 6.29e+00, 5.14e+00]

Best model at step 30000:
  train loss: 8.86e+00
  test loss: 9.04e+00
  test metric: [6.38e+00, 5.03e+00, 6.29e+00, 5.14e+00]

'train' took 39.341379 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 3
[ 83 111 140   9  63  43  91 104 138 112] ==> [ 25  29 114 117  19  79 116  27  31 119 118  88 128  52 126 107 125  65
 124  97  14  42   2  77   1  56 132  37  86 105 142  75  95 120  54  87
  61  11  10  82  16  80  47  81  62  12  22  48  78 137 135  34  23  64
  90  13 102  49 133  41 123  55  15 106 103  20  66  35  17  89   4  45
  50  57  59  44  70  32 143 130  38 110  92  46 139   8  21 100  93 108
  30   5 127 134  71 101 122 136  85  67 141  84  72  94   7 115 121  53
  73  51 109  96  26  68  28  33  60  40  98   3   6  69  99  36 129  24
  39   0 131  58  74 113  18  76]
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.080156 s

'compile' took 0.381036 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.01e+02, 1.01e+02, 2.86e+00]    [0.00e+00, 9.99e+01, 2.86e+00]    [9.99e+01, 9.99e+01, 1.50e-01, 1.50e-01]
1000      [6.96e+01, 7.17e+01, 3.20e+00]    [0.00e+00, 7.96e+01, 3.20e+00]    [7.99e+01, 7.96e+01, 6.74e-01, 6.84e-01]
2000      [2.21e+01, 1.15e+01, 4.51e+00]    [0.00e+00, 2.41e+00, 4.51e+00]    [2.82e+00, 2.41e+00, 1.75e+00, 1.98e+00]
3000      [1.06e+01, 4.56e+00, 4.79e+00]    [0.00e+00, 3.82e+00, 4.79e+00]    [2.82e+00, 3.82e+00, 2.55e+00, 2.66e+00]
4000      [7.68e+00, 3.51e+00, 4.80e+00]    [0.00e+00, 4.19e+00, 4.80e+00]    [5.18e+00, 4.19e+00, 4.10e+00, 3.96e+00]
5000      [6.52e+00, 3.32e+00, 4.78e+00]    [0.00e+00, 4.54e+00, 4.78e+00]    [7.16e+00, 4.54e+00, 4.65e+00, 4.33e+00]
6000      [5.97e+00, 3.10e+00, 4.75e+00]    [0.00e+00, 4.55e+00, 4.75e+00]    [7.55e+00, 4.55e+00, 5.15e+00, 4.66e+00]
7000      [5.56e+00, 3.01e+00, 4.71e+00]    [0.00e+00, 4.85e+00, 4.71e+00]    [7.90e+00, 4.85e+00, 5.44e+00, 4.92e+00]
8000      [5.24e+00, 3.01e+00, 4.67e+00]    [0.00e+00, 5.08e+00, 4.67e+00]    [8.34e+00, 5.08e+00, 5.64e+00, 5.09e+00]
9000      [5.09e+00, 2.87e+00, 4.63e+00]    [0.00e+00, 5.21e+00, 4.63e+00]    [8.43e+00, 5.21e+00, 5.73e+00, 5.18e+00]
10000     [4.94e+00, 2.78e+00, 4.59e+00]    [0.00e+00, 5.27e+00, 4.59e+00]    [8.57e+00, 5.27e+00, 5.79e+00, 5.23e+00]
11000     [4.81e+00, 2.70e+00, 4.54e+00]    [0.00e+00, 5.25e+00, 4.54e+00]    [8.51e+00, 5.25e+00, 5.86e+00, 5.27e+00]
12000     [4.75e+00, 2.65e+00, 4.50e+00]    [0.00e+00, 5.15e+00, 4.50e+00]    [8.34e+00, 5.15e+00, 5.94e+00, 5.27e+00]
13000     [4.70e+00, 2.59e+00, 4.47e+00]    [0.00e+00, 5.01e+00, 4.47e+00]    [8.11e+00, 5.01e+00, 6.01e+00, 5.26e+00]
14000     [4.68e+00, 2.50e+00, 4.44e+00]    [0.00e+00, 4.84e+00, 4.44e+00]    [7.84e+00, 4.84e+00, 6.05e+00, 5.22e+00]
15000     [4.63e+00, 2.44e+00, 4.42e+00]    [0.00e+00, 4.70e+00, 4.42e+00]    [7.56e+00, 4.70e+00, 6.09e+00, 5.17e+00]
16000     [4.57e+00, 2.38e+00, 4.39e+00]    [0.00e+00, 4.63e+00, 4.39e+00]    [7.33e+00, 4.63e+00, 6.12e+00, 5.12e+00]
17000     [4.55e+00, 2.29e+00, 4.37e+00]    [0.00e+00, 4.55e+00, 4.37e+00]    [6.98e+00, 4.55e+00, 6.15e+00, 5.06e+00]
18000     [4.53e+00, 2.28e+00, 4.34e+00]    [0.00e+00, 4.53e+00, 4.34e+00]    [6.91e+00, 4.53e+00, 6.15e+00, 5.03e+00]
19000     [4.49e+00, 2.24e+00, 4.32e+00]    [0.00e+00, 4.48e+00, 4.32e+00]    [6.86e+00, 4.48e+00, 6.13e+00, 4.98e+00]
20000     [4.43e+00, 2.18e+00, 4.29e+00]    [0.00e+00, 4.43e+00, 4.29e+00]    [6.80e+00, 4.43e+00, 6.11e+00, 4.94e+00]
21000     [4.43e+00, 2.18e+00, 4.26e+00]    [0.00e+00, 4.41e+00, 4.26e+00]    [6.77e+00, 4.41e+00, 6.10e+00, 4.91e+00]
22000     [4.35e+00, 2.15e+00, 4.24e+00]    [0.00e+00, 4.40e+00, 4.24e+00]    [6.75e+00, 4.40e+00, 6.09e+00, 4.90e+00]
23000     [4.34e+00, 2.16e+00, 4.22e+00]    [0.00e+00, 4.39e+00, 4.22e+00]    [6.74e+00, 4.39e+00, 6.09e+00, 4.89e+00]
24000     [4.26e+00, 2.11e+00, 4.19e+00]    [0.00e+00, 4.38e+00, 4.19e+00]    [6.69e+00, 4.38e+00, 6.08e+00, 4.87e+00]
25000     [4.23e+00, 2.11e+00, 4.17e+00]    [0.00e+00, 4.38e+00, 4.17e+00]    [6.68e+00, 4.38e+00, 6.08e+00, 4.87e+00]
26000     [4.18e+00, 2.10e+00, 4.15e+00]    [0.00e+00, 4.38e+00, 4.15e+00]    [6.69e+00, 4.38e+00, 6.09e+00, 4.87e+00]
27000     [4.13e+00, 2.09e+00, 4.13e+00]    [0.00e+00, 4.39e+00, 4.13e+00]    [6.72e+00, 4.39e+00, 6.10e+00, 4.88e+00]
28000     [4.10e+00, 2.10e+00, 4.12e+00]    [0.00e+00, 4.41e+00, 4.12e+00]    [6.75e+00, 4.41e+00, 6.12e+00, 4.89e+00]
29000     [4.06e+00, 2.08e+00, 4.10e+00]    [0.00e+00, 4.42e+00, 4.10e+00]    [6.78e+00, 4.42e+00, 6.13e+00, 4.90e+00]
30000     [4.01e+00, 2.07e+00, 4.08e+00]    [0.00e+00, 4.44e+00, 4.08e+00]    [6.78e+00, 4.44e+00, 6.15e+00, 4.91e+00]

Best model at step 30000:
  train loss: 1.02e+01
  test loss: 8.52e+00
  test metric: [6.78e+00, 4.44e+00, 6.15e+00, 4.91e+00]

'train' took 40.698180 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 4
[ 66 138 115  28 134  70 143   3  34  88] ==> [ 98  13  14  22  75  15  11  99 128  39  96  65 117  19  30 104 112  97
 118  93  27  95   6   2 122  94  56  38 137  76 120 111 103 108  20  91
  52  64  26  44 133 119  48  87  68  83 125   9  80   0  60  41  43  50
  61 110  23  51 142  32 127 135 132 105  29 106  46  53   7  18  33  79
  55  49  84  36  40 121 140  24  63  85  17   4  58  69  82 113 141  89
  57  10  67  42  59  47 107 114  71  72  62  78 126  74  90  12 139   5
   1  37 116  16  86 131  45 124  25   8 136 129  35  81  31 130  21  77
  54 100  73 109 123 101 102  92]
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.081305 s

'compile' took 0.384649 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.01e+02, 1.00e+02, 2.82e+00]    [0.00e+00, 9.94e+01, 2.82e+00]    [9.94e+01, 9.94e+01, 4.10e-02, 4.10e-02]
1000      [6.90e+01, 7.03e+01, 3.08e+00]    [0.00e+00, 7.70e+01, 3.08e+00]    [7.74e+01, 7.70e+01, 8.49e-01, 8.63e-01]
2000      [1.97e+01, 1.36e+01, 4.26e+00]    [0.00e+00, 8.85e+00, 4.26e+00]    [4.30e+00, 8.85e+00, 2.44e+00, 2.69e+00]
3000      [7.27e+00, 1.01e+01, 4.46e+00]    [0.00e+00, 1.90e+01, 4.46e+00]    [1.77e+01, 1.90e+01, 2.36e+00, 2.39e+00]
4000      [7.33e+00, 7.00e+00, 4.42e+00]    [0.00e+00, 1.10e+01, 4.42e+00]    [1.16e+01, 1.10e+01, 2.48e+00, 2.44e+00]
5000      [7.25e+00, 4.70e+00, 4.42e+00]    [0.00e+00, 4.40e+00, 4.42e+00]    [8.24e+00, 4.40e+00, 2.54e+00, 2.33e+00]
6000      [6.99e+00, 3.98e+00, 4.41e+00]    [0.00e+00, 2.36e+00, 4.41e+00]    [7.69e+00, 2.36e+00, 2.76e+00, 2.21e+00]
7000      [6.46e+00, 3.93e+00, 4.39e+00]    [0.00e+00, 2.45e+00, 4.39e+00]    [8.83e+00, 2.45e+00, 3.05e+00, 2.39e+00]
8000      [6.02e+00, 3.97e+00, 4.36e+00]    [0.00e+00, 2.58e+00, 4.36e+00]    [8.73e+00, 2.58e+00, 3.37e+00, 2.60e+00]
9000      [5.60e+00, 3.97e+00, 4.35e+00]    [0.00e+00, 2.78e+00, 4.35e+00]    [8.86e+00, 2.78e+00, 3.69e+00, 2.84e+00]
10000     [5.27e+00, 3.92e+00, 4.34e+00]    [0.00e+00, 2.98e+00, 4.34e+00]    [8.70e+00, 2.98e+00, 3.92e+00, 3.06e+00]
11000     [4.97e+00, 3.91e+00, 4.33e+00]    [0.00e+00, 3.17e+00, 4.33e+00]    [8.58e+00, 3.17e+00, 4.16e+00, 3.25e+00]
12000     [4.71e+00, 3.89e+00, 4.32e+00]    [0.00e+00, 3.34e+00, 4.32e+00]    [8.63e+00, 3.34e+00, 4.36e+00, 3.42e+00]
13000     [4.53e+00, 3.87e+00, 4.30e+00]    [0.00e+00, 3.47e+00, 4.30e+00]    [8.49e+00, 3.47e+00, 4.52e+00, 3.55e+00]
14000     [4.39e+00, 3.85e+00, 4.29e+00]    [0.00e+00, 3.58e+00, 4.29e+00]    [8.41e+00, 3.58e+00, 4.64e+00, 3.66e+00]
15000     [4.26e+00, 3.84e+00, 4.26e+00]    [0.00e+00, 3.68e+00, 4.26e+00]    [8.13e+00, 3.68e+00, 4.75e+00, 3.77e+00]
16000     [4.14e+00, 3.84e+00, 4.24e+00]    [0.00e+00, 3.79e+00, 4.24e+00]    [7.83e+00, 3.79e+00, 4.84e+00, 3.89e+00]
17000     [4.10e+00, 3.81e+00, 4.22e+00]    [0.00e+00, 3.86e+00, 4.22e+00]    [7.54e+00, 3.86e+00, 4.89e+00, 3.97e+00]
18000     [4.05e+00, 3.76e+00, 4.19e+00]    [0.00e+00, 3.91e+00, 4.19e+00]    [7.30e+00, 3.91e+00, 4.93e+00, 4.02e+00]
19000     [3.99e+00, 3.69e+00, 4.17e+00]    [0.00e+00, 3.94e+00, 4.17e+00]    [7.22e+00, 3.94e+00, 4.95e+00, 4.04e+00]
20000     [3.93e+00, 3.65e+00, 4.14e+00]    [0.00e+00, 3.98e+00, 4.14e+00]    [7.13e+00, 3.98e+00, 4.98e+00, 4.08e+00]
21000     [3.93e+00, 3.61e+00, 4.11e+00]    [0.00e+00, 4.00e+00, 4.11e+00]    [6.90e+00, 4.00e+00, 5.00e+00, 4.10e+00]
22000     [3.91e+00, 3.56e+00, 4.09e+00]    [0.00e+00, 4.00e+00, 4.09e+00]    [6.61e+00, 4.00e+00, 5.02e+00, 4.10e+00]
23000     [3.90e+00, 3.50e+00, 4.07e+00]    [0.00e+00, 3.97e+00, 4.07e+00]    [6.39e+00, 3.97e+00, 5.02e+00, 4.09e+00]
24000     [3.92e+00, 3.45e+00, 4.05e+00]    [0.00e+00, 3.96e+00, 4.05e+00]    [6.19e+00, 3.96e+00, 5.01e+00, 4.09e+00]
25000     [3.88e+00, 3.40e+00, 4.03e+00]    [0.00e+00, 3.95e+00, 4.03e+00]    [6.07e+00, 3.95e+00, 5.00e+00, 4.09e+00]
26000     [3.85e+00, 3.38e+00, 4.00e+00]    [0.00e+00, 3.96e+00, 4.00e+00]    [6.03e+00, 3.96e+00, 5.00e+00, 4.09e+00]
27000     [3.84e+00, 3.38e+00, 3.98e+00]    [0.00e+00, 3.96e+00, 3.98e+00]    [6.02e+00, 3.96e+00, 5.01e+00, 4.10e+00]
28000     [3.80e+00, 3.32e+00, 3.96e+00]    [0.00e+00, 3.97e+00, 3.96e+00]    [6.00e+00, 3.97e+00, 5.02e+00, 4.11e+00]
29000     [3.83e+00, 3.37e+00, 3.94e+00]    [0.00e+00, 3.98e+00, 3.94e+00]    [6.01e+00, 3.98e+00, 5.03e+00, 4.12e+00]
30000     [3.76e+00, 3.31e+00, 3.92e+00]    [0.00e+00, 3.98e+00, 3.92e+00]    [5.96e+00, 3.98e+00, 5.02e+00, 4.12e+00]

Best model at step 30000:
  train loss: 1.10e+01
  test loss: 7.90e+00
  test metric: [5.96e+00, 3.98e+00, 5.02e+00, 4.12e+00]

'train' took 37.977055 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 5
[134   0  54  45  28 123  39  18  72  20] ==> [ 46  90  25 139  55  86  24 127 114  58   8  69  49  77  84 124  91  88
  31  98  92 104  34  42  96  11  89 138 137  35 140  41  82  70 125  27
  57 108  67  61   4 119  14  95  56  44 106  17  78 130 101  87  60 136
   3  74  80 120 135  71  16  52   7 126  73  83  68  36  81  19  22 110
  64 117 113  94  32  59  37  63 121 133  30  26  40  48 111 107  13 141
  21  93  47 103  62  79  43  65  53 116 142   9  29  76   2  51 132  85
  75 115 131 112   6  10   1  38   5 122  23  33 129  15  12  97  66 105
 109 100 128 118 143 102  50  99]
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.079203 s

'compile' took 0.377792 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [9.71e+01, 9.98e+01, 2.97e+00]    [0.00e+00, 1.01e+02, 2.97e+00]    [1.01e+02, 1.01e+02, 1.10e-01, 1.10e-01]
1000      [7.15e+01, 7.32e+01, 3.14e+00]    [0.00e+00, 7.96e+01, 3.14e+00]    [8.00e+01, 7.96e+01, 5.94e-01, 6.05e-01]
2000      [2.29e+01, 1.29e+01, 4.33e+00]    [0.00e+00, 1.03e+01, 4.33e+00]    [2.48e+00, 1.03e+01, 1.47e+00, 1.79e+00]
3000      [7.71e+00, 1.05e+01, 4.59e+00]    [0.00e+00, 2.14e+01, 4.59e+00]    [1.99e+01, 2.14e+01, 2.05e+00, 2.08e+00]
4000      [6.50e+00, 7.13e+00, 4.61e+00]    [0.00e+00, 1.21e+01, 4.61e+00]    [1.76e+01, 1.21e+01, 2.56e+00, 2.44e+00]
5000      [6.79e+00, 4.61e+00, 4.61e+00]    [0.00e+00, 4.97e+00, 4.61e+00]    [1.33e+01, 4.97e+00, 3.04e+00, 2.65e+00]
6000      [6.29e+00, 3.91e+00, 4.62e+00]    [0.00e+00, 4.65e+00, 4.62e+00]    [1.30e+01, 4.65e+00, 3.49e+00, 2.94e+00]
7000      [5.56e+00, 3.59e+00, 4.62e+00]    [0.00e+00, 4.40e+00, 4.62e+00]    [1.21e+01, 4.40e+00, 3.94e+00, 3.38e+00]
8000      [5.20e+00, 3.35e+00, 4.60e+00]    [0.00e+00, 4.35e+00, 4.60e+00]    [1.14e+01, 4.35e+00, 4.17e+00, 3.60e+00]
9000      [4.97e+00, 3.12e+00, 4.59e+00]    [0.00e+00, 4.58e+00, 4.59e+00]    [1.09e+01, 4.58e+00, 4.40e+00, 3.83e+00]
10000     [4.76e+00, 3.01e+00, 4.58e+00]    [0.00e+00, 4.66e+00, 4.58e+00]    [1.05e+01, 4.66e+00, 4.60e+00, 4.00e+00]
11000     [4.62e+00, 2.88e+00, 4.56e+00]    [0.00e+00, 4.75e+00, 4.56e+00]    [1.03e+01, 4.75e+00, 4.72e+00, 4.10e+00]
12000     [4.45e+00, 2.78e+00, 4.54e+00]    [0.00e+00, 4.86e+00, 4.54e+00]    [1.00e+01, 4.86e+00, 4.86e+00, 4.23e+00]
13000     [4.39e+00, 2.67e+00, 4.52e+00]    [0.00e+00, 4.83e+00, 4.52e+00]    [9.81e+00, 4.83e+00, 4.89e+00, 4.26e+00]
14000     [4.30e+00, 2.54e+00, 4.49e+00]    [0.00e+00, 4.86e+00, 4.49e+00]    [9.71e+00, 4.86e+00, 4.93e+00, 4.29e+00]
15000     [4.25e+00, 2.40e+00, 4.47e+00]    [0.00e+00, 4.83e+00, 4.47e+00]    [9.47e+00, 4.83e+00, 4.95e+00, 4.29e+00]
16000     [4.19e+00, 2.35e+00, 4.45e+00]    [0.00e+00, 4.83e+00, 4.45e+00]    [9.50e+00, 4.83e+00, 5.00e+00, 4.31e+00]
17000     [4.15e+00, 2.18e+00, 4.43e+00]    [0.00e+00, 4.45e+00, 4.43e+00]    [8.91e+00, 4.45e+00, 5.02e+00, 4.29e+00]
18000     [4.14e+00, 2.14e+00, 4.40e+00]    [0.00e+00, 4.39e+00, 4.40e+00]    [8.78e+00, 4.39e+00, 5.06e+00, 4.31e+00]
19000     [4.08e+00, 2.10e+00, 4.37e+00]    [0.00e+00, 4.41e+00, 4.37e+00]    [8.70e+00, 4.41e+00, 5.09e+00, 4.34e+00]
20000     [4.01e+00, 2.03e+00, 4.34e+00]    [0.00e+00, 4.41e+00, 4.34e+00]    [8.67e+00, 4.41e+00, 5.13e+00, 4.37e+00]
21000     [4.00e+00, 2.05e+00, 4.31e+00]    [0.00e+00, 4.43e+00, 4.31e+00]    [8.62e+00, 4.43e+00, 5.17e+00, 4.41e+00]
22000     [3.92e+00, 2.05e+00, 4.29e+00]    [0.00e+00, 4.45e+00, 4.29e+00]    [8.59e+00, 4.45e+00, 5.21e+00, 4.45e+00]
23000     [3.84e+00, 2.02e+00, 4.26e+00]    [0.00e+00, 4.47e+00, 4.26e+00]    [8.55e+00, 4.47e+00, 5.25e+00, 4.50e+00]
24000     [3.79e+00, 2.01e+00, 4.24e+00]    [0.00e+00, 4.51e+00, 4.24e+00]    [8.52e+00, 4.51e+00, 5.27e+00, 4.52e+00]
25000     [3.84e+00, 2.07e+00, 4.21e+00]    [0.00e+00, 4.54e+00, 4.21e+00]    [8.50e+00, 4.54e+00, 5.29e+00, 4.55e+00]
26000     [3.71e+00, 1.99e+00, 4.19e+00]    [0.00e+00, 4.56e+00, 4.19e+00]    [8.46e+00, 4.56e+00, 5.32e+00, 4.58e+00]
27000     [3.71e+00, 1.98e+00, 4.17e+00]    [0.00e+00, 4.59e+00, 4.17e+00]    [8.42e+00, 4.59e+00, 5.34e+00, 4.61e+00]
28000     [3.66e+00, 1.99e+00, 4.14e+00]    [0.00e+00, 4.63e+00, 4.14e+00]    [8.35e+00, 4.63e+00, 5.36e+00, 4.64e+00]
29000     [3.67e+00, 2.01e+00, 4.12e+00]    [0.00e+00, 4.66e+00, 4.12e+00]    [8.29e+00, 4.66e+00, 5.38e+00, 4.67e+00]
30000     [3.59e+00, 1.96e+00, 4.10e+00]    [0.00e+00, 4.71e+00, 4.10e+00]    [8.28e+00, 4.71e+00, 5.41e+00, 4.70e+00]

Best model at step 30000:
  train loss: 9.64e+00
  test loss: 8.82e+00
  test metric: [8.28e+00, 4.71e+00, 5.41e+00, 4.70e+00]

'train' took 39.148064 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 6
[135  40  93  60 123  74 106   4  19  30] ==> [121  27  77  32  28   1 143 127 140  56  12  18  90  62 114  70 137  58
 117  84  85  75 134 103  44  66 118  14  15 128 109  81  73 130  64 122
 105 138 129 107 139  45 113  31  50  61  51  53  97 104  46 100  95  96
  34  68  76  83  55  67  52  25 119 108  57  42  20  92  47  98  13 110
   3  91  24  78 141   9  49  35  86 131 112  36 133 125 136  38   5  10
   8   2  21 120  82  54  71  48 116   7  33 102  11  22 142  69  43  80
  17  41 101  79  88   6 115  89  26  63 111  37  87   0  72  23  65 124
 132  39  99  16 126  94  29  59]
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.099246 s

'compile' took 0.401938 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.03e+02, 1.01e+02, 2.72e+00]    [0.00e+00, 9.90e+01, 2.72e+00]    [9.90e+01, 9.90e+01, 7.89e-02, 7.89e-02]
1000      [7.38e+01, 7.71e+01, 2.90e+00]    [0.00e+00, 8.62e+01, 2.90e+00]    [8.64e+01, 8.62e+01, 3.62e-01, 3.67e-01]
2000      [2.20e+01, 1.42e+01, 4.19e+00]    [0.00e+00, 1.16e+01, 4.19e+00]    [4.49e+00, 1.16e+01, 2.51e+00, 2.92e+00]
3000      [6.54e+00, 1.22e+01, 4.45e+00]    [0.00e+00, 2.60e+01, 4.45e+00]    [2.41e+01, 2.60e+01, 2.55e+00, 2.58e+00]
4000      [6.19e+00, 8.62e+00, 4.45e+00]    [0.00e+00, 1.70e+01, 4.45e+00]    [1.78e+01, 1.70e+01, 3.28e+00, 3.26e+00]
5000      [7.22e+00, 4.39e+00, 4.46e+00]    [0.00e+00, 5.45e+00, 4.46e+00]    [9.15e+00, 5.45e+00, 3.74e+00, 3.48e+00]
6000      [6.55e+00, 3.24e+00, 4.47e+00]    [0.00e+00, 3.51e+00, 4.47e+00]    [8.58e+00, 3.51e+00, 4.51e+00, 3.71e+00]
7000      [5.74e+00, 3.14e+00, 4.44e+00]    [0.00e+00, 3.95e+00, 4.44e+00]    [9.14e+00, 3.95e+00, 5.06e+00, 4.17e+00]
8000      [5.28e+00, 2.83e+00, 4.42e+00]    [0.00e+00, 4.12e+00, 4.42e+00]    [8.84e+00, 4.12e+00, 5.49e+00, 4.48e+00]
9000      [4.92e+00, 2.67e+00, 4.39e+00]    [0.00e+00, 4.26e+00, 4.39e+00]    [8.74e+00, 4.26e+00, 5.66e+00, 4.64e+00]
10000     [4.75e+00, 2.55e+00, 4.34e+00]    [0.00e+00, 4.36e+00, 4.34e+00]    [8.62e+00, 4.36e+00, 5.75e+00, 4.73e+00]
11000     [4.62e+00, 2.54e+00, 4.30e+00]    [0.00e+00, 4.43e+00, 4.30e+00]    [8.56e+00, 4.43e+00, 5.82e+00, 4.80e+00]
12000     [4.54e+00, 2.51e+00, 4.27e+00]    [0.00e+00, 4.50e+00, 4.27e+00]    [8.52e+00, 4.50e+00, 5.86e+00, 4.86e+00]
13000     [4.43e+00, 2.47e+00, 4.23e+00]    [0.00e+00, 4.54e+00, 4.23e+00]    [8.46e+00, 4.54e+00, 5.89e+00, 4.89e+00]
14000     [4.37e+00, 2.46e+00, 4.20e+00]    [0.00e+00, 4.60e+00, 4.20e+00]    [8.46e+00, 4.60e+00, 5.93e+00, 4.94e+00]
15000     [4.29e+00, 2.41e+00, 4.17e+00]    [0.00e+00, 4.63e+00, 4.17e+00]    [8.41e+00, 4.63e+00, 5.97e+00, 4.98e+00]
16000     [4.26e+00, 2.36e+00, 4.14e+00]    [0.00e+00, 4.63e+00, 4.14e+00]    [8.21e+00, 4.63e+00, 6.01e+00, 4.99e+00]
17000     [4.24e+00, 2.36e+00, 4.12e+00]    [0.00e+00, 4.62e+00, 4.12e+00]    [7.98e+00, 4.62e+00, 6.07e+00, 5.00e+00]
18000     [4.13e+00, 2.29e+00, 4.10e+00]    [0.00e+00, 4.68e+00, 4.10e+00]    [7.89e+00, 4.68e+00, 6.12e+00, 5.04e+00]
19000     [4.09e+00, 2.30e+00, 4.07e+00]    [0.00e+00, 4.76e+00, 4.07e+00]    [7.84e+00, 4.76e+00, 6.18e+00, 5.10e+00]
20000     [4.04e+00, 2.29e+00, 4.06e+00]    [0.00e+00, 4.82e+00, 4.06e+00]    [7.78e+00, 4.82e+00, 6.25e+00, 5.16e+00]
21000     [3.91e+00, 2.24e+00, 4.04e+00]    [0.00e+00, 4.91e+00, 4.04e+00]    [7.79e+00, 4.91e+00, 6.31e+00, 5.23e+00]
22000     [3.86e+00, 2.25e+00, 4.02e+00]    [0.00e+00, 4.97e+00, 4.02e+00]    [7.76e+00, 4.97e+00, 6.37e+00, 5.28e+00]
23000     [3.80e+00, 2.25e+00, 4.00e+00]    [0.00e+00, 5.03e+00, 4.00e+00]    [7.77e+00, 5.03e+00, 6.42e+00, 5.32e+00]
24000     [3.72e+00, 2.22e+00, 3.98e+00]    [0.00e+00, 5.08e+00, 3.98e+00]    [7.76e+00, 5.08e+00, 6.46e+00, 5.36e+00]
25000     [3.66e+00, 2.25e+00, 3.96e+00]    [0.00e+00, 5.15e+00, 3.96e+00]    [7.74e+00, 5.15e+00, 6.51e+00, 5.42e+00]
26000     [3.63e+00, 2.22e+00, 3.94e+00]    [0.00e+00, 5.18e+00, 3.94e+00]    [7.69e+00, 5.18e+00, 6.54e+00, 5.45e+00]
27000     [3.58e+00, 2.23e+00, 3.92e+00]    [0.00e+00, 5.24e+00, 3.92e+00]    [7.71e+00, 5.24e+00, 6.59e+00, 5.50e+00]
28000     [3.54e+00, 2.24e+00, 3.90e+00]    [0.00e+00, 5.29e+00, 3.90e+00]    [7.73e+00, 5.29e+00, 6.64e+00, 5.54e+00]
29000     [3.47e+00, 2.22e+00, 3.88e+00]    [0.00e+00, 5.33e+00, 3.88e+00]    [7.70e+00, 5.33e+00, 6.67e+00, 5.58e+00]
30000     [3.44e+00, 2.21e+00, 3.86e+00]    [0.00e+00, 5.38e+00, 3.86e+00]    [7.68e+00, 5.38e+00, 6.71e+00, 5.61e+00]

Best model at step 30000:
  train loss: 9.51e+00
  test loss: 9.24e+00
  test metric: [7.68e+00, 5.38e+00, 6.71e+00, 5.61e+00]

'train' took 41.115276 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 7
[103 123  60  16 139  11  78  29 135   3] ==> [ 14  30  19 118  24  37  84  36   8  48  86  39 136  49  47 102  59  62
  56 100   5  54  42 115  61  52   7 113   9 125 119  35  96  82 108 142
  94  74   6   0  38 116  17   2  26  41 104  69  98  71   4  66  46 110
 141  89 133  95  99  63 128  53  43 101 127  20  18  13 120  75  27 121
 130  87  92  57  50 105  91  67  21  70  80  10  45  12  51 126  97  79
  93  64  22  31  68  23  32  83  77  72  40  44 112  33 124 107 137   1
  34 109  90 114 134  25 138  76  65 111 140 122  58 132 131  88 106 143
  15  55  81 117  73  85  28 129]
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.079087 s

'compile' took 0.391699 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [9.94e+01, 9.99e+01, 2.62e+00]    [0.00e+00, 1.00e+02, 2.62e+00]    [1.00e+02, 1.00e+02, 7.57e-02, 7.57e-02]
1000      [6.95e+01, 7.18e+01, 2.93e+00]    [0.00e+00, 7.96e+01, 2.93e+00]    [7.99e+01, 7.96e+01, 6.90e-01, 6.99e-01]
2000      [2.09e+01, 1.25e+01, 4.23e+00]    [0.00e+00, 5.01e+00, 4.23e+00]    [2.27e+00, 5.01e+00, 2.34e+00, 2.97e+00]
3000      [7.59e+00, 8.36e+00, 4.53e+00]    [0.00e+00, 1.39e+01, 4.53e+00]    [1.41e+01, 1.39e+01, 2.63e+00, 2.62e+00]
4000      [7.67e+00, 3.97e+00, 4.54e+00]    [0.00e+00, 2.65e+00, 4.54e+00]    [7.33e+00, 2.65e+00, 3.05e+00, 2.67e+00]
5000      [6.70e+00, 3.72e+00, 4.51e+00]    [0.00e+00, 2.50e+00, 4.51e+00]    [9.23e+00, 2.50e+00, 3.27e+00, 2.72e+00]
6000      [6.07e+00, 3.62e+00, 4.47e+00]    [0.00e+00, 2.65e+00, 4.47e+00]    [9.85e+00, 2.65e+00, 3.55e+00, 2.90e+00]
7000      [5.64e+00, 3.50e+00, 4.44e+00]    [0.00e+00, 2.84e+00, 4.44e+00]    [1.00e+01, 2.84e+00, 3.82e+00, 3.12e+00]
8000      [5.36e+00, 3.37e+00, 4.41e+00]    [0.00e+00, 2.99e+00, 4.41e+00]    [1.00e+01, 2.99e+00, 4.05e+00, 3.29e+00]
9000      [5.12e+00, 3.31e+00, 4.38e+00]    [0.00e+00, 3.11e+00, 4.38e+00]    [9.55e+00, 3.11e+00, 4.23e+00, 3.45e+00]
10000     [4.91e+00, 3.20e+00, 4.35e+00]    [0.00e+00, 3.24e+00, 4.35e+00]    [9.19e+00, 3.24e+00, 4.35e+00, 3.56e+00]
11000     [4.78e+00, 3.10e+00, 4.32e+00]    [0.00e+00, 3.33e+00, 4.32e+00]    [8.78e+00, 3.33e+00, 4.45e+00, 3.64e+00]
12000     [4.67e+00, 3.02e+00, 4.30e+00]    [0.00e+00, 3.40e+00, 4.30e+00]    [8.49e+00, 3.40e+00, 4.53e+00, 3.72e+00]
13000     [4.57e+00, 2.94e+00, 4.28e+00]    [0.00e+00, 3.49e+00, 4.28e+00]    [8.32e+00, 3.49e+00, 4.61e+00, 3.81e+00]
14000     [4.50e+00, 2.92e+00, 4.27e+00]    [0.00e+00, 3.55e+00, 4.27e+00]    [8.31e+00, 3.55e+00, 4.67e+00, 3.86e+00]
15000     [4.44e+00, 2.86e+00, 4.25e+00]    [0.00e+00, 3.61e+00, 4.25e+00]    [8.31e+00, 3.61e+00, 4.74e+00, 3.93e+00]
16000     [4.33e+00, 2.80e+00, 4.24e+00]    [0.00e+00, 3.69e+00, 4.24e+00]    [8.34e+00, 3.69e+00, 4.83e+00, 4.01e+00]
17000     [4.26e+00, 2.71e+00, 4.22e+00]    [0.00e+00, 3.70e+00, 4.22e+00]    [8.11e+00, 3.70e+00, 4.86e+00, 4.03e+00]
18000     [4.25e+00, 2.65e+00, 4.20e+00]    [0.00e+00, 3.71e+00, 4.20e+00]    [7.93e+00, 3.71e+00, 4.86e+00, 4.04e+00]
19000     [4.21e+00, 2.56e+00, 4.18e+00]    [0.00e+00, 3.69e+00, 4.18e+00]    [7.84e+00, 3.69e+00, 4.85e+00, 4.02e+00]
20000     [4.19e+00, 2.50e+00, 4.16e+00]    [0.00e+00, 3.67e+00, 4.16e+00]    [7.71e+00, 3.67e+00, 4.85e+00, 4.00e+00]
21000     [4.22e+00, 2.48e+00, 4.14e+00]    [0.00e+00, 3.65e+00, 4.14e+00]    [7.58e+00, 3.65e+00, 4.84e+00, 3.99e+00]
22000     [4.15e+00, 2.40e+00, 4.12e+00]    [0.00e+00, 3.64e+00, 4.12e+00]    [7.54e+00, 3.64e+00, 4.84e+00, 3.98e+00]
23000     [4.16e+00, 2.33e+00, 4.10e+00]    [0.00e+00, 3.62e+00, 4.10e+00]    [7.46e+00, 3.62e+00, 4.83e+00, 3.97e+00]
24000     [4.11e+00, 2.27e+00, 4.07e+00]    [0.00e+00, 3.60e+00, 4.07e+00]    [7.30e+00, 3.60e+00, 4.83e+00, 3.96e+00]
25000     [4.16e+00, 2.25e+00, 4.05e+00]    [0.00e+00, 3.57e+00, 4.05e+00]    [7.10e+00, 3.57e+00, 4.81e+00, 3.95e+00]
26000     [4.14e+00, 2.18e+00, 4.03e+00]    [0.00e+00, 3.54e+00, 4.03e+00]    [6.88e+00, 3.54e+00, 4.79e+00, 3.94e+00]
27000     [4.25e+00, 2.20e+00, 4.01e+00]    [0.00e+00, 3.51e+00, 4.01e+00]    [6.69e+00, 3.51e+00, 4.79e+00, 3.93e+00]
28000     [4.14e+00, 2.08e+00, 3.98e+00]    [0.00e+00, 3.49e+00, 3.98e+00]    [6.54e+00, 3.49e+00, 4.78e+00, 3.92e+00]
29000     [4.14e+00, 2.09e+00, 3.96e+00]    [0.00e+00, 3.52e+00, 3.96e+00]    [6.52e+00, 3.52e+00, 4.77e+00, 3.92e+00]
30000     [4.13e+00, 2.06e+00, 3.94e+00]    [0.00e+00, 3.54e+00, 3.94e+00]    [6.55e+00, 3.54e+00, 4.78e+00, 3.92e+00]

Best model at step 30000:
  train loss: 1.01e+01
  test loss: 7.48e+00
  test metric: [6.55e+00, 3.54e+00, 4.78e+00, 3.92e+00]

'train' took 39.303809 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 8
[119  81  32 134 100  90 132  29  73 131] ==> [ 88 124  20 128  18  23 121  64  25  11 139 117  45   5  97 112  22  89
   6 110  31  17  15 104  46 115  41  71  63 142  62 101  43  98  55  26
 113  83 106  79  74  67   2   1  13 143 133  85 114  24  52 102  66  10
  42  39  21 123  94  65  54 120  53  33   3 130  69  60 129 122  14  87
  44  38  19  30 138  91  70 109 126  96   9 105  49 103  48  35  99  82
  80  36   4  57  76  68  50  61  47 140  77 116 107   0  40  93  75  84
 136  12   7 108 127  72  16 111  92 135  58  34 125  28 141  56   8  95
  86  59  78  51  27 137  37 118]
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.079191 s

'compile' took 0.374489 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [9.90e+01, 1.00e+02, 2.76e+00]    [0.00e+00, 1.00e+02, 2.76e+00]    [1.00e+02, 1.00e+02, 6.60e-02, 6.60e-02]
1000      [7.05e+01, 7.21e+01, 3.08e+00]    [0.00e+00, 7.83e+01, 3.08e+00]    [7.86e+01, 7.83e+01, 6.36e-01, 6.45e-01]
2000      [2.48e+01, 1.38e+01, 4.40e+00]    [0.00e+00, 5.13e+00, 4.40e+00]    [1.75e+00, 5.13e+00, 1.90e+00, 2.53e+00]
3000      [9.10e+00, 6.76e+00, 4.75e+00]    [0.00e+00, 1.09e+01, 4.75e+00]    [9.64e+00, 1.09e+01, 2.21e+00, 2.23e+00]
4000      [9.02e+00, 3.22e+00, 4.74e+00]    [0.00e+00, 2.28e+00, 4.74e+00]    [2.73e+00, 2.28e+00, 2.60e+00, 2.47e+00]
5000      [7.55e+00, 3.09e+00, 4.73e+00]    [0.00e+00, 2.57e+00, 4.73e+00]    [5.09e+00, 2.57e+00, 3.32e+00, 2.88e+00]
6000      [6.64e+00, 2.95e+00, 4.71e+00]    [0.00e+00, 2.90e+00, 4.71e+00]    [6.69e+00, 2.90e+00, 3.73e+00, 3.19e+00]
7000      [6.02e+00, 2.84e+00, 4.69e+00]    [0.00e+00, 3.23e+00, 4.69e+00]    [7.39e+00, 3.23e+00, 4.12e+00, 3.51e+00]
8000      [5.69e+00, 2.64e+00, 4.67e+00]    [0.00e+00, 3.30e+00, 4.67e+00]    [7.45e+00, 3.30e+00, 4.37e+00, 3.67e+00]
9000      [5.53e+00, 2.45e+00, 4.65e+00]    [0.00e+00, 3.21e+00, 4.65e+00]    [6.88e+00, 3.21e+00, 4.59e+00, 3.73e+00]
10000     [5.24e+00, 2.36e+00, 4.64e+00]    [0.00e+00, 3.43e+00, 4.64e+00]    [6.88e+00, 3.43e+00, 4.84e+00, 3.96e+00]
11000     [5.07e+00, 2.25e+00, 4.62e+00]    [0.00e+00, 3.52e+00, 4.62e+00]    [7.10e+00, 3.52e+00, 4.97e+00, 4.06e+00]
12000     [4.89e+00, 2.19e+00, 4.60e+00]    [0.00e+00, 3.58e+00, 4.60e+00]    [7.50e+00, 3.58e+00, 5.09e+00, 4.11e+00]
13000     [4.86e+00, 2.16e+00, 4.57e+00]    [0.00e+00, 3.59e+00, 4.57e+00]    [7.47e+00, 3.59e+00, 5.11e+00, 4.14e+00]
14000     [4.69e+00, 2.10e+00, 4.54e+00]    [0.00e+00, 3.63e+00, 4.54e+00]    [7.48e+00, 3.63e+00, 5.15e+00, 4.17e+00]
15000     [4.59e+00, 2.07e+00, 4.51e+00]    [0.00e+00, 3.63e+00, 4.51e+00]    [7.39e+00, 3.63e+00, 5.16e+00, 4.18e+00]
16000     [4.59e+00, 2.06e+00, 4.48e+00]    [0.00e+00, 3.62e+00, 4.48e+00]    [7.17e+00, 3.62e+00, 5.16e+00, 4.17e+00]
17000     [4.49e+00, 2.03e+00, 4.45e+00]    [0.00e+00, 3.64e+00, 4.45e+00]    [7.01e+00, 3.64e+00, 5.19e+00, 4.19e+00]
18000     [4.39e+00, 1.99e+00, 4.43e+00]    [0.00e+00, 3.66e+00, 4.43e+00]    [6.85e+00, 3.66e+00, 5.21e+00, 4.20e+00]
19000     [4.32e+00, 1.98e+00, 4.40e+00]    [0.00e+00, 3.69e+00, 4.40e+00]    [6.81e+00, 3.69e+00, 5.23e+00, 4.22e+00]
20000     [4.22e+00, 1.98e+00, 4.37e+00]    [0.00e+00, 3.72e+00, 4.37e+00]    [6.78e+00, 3.72e+00, 5.26e+00, 4.25e+00]
21000     [4.15e+00, 1.98e+00, 4.34e+00]    [0.00e+00, 3.75e+00, 4.34e+00]    [6.74e+00, 3.75e+00, 5.27e+00, 4.27e+00]
22000     [4.12e+00, 1.97e+00, 4.32e+00]    [0.00e+00, 3.77e+00, 4.32e+00]    [6.70e+00, 3.77e+00, 5.29e+00, 4.28e+00]
23000     [4.04e+00, 1.96e+00, 4.29e+00]    [0.00e+00, 3.79e+00, 4.29e+00]    [6.69e+00, 3.79e+00, 5.31e+00, 4.29e+00]
24000     [4.00e+00, 1.95e+00, 4.27e+00]    [0.00e+00, 3.82e+00, 4.27e+00]    [6.69e+00, 3.82e+00, 5.33e+00, 4.32e+00]
25000     [3.97e+00, 1.94e+00, 4.24e+00]    [0.00e+00, 3.85e+00, 4.24e+00]    [6.64e+00, 3.85e+00, 5.34e+00, 4.34e+00]
26000     [3.92e+00, 1.94e+00, 4.22e+00]    [0.00e+00, 3.88e+00, 4.22e+00]    [6.62e+00, 3.88e+00, 5.36e+00, 4.37e+00]
27000     [3.90e+00, 1.94e+00, 4.19e+00]    [0.00e+00, 3.90e+00, 4.19e+00]    [6.57e+00, 3.90e+00, 5.37e+00, 4.39e+00]
28000     [3.84e+00, 1.92e+00, 4.17e+00]    [0.00e+00, 3.93e+00, 4.17e+00]    [6.55e+00, 3.93e+00, 5.39e+00, 4.42e+00]
29000     [3.79e+00, 1.92e+00, 4.14e+00]    [0.00e+00, 3.96e+00, 4.14e+00]    [6.51e+00, 3.96e+00, 5.41e+00, 4.44e+00]
30000     [3.79e+00, 1.97e+00, 4.11e+00]    [0.00e+00, 3.98e+00, 4.11e+00]    [6.48e+00, 3.98e+00, 5.42e+00, 4.47e+00]

Best model at step 29000:
  train loss: 9.85e+00
  test loss: 8.10e+00
  test metric: [6.51e+00, 3.96e+00, 5.41e+00, 4.44e+00]

'train' took 39.602142 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 9
[111  40 106  37  42  38  81 131  20 109] ==> [  3 139 102  66  41  61  62 116  73  78  82   4  53  63  99 137 135  12
 130   9   6 105  22   0  13  17  36 125  49 112  59  26 121  47 138 141
  29  54  98  79  74  95  92  58  57  14 118 113  75  31  85  51  11  97
 129   2 115  21  86  67  34 114 142 107  64 119   8  70  93 124  43 143
  50  25  89  76   1 117 110  69 128  33  72  68  65  28   5  91  48 123
 120  87 140  45 133  55  83  44 126  23  84  94  96  46  80 127  90  52
  16 136 100 104  39 108  32  24  60  71  18 122 101  56 103  77  19  10
  15 134  88   7  30  35 132  27]
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.086719 s

'compile' took 0.383923 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [9.97e+01, 1.00e+02, 2.73e+00]    [0.00e+00, 1.00e+02, 2.73e+00]    [1.00e+02, 1.00e+02, 3.66e-02, 3.66e-02]
1000      [7.30e+01, 7.71e+01, 2.84e+00]    [0.00e+00, 8.50e+01, 2.84e+00]    [8.51e+01, 8.50e+01, 2.66e-01, 2.68e-01]
2000      [2.30e+01, 1.39e+01, 4.09e+00]    [0.00e+00, 6.95e+00, 4.09e+00]    [3.38e+00, 6.95e+00, 1.90e+00, 2.03e+00]
3000      [6.35e+00, 1.12e+01, 4.40e+00]    [0.00e+00, 2.38e+01, 4.40e+00]    [2.28e+01, 2.38e+01, 2.49e+00, 2.51e+00]
4000      [5.81e+00, 7.28e+00, 4.40e+00]    [0.00e+00, 1.41e+01, 4.40e+00]    [1.75e+01, 1.41e+01, 3.38e+00, 3.28e+00]
5000      [5.86e+00, 4.49e+00, 4.42e+00]    [0.00e+00, 7.39e+00, 4.42e+00]    [1.36e+01, 7.39e+00, 4.06e+00, 3.68e+00]
6000      [5.49e+00, 3.60e+00, 4.41e+00]    [0.00e+00, 5.20e+00, 4.41e+00]    [1.16e+01, 5.20e+00, 4.58e+00, 4.11e+00]
7000      [5.30e+00, 2.99e+00, 4.40e+00]    [0.00e+00, 4.26e+00, 4.40e+00]    [1.05e+01, 4.26e+00, 5.01e+00, 4.27e+00]
8000      [4.98e+00, 2.92e+00, 4.38e+00]    [0.00e+00, 4.62e+00, 4.38e+00]    [1.07e+01, 4.62e+00, 5.42e+00, 4.65e+00]
9000      [4.68e+00, 2.83e+00, 4.35e+00]    [0.00e+00, 4.91e+00, 4.35e+00]    [1.07e+01, 4.91e+00, 5.64e+00, 4.88e+00]
10000     [4.55e+00, 2.78e+00, 4.31e+00]    [0.00e+00, 5.05e+00, 4.31e+00]    [1.06e+01, 5.05e+00, 5.77e+00, 5.01e+00]
11000     [4.47e+00, 2.75e+00, 4.28e+00]    [0.00e+00, 5.10e+00, 4.28e+00]    [1.05e+01, 5.10e+00, 5.83e+00, 5.07e+00]
12000     [4.39e+00, 2.76e+00, 4.24e+00]    [0.00e+00, 5.13e+00, 4.24e+00]    [1.05e+01, 5.13e+00, 5.89e+00, 5.11e+00]
13000     [4.32e+00, 2.71e+00, 4.21e+00]    [0.00e+00, 5.16e+00, 4.21e+00]    [1.04e+01, 5.16e+00, 5.95e+00, 5.15e+00]
14000     [4.26e+00, 2.71e+00, 4.18e+00]    [0.00e+00, 5.19e+00, 4.18e+00]    [1.04e+01, 5.19e+00, 6.00e+00, 5.19e+00]
15000     [4.22e+00, 2.72e+00, 4.16e+00]    [0.00e+00, 5.25e+00, 4.16e+00]    [1.03e+01, 5.25e+00, 6.04e+00, 5.24e+00]
16000     [4.11e+00, 2.75e+00, 4.13e+00]    [0.00e+00, 5.32e+00, 4.13e+00]    [1.04e+01, 5.32e+00, 6.11e+00, 5.31e+00]
17000     [4.02e+00, 2.70e+00, 4.11e+00]    [0.00e+00, 5.37e+00, 4.11e+00]    [1.03e+01, 5.37e+00, 6.16e+00, 5.36e+00]
18000     [4.05e+00, 2.75e+00, 4.08e+00]    [0.00e+00, 5.40e+00, 4.08e+00]    [1.03e+01, 5.40e+00, 6.20e+00, 5.39e+00]
19000     [3.91e+00, 2.68e+00, 4.06e+00]    [0.00e+00, 5.46e+00, 4.06e+00]    [1.03e+01, 5.46e+00, 6.25e+00, 5.44e+00]
20000     [3.87e+00, 2.70e+00, 4.04e+00]    [0.00e+00, 5.53e+00, 4.04e+00]    [1.03e+01, 5.53e+00, 6.29e+00, 5.49e+00]
21000     [3.79e+00, 2.69e+00, 4.02e+00]    [0.00e+00, 5.57e+00, 4.02e+00]    [1.03e+01, 5.57e+00, 6.34e+00, 5.53e+00]
22000     [3.73e+00, 2.70e+00, 4.00e+00]    [0.00e+00, 5.61e+00, 4.00e+00]    [1.02e+01, 5.61e+00, 6.38e+00, 5.57e+00]
23000     [3.72e+00, 2.65e+00, 3.98e+00]    [0.00e+00, 5.48e+00, 3.98e+00]    [9.90e+00, 5.48e+00, 6.41e+00, 5.55e+00]
24000     [3.73e+00, 2.59e+00, 3.96e+00]    [0.00e+00, 5.35e+00, 3.96e+00]    [9.55e+00, 5.35e+00, 6.44e+00, 5.50e+00]
25000     [3.68e+00, 2.54e+00, 3.94e+00]    [0.00e+00, 5.26e+00, 3.94e+00]    [9.24e+00, 5.26e+00, 6.49e+00, 5.48e+00]
26000     [3.66e+00, 2.48e+00, 3.93e+00]    [0.00e+00, 5.19e+00, 3.93e+00]    [8.94e+00, 5.19e+00, 6.55e+00, 5.46e+00]
27000     [3.70e+00, 2.48e+00, 3.91e+00]    [0.00e+00, 5.17e+00, 3.91e+00]    [8.78e+00, 5.17e+00, 6.58e+00, 5.47e+00]
28000     [3.61e+00, 2.43e+00, 3.90e+00]    [0.00e+00, 5.20e+00, 3.90e+00]    [8.76e+00, 5.20e+00, 6.61e+00, 5.50e+00]
29000     [3.58e+00, 2.39e+00, 3.88e+00]    [0.00e+00, 5.23e+00, 3.88e+00]    [8.69e+00, 5.23e+00, 6.63e+00, 5.53e+00]
30000     [3.57e+00, 2.44e+00, 3.86e+00]    [0.00e+00, 5.29e+00, 3.86e+00]    [8.70e+00, 5.29e+00, 6.67e+00, 5.58e+00]

Best model at step 29000:
  train loss: 9.85e+00
  test loss: 9.10e+00
  test metric: [8.69e+00, 5.23e+00, 6.63e+00, 5.53e+00]

'train' took 39.712520 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...
Estar
[4.58752239 4.85317291] [0.73520887 0.66700031]
              Case          n     E (GPa)   sy (GPa)     C (GPa)    dP/dh (N/m)      Wp/Wt    E* (GPa)      sy/E*
count    97.000000  97.000000   97.000000  97.000000   97.000000      97.000000  97.000000   97.000000  97.000000
mean    279.030928   0.213917  107.163804   1.327507   92.953350  128652.067967   0.731227  102.813375   0.013835
std     411.446469   0.178797   67.175628   1.126477   66.232911   85768.846959   0.134844   60.541899   0.009753
min       1.000000   0.000000   10.000000   0.030000    2.731300    9801.900000   0.451835   10.880844   0.001399
25%      37.000000   0.100000   50.000000   0.300000   39.269000   53194.000000   0.628612   52.343315   0.005508
50%      67.000000   0.177243  100.806000   1.000000   80.813000  123181.000000   0.740598  100.685905   0.011463
75%      91.000000   0.300000  170.000000   2.000000  144.840000  194391.000000   0.830543  159.806250   0.019105
max    1023.000000   0.500000  210.000000   5.298021  261.330000  336838.000000   0.971835  190.913667   0.038209
              Case          n     E (GPa)   sy (GPa)     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  14.000000   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499   1.436338   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848   1.652156   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000   0.036792    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500   0.303588   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000   0.985823   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000   1.719617   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000   5.298021  239.235773  326727.270700   0.971982
             Case     C (GPa)    dP/dh (N/m)       Wp/Wt     hm (um)        Pm (N)  E* (GPa)      sy (GPa)  s0.008 (GPa)  s0.015 (GPa)  s0.033 (GPa)
count  144.000000  144.000000     144.000000  144.000000  144.000000  1.440000e+02    144.00  1.440000e+02  1.440000e+02      144.0000    144.000000
mean    71.500000  137.712090  202749.537112    0.725860    0.256082  9.000019e-03    109.63  1.121000e+00  1.165600e+00        1.2001      1.234167
std     41.713307    9.399233    4178.310300    0.009606    0.008684  2.479224e-08      0.00  2.228196e-16  2.228196e-16        0.0000      0.000000
min      0.000000  101.595236  192856.539700    0.696906    0.223895  8.999961e-03    109.63  1.121000e+00  1.165600e+00        1.2001      1.234167
25%     35.750000  132.175084  199461.369750    0.718985    0.251139  9.000003e-03    109.63  1.121000e+00  1.165600e+00        1.2001      1.234167
50%     71.500000  136.682184  202962.030950    0.725867    0.256606  9.000022e-03    109.63  1.121000e+00  1.165600e+00        1.2001      1.234167
75%    107.250000  142.696850  205897.992300    0.732360    0.260943  9.000038e-03    109.63  1.121000e+00  1.165600e+00        1.2001      1.234167
max    143.000000  179.537847  212396.807300    0.754513    0.297635  9.000075e-03    109.63  1.121000e+00  1.165600e+00        1.2001      1.234167
             Case     C (GPa)    dP/dh (N/m)       Wp/Wt     hm (um)        Pm (N)  E* (GPa)  sy (GPa)  s0.008 (GPa)  s0.015 (GPa)  s0.033 (GPa)
count  144.000000  144.000000     144.000000  144.000000  144.000000  1.440000e+02    144.00   144.000    144.000000      144.0000       144.000
mean    71.500000  137.415980  195873.789365    0.718020    0.256371  9.000020e-03    109.63     1.168      1.232767        1.2634         1.285
std     41.713307    9.039179    3895.400652    0.009209    0.009170  2.199332e-08      0.00     0.000      0.000000        0.0000         0.000
min      0.000000   99.250944  187462.619200    0.697845    0.241892  8.999963e-03    109.63     1.168      1.232767        1.2634         1.285
25%     35.750000  134.713027  193513.104325    0.712033    0.250375  9.000005e-03    109.63     1.168      1.232767        1.2634         1.285
50%     71.500000  137.683012  195333.420050    0.717445    0.255671  9.000021e-03    109.63     1.168      1.232767        1.2634         1.285
75%    107.250000  143.568658  198029.934450    0.723614    0.258474  9.000034e-03    109.63     1.168      1.232767        1.2634         1.285
max    143.000000  153.816655  207237.679100    0.744966    0.301131  9.000066e-03    109.63     1.168      1.232767        1.2634         1.285

Iteration: 0
[ 88  70  87  36  21   9 103  67 117  47]
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.078181 s

'compile' took 0.383287 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.00e+02, 9.98e+01, 2.74e+00]    [0.00e+00, 9.99e+01, 2.74e+00]    [9.99e+01, 9.99e+01, 9.46e-02, 9.46e-02]
1000      [7.13e+01, 7.48e+01, 2.96e+00]    [0.00e+00, 8.33e+01, 2.96e+00]    [8.34e+01, 8.33e+01, 4.42e-01, 4.45e-01]
2000      [1.95e+01, 1.32e+01, 4.40e+00]    [0.00e+00, 6.84e+00, 4.40e+00]    [3.75e+00, 6.84e+00, 1.73e+00, 1.97e+00]
3000      [6.91e+00, 1.09e+01, 4.63e+00]    [0.00e+00, 1.94e+01, 4.63e+00]    [1.81e+01, 1.94e+01, 3.03e+00, 3.07e+00]
4000      [7.87e+00, 6.38e+00, 4.58e+00]    [0.00e+00, 8.95e+00, 4.58e+00]    [8.94e+00, 8.95e+00, 2.76e+00, 2.76e+00]
5000      [8.57e+00, 3.13e+00, 4.57e+00]    [0.00e+00, 2.71e+00, 4.57e+00]    [3.13e+00, 2.71e+00, 2.78e+00, 2.74e+00]
6000      [7.32e+00, 2.95e+00, 4.58e+00]    [0.00e+00, 3.35e+00, 4.58e+00]    [4.83e+00, 3.35e+00, 3.45e+00, 3.35e+00]
7000      [6.13e+00, 2.81e+00, 4.61e+00]    [0.00e+00, 3.88e+00, 4.61e+00]    [6.56e+00, 3.88e+00, 4.07e+00, 3.88e+00]
8000      [5.38e+00, 2.63e+00, 4.62e+00]    [0.00e+00, 4.31e+00, 4.62e+00]    [7.56e+00, 4.31e+00, 4.57e+00, 4.33e+00]
9000      [4.99e+00, 2.50e+00, 4.60e+00]    [0.00e+00, 4.53e+00, 4.60e+00]    [7.88e+00, 4.53e+00, 4.81e+00, 4.57e+00]
10000     [4.82e+00, 2.41e+00, 4.57e+00]    [0.00e+00, 4.67e+00, 4.57e+00]    [7.94e+00, 4.67e+00, 4.96e+00, 4.73e+00]
11000     [4.68e+00, 2.33e+00, 4.53e+00]    [0.00e+00, 4.81e+00, 4.53e+00]    [7.88e+00, 4.81e+00, 5.06e+00, 4.86e+00]
12000     [4.57e+00, 2.26e+00, 4.49e+00]    [0.00e+00, 4.91e+00, 4.49e+00]    [7.82e+00, 4.91e+00, 5.15e+00, 4.99e+00]
13000     [4.51e+00, 2.18e+00, 4.45e+00]    [0.00e+00, 5.02e+00, 4.45e+00]    [7.67e+00, 5.02e+00, 5.27e+00, 5.14e+00]
14000     [4.41e+00, 2.13e+00, 4.41e+00]    [0.00e+00, 5.13e+00, 4.41e+00]    [7.64e+00, 5.13e+00, 5.37e+00, 5.26e+00]
15000     [4.33e+00, 2.06e+00, 4.38e+00]    [0.00e+00, 5.22e+00, 4.38e+00]    [7.58e+00, 5.22e+00, 5.45e+00, 5.35e+00]
16000     [4.25e+00, 2.05e+00, 4.34e+00]    [0.00e+00, 5.31e+00, 4.34e+00]    [7.53e+00, 5.31e+00, 5.54e+00, 5.45e+00]
17000     [4.16e+00, 1.99e+00, 4.31e+00]    [0.00e+00, 5.39e+00, 4.31e+00]    [7.48e+00, 5.39e+00, 5.62e+00, 5.54e+00]
18000     [4.08e+00, 1.98e+00, 4.28e+00]    [0.00e+00, 5.48e+00, 4.28e+00]    [7.42e+00, 5.48e+00, 5.72e+00, 5.65e+00]
19000     [3.99e+00, 1.96e+00, 4.26e+00]    [0.00e+00, 5.59e+00, 4.26e+00]    [7.38e+00, 5.59e+00, 5.82e+00, 5.76e+00]
20000     [3.93e+00, 1.93e+00, 4.23e+00]    [0.00e+00, 5.67e+00, 4.23e+00]    [7.41e+00, 5.67e+00, 5.91e+00, 5.85e+00]
21000     [3.85e+00, 1.90e+00, 4.21e+00]    [0.00e+00, 5.74e+00, 4.21e+00]    [7.44e+00, 5.74e+00, 5.99e+00, 5.93e+00]
22000     [3.80e+00, 1.93e+00, 4.19e+00]    [0.00e+00, 5.83e+00, 4.19e+00]    [7.54e+00, 5.83e+00, 6.08e+00, 6.02e+00]
23000     [3.68e+00, 1.86e+00, 4.17e+00]    [0.00e+00, 5.91e+00, 4.17e+00]    [7.56e+00, 5.91e+00, 6.17e+00, 6.12e+00]
24000     [3.61e+00, 1.83e+00, 4.15e+00]    [0.00e+00, 6.00e+00, 4.15e+00]    [7.59e+00, 6.00e+00, 6.26e+00, 6.21e+00]
25000     [3.62e+00, 1.89e+00, 4.13e+00]    [0.00e+00, 6.08e+00, 4.13e+00]    [7.62e+00, 6.08e+00, 6.36e+00, 6.31e+00]
26000     [3.57e+00, 1.88e+00, 4.12e+00]    [0.00e+00, 6.16e+00, 4.12e+00]    [7.63e+00, 6.16e+00, 6.45e+00, 6.41e+00]
27000     [3.42e+00, 1.79e+00, 4.09e+00]    [0.00e+00, 6.23e+00, 4.09e+00]    [7.64e+00, 6.23e+00, 6.52e+00, 6.47e+00]
28000     [3.39e+00, 1.80e+00, 4.07e+00]    [0.00e+00, 6.26e+00, 4.07e+00]    [7.63e+00, 6.26e+00, 6.56e+00, 6.52e+00]
29000     [3.38e+00, 1.80e+00, 4.04e+00]    [0.00e+00, 6.29e+00, 4.04e+00]    [7.59e+00, 6.29e+00, 6.59e+00, 6.55e+00]
30000     [3.31e+00, 1.76e+00, 4.02e+00]    [0.00e+00, 6.30e+00, 4.02e+00]    [7.55e+00, 6.30e+00, 6.61e+00, 6.58e+00]

Best model at step 30000:
  train loss: 9.09e+00
  test loss: 1.03e+01
  test metric: [7.55e+00, 6.30e+00, 6.61e+00, 6.58e+00]

'train' took 38.202605 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 1
[ 40 112  13 107  94   3   2 125  24  30]
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.080464 s

'compile' took 0.394393 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.01e+02, 9.99e+01, 2.77e+00]    [0.00e+00, 9.91e+01, 2.77e+00]    [9.91e+01, 9.91e+01, 9.25e-02, 9.25e-02]
1000      [6.99e+01, 7.45e+01, 2.99e+00]    [0.00e+00, 8.24e+01, 2.99e+00]    [8.27e+01, 8.24e+01, 1.67e+00, 1.69e+00]
2000      [2.12e+01, 1.22e+01, 4.24e+00]    [0.00e+00, 6.95e+00, 4.24e+00]    [3.61e+00, 6.95e+00, 4.09e+00, 3.69e+00]
3000      [7.94e+00, 9.91e+00, 4.46e+00]    [0.00e+00, 1.63e+01, 4.46e+00]    [1.40e+01, 1.63e+01, 3.69e+00, 3.92e+00]
4000      [6.73e+00, 7.03e+00, 4.46e+00]    [0.00e+00, 1.16e+01, 4.46e+00]    [1.11e+01, 1.16e+01, 3.34e+00, 3.41e+00]
5000      [7.17e+00, 4.66e+00, 4.42e+00]    [0.00e+00, 5.94e+00, 4.42e+00]    [6.42e+00, 5.94e+00, 3.09e+00, 3.09e+00]
6000      [6.05e+00, 4.51e+00, 4.39e+00]    [0.00e+00, 6.19e+00, 4.39e+00]    [7.65e+00, 6.19e+00, 3.58e+00, 3.57e+00]
7000      [5.71e+00, 3.78e+00, 4.38e+00]    [0.00e+00, 4.76e+00, 4.38e+00]    [6.95e+00, 4.76e+00, 3.95e+00, 4.00e+00]
8000      [5.04e+00, 3.69e+00, 4.38e+00]    [0.00e+00, 4.90e+00, 4.38e+00]    [8.07e+00, 4.90e+00, 4.25e+00, 4.28e+00]
9000      [4.73e+00, 3.54e+00, 4.36e+00]    [0.00e+00, 4.72e+00, 4.36e+00]    [8.11e+00, 4.72e+00, 4.43e+00, 4.47e+00]
10000     [4.46e+00, 3.45e+00, 4.33e+00]    [0.00e+00, 4.89e+00, 4.33e+00]    [8.03e+00, 4.89e+00, 4.60e+00, 4.63e+00]
11000     [4.30e+00, 3.45e+00, 4.30e+00]    [0.00e+00, 5.04e+00, 4.30e+00]    [7.87e+00, 5.04e+00, 4.74e+00, 4.76e+00]
12000     [4.13e+00, 3.38e+00, 4.27e+00]    [0.00e+00, 5.15e+00, 4.27e+00]    [7.80e+00, 5.15e+00, 4.85e+00, 4.85e+00]
13000     [4.00e+00, 3.27e+00, 4.25e+00]    [0.00e+00, 5.21e+00, 4.25e+00]    [7.69e+00, 5.21e+00, 4.94e+00, 4.92e+00]
14000     [3.97e+00, 3.21e+00, 4.22e+00]    [0.00e+00, 5.20e+00, 4.22e+00]    [7.52e+00, 5.20e+00, 4.99e+00, 4.95e+00]
15000     [3.89e+00, 3.16e+00, 4.19e+00]    [0.00e+00, 5.15e+00, 4.19e+00]    [7.41e+00, 5.15e+00, 5.00e+00, 4.95e+00]
16000     [3.86e+00, 3.09e+00, 4.17e+00]    [0.00e+00, 5.05e+00, 4.17e+00]    [7.25e+00, 5.05e+00, 4.99e+00, 4.92e+00]
17000     [3.81e+00, 2.98e+00, 4.15e+00]    [0.00e+00, 4.98e+00, 4.15e+00]    [7.10e+00, 4.98e+00, 4.99e+00, 4.91e+00]
18000     [3.78e+00, 2.93e+00, 4.13e+00]    [0.00e+00, 4.88e+00, 4.13e+00]    [6.90e+00, 4.88e+00, 4.98e+00, 4.89e+00]
19000     [3.75e+00, 2.81e+00, 4.11e+00]    [0.00e+00, 4.78e+00, 4.11e+00]    [6.59e+00, 4.78e+00, 4.99e+00, 4.90e+00]
20000     [3.74e+00, 2.72e+00, 4.09e+00]    [0.00e+00, 4.72e+00, 4.09e+00]    [6.27e+00, 4.72e+00, 5.00e+00, 4.92e+00]
21000     [3.74e+00, 2.63e+00, 4.07e+00]    [0.00e+00, 4.73e+00, 4.07e+00]    [5.95e+00, 4.73e+00, 5.01e+00, 4.91e+00]
22000     [3.76e+00, 2.56e+00, 4.05e+00]    [0.00e+00, 4.76e+00, 4.05e+00]    [5.71e+00, 4.76e+00, 5.04e+00, 4.95e+00]
23000     [3.74e+00, 2.48e+00, 4.04e+00]    [0.00e+00, 4.81e+00, 4.04e+00]    [5.51e+00, 4.81e+00, 5.07e+00, 5.00e+00]
24000     [3.76e+00, 2.42e+00, 4.02e+00]    [0.00e+00, 4.87e+00, 4.02e+00]    [5.35e+00, 4.87e+00, 5.10e+00, 5.07e+00]
25000     [3.78e+00, 2.40e+00, 4.00e+00]    [0.00e+00, 4.89e+00, 4.00e+00]    [5.30e+00, 4.89e+00, 5.10e+00, 5.09e+00]
26000     [3.75e+00, 2.37e+00, 3.97e+00]    [0.00e+00, 4.89e+00, 3.97e+00]    [5.29e+00, 4.89e+00, 5.09e+00, 5.08e+00]
27000     [3.72e+00, 2.34e+00, 3.95e+00]    [0.00e+00, 4.88e+00, 3.95e+00]    [5.26e+00, 4.88e+00, 5.08e+00, 5.07e+00]
28000     [3.72e+00, 2.35e+00, 3.93e+00]    [0.00e+00, 4.88e+00, 3.93e+00]    [5.26e+00, 4.88e+00, 5.07e+00, 5.07e+00]
29000     [3.68e+00, 2.30e+00, 3.91e+00]    [0.00e+00, 4.88e+00, 3.91e+00]    [5.25e+00, 4.88e+00, 5.07e+00, 5.07e+00]
30000     [3.67e+00, 2.30e+00, 3.90e+00]    [0.00e+00, 4.88e+00, 3.90e+00]    [5.25e+00, 4.88e+00, 5.06e+00, 5.07e+00]

Best model at step 30000:
  train loss: 9.87e+00
  test loss: 8.78e+00
  test metric: [5.25e+00, 4.88e+00, 5.06e+00, 5.07e+00]

'train' took 39.315037 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 2
[105  28  73  89  93  41  83  43  92  18]
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.081830 s

'compile' took 0.388543 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [9.95e+01, 9.94e+01, 2.68e+00]    [0.00e+00, 1.00e+02, 2.68e+00]    [1.00e+02, 1.00e+02, 1.63e-01, 1.63e-01]
1000      [6.93e+01, 7.13e+01, 2.99e+00]    [0.00e+00, 7.75e+01, 2.99e+00]    [7.78e+01, 7.75e+01, 4.99e-01, 5.08e-01]
2000      [1.74e+01, 1.24e+01, 4.43e+00]    [0.00e+00, 9.24e+00, 4.43e+00]    [5.28e+00, 9.24e+00, 1.80e+00, 2.26e+00]
3000      [8.17e+00, 8.61e+00, 4.60e+00]    [0.00e+00, 1.38e+01, 4.60e+00]    [1.46e+01, 1.38e+01, 2.58e+00, 2.56e+00]
4000      [7.74e+00, 5.06e+00, 4.60e+00]    [0.00e+00, 4.62e+00, 4.60e+00]    [1.07e+01, 4.62e+00, 2.96e+00, 2.12e+00]
5000      [7.19e+00, 3.43e+00, 4.62e+00]    [0.00e+00, 2.46e+00, 4.62e+00]    [9.85e+00, 2.46e+00, 3.20e+00, 2.38e+00]
6000      [6.44e+00, 3.44e+00, 4.58e+00]    [0.00e+00, 2.89e+00, 4.58e+00]    [9.78e+00, 2.89e+00, 3.49e+00, 2.76e+00]
7000      [6.01e+00, 3.38e+00, 4.53e+00]    [0.00e+00, 3.26e+00, 4.53e+00]    [9.19e+00, 3.26e+00, 3.58e+00, 3.08e+00]
8000      [5.84e+00, 3.08e+00, 4.50e+00]    [0.00e+00, 3.35e+00, 4.50e+00]    [8.16e+00, 3.35e+00, 3.62e+00, 3.30e+00]
9000      [5.72e+00, 2.66e+00, 4.49e+00]    [0.00e+00, 3.48e+00, 4.49e+00]    [7.28e+00, 3.48e+00, 3.73e+00, 3.53e+00]
10000     [5.52e+00, 2.47e+00, 4.48e+00]    [0.00e+00, 3.73e+00, 4.48e+00]    [7.13e+00, 3.73e+00, 3.95e+00, 3.78e+00]
11000     [5.31e+00, 2.22e+00, 4.47e+00]    [0.00e+00, 3.95e+00, 4.47e+00]    [6.76e+00, 3.95e+00, 4.17e+00, 4.02e+00]
12000     [5.18e+00, 2.04e+00, 4.45e+00]    [0.00e+00, 4.10e+00, 4.45e+00]    [6.24e+00, 4.10e+00, 4.29e+00, 4.16e+00]
13000     [5.06e+00, 1.87e+00, 4.42e+00]    [0.00e+00, 4.22e+00, 4.42e+00]    [5.89e+00, 4.22e+00, 4.32e+00, 4.25e+00]
14000     [4.98e+00, 1.82e+00, 4.38e+00]    [0.00e+00, 4.33e+00, 4.38e+00]    [5.77e+00, 4.33e+00, 4.41e+00, 4.34e+00]
15000     [4.86e+00, 1.73e+00, 4.35e+00]    [0.00e+00, 4.47e+00, 4.35e+00]    [5.62e+00, 4.47e+00, 4.52e+00, 4.45e+00]
16000     [4.72e+00, 1.72e+00, 4.33e+00]    [0.00e+00, 4.61e+00, 4.33e+00]    [5.65e+00, 4.61e+00, 4.65e+00, 4.57e+00]
17000     [4.57e+00, 1.68e+00, 4.31e+00]    [0.00e+00, 4.77e+00, 4.31e+00]    [5.71e+00, 4.77e+00, 4.80e+00, 4.71e+00]
18000     [4.43e+00, 1.66e+00, 4.29e+00]    [0.00e+00, 4.92e+00, 4.29e+00]    [5.78e+00, 4.92e+00, 4.93e+00, 4.83e+00]
19000     [4.29e+00, 1.64e+00, 4.28e+00]    [0.00e+00, 5.07e+00, 4.28e+00]    [5.87e+00, 5.07e+00, 5.05e+00, 4.96e+00]
20000     [4.19e+00, 1.61e+00, 4.26e+00]    [0.00e+00, 5.18e+00, 4.26e+00]    [5.90e+00, 5.18e+00, 5.11e+00, 5.03e+00]
21000     [4.07e+00, 1.58e+00, 4.24e+00]    [0.00e+00, 5.28e+00, 4.24e+00]    [5.95e+00, 5.28e+00, 5.14e+00, 5.07e+00]
22000     [3.98e+00, 1.58e+00, 4.23e+00]    [0.00e+00, 5.38e+00, 4.23e+00]    [6.00e+00, 5.38e+00, 5.17e+00, 5.10e+00]
23000     [3.85e+00, 1.55e+00, 4.21e+00]    [0.00e+00, 5.47e+00, 4.21e+00]    [6.05e+00, 5.47e+00, 5.18e+00, 5.11e+00]
24000     [3.81e+00, 1.54e+00, 4.18e+00]    [0.00e+00, 5.51e+00, 4.18e+00]    [6.01e+00, 5.51e+00, 5.14e+00, 5.05e+00]
25000     [3.70e+00, 1.48e+00, 4.16e+00]    [0.00e+00, 5.48e+00, 4.16e+00]    [5.95e+00, 5.48e+00, 5.10e+00, 4.97e+00]
26000     [3.62e+00, 1.45e+00, 4.14e+00]    [0.00e+00, 5.38e+00, 4.14e+00]    [5.86e+00, 5.38e+00, 5.05e+00, 4.88e+00]
27000     [3.54e+00, 1.39e+00, 4.13e+00]    [0.00e+00, 5.27e+00, 4.13e+00]    [5.79e+00, 5.27e+00, 4.98e+00, 4.78e+00]
28000     [3.46e+00, 1.33e+00, 4.12e+00]    [0.00e+00, 5.20e+00, 4.12e+00]    [5.75e+00, 5.20e+00, 4.93e+00, 4.71e+00]
29000     [3.43e+00, 1.30e+00, 4.10e+00]    [0.00e+00, 5.11e+00, 4.10e+00]    [5.71e+00, 5.11e+00, 4.89e+00, 4.61e+00]
30000     [3.35e+00, 1.22e+00, 4.09e+00]    [0.00e+00, 5.03e+00, 4.09e+00]    [5.68e+00, 5.03e+00, 4.87e+00, 4.55e+00]

Best model at step 30000:
  train loss: 8.67e+00
  test loss: 9.12e+00
  test metric: [5.68e+00, 5.03e+00, 4.87e+00, 4.55e+00]

'train' took 39.075474 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 3
[ 83 111 140   9  63  43  91 104 138 112]
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.079097 s

'compile' took 0.381452 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.00e+02, 1.00e+02, 2.70e+00]    [0.00e+00, 1.00e+02, 2.70e+00]    [1.00e+02, 1.00e+02, 3.40e-02, 3.40e-02]
1000      [7.25e+01, 7.56e+01, 2.93e+00]    [0.00e+00, 8.32e+01, 2.93e+00]    [8.34e+01, 8.32e+01, 6.01e-01, 6.05e-01]
2000      [2.06e+01, 1.37e+01, 4.27e+00]    [0.00e+00, 8.61e+00, 4.27e+00]    [4.99e+00, 8.61e+00, 2.30e+00, 2.71e+00]
3000      [6.55e+00, 1.07e+01, 4.53e+00]    [0.00e+00, 1.99e+01, 4.53e+00]    [1.89e+01, 1.99e+01, 3.27e+00, 3.30e+00]
4000      [5.90e+00, 7.85e+00, 4.53e+00]    [0.00e+00, 1.38e+01, 4.53e+00]    [1.55e+01, 1.38e+01, 3.74e+00, 3.56e+00]
5000      [6.26e+00, 4.58e+00, 4.55e+00]    [0.00e+00, 5.34e+00, 4.55e+00]    [1.11e+01, 5.34e+00, 3.66e+00, 2.88e+00]
6000      [5.80e+00, 3.72e+00, 4.56e+00]    [0.00e+00, 4.42e+00, 4.56e+00]    [1.10e+01, 4.42e+00, 4.03e+00, 3.31e+00]
7000      [5.25e+00, 3.54e+00, 4.53e+00]    [0.00e+00, 5.09e+00, 4.53e+00]    [1.11e+01, 5.09e+00, 4.31e+00, 3.73e+00]
8000      [5.00e+00, 3.26e+00, 4.50e+00]    [0.00e+00, 4.94e+00, 4.50e+00]    [1.03e+01, 4.94e+00, 4.44e+00, 4.02e+00]
9000      [4.75e+00, 3.18e+00, 4.46e+00]    [0.00e+00, 5.10e+00, 4.46e+00]    [1.01e+01, 5.10e+00, 4.53e+00, 4.16e+00]
10000     [4.68e+00, 3.11e+00, 4.42e+00]    [0.00e+00, 5.09e+00, 4.42e+00]    [1.00e+01, 5.09e+00, 4.57e+00, 4.23e+00]
11000     [4.54e+00, 3.04e+00, 4.38e+00]    [0.00e+00, 5.11e+00, 4.38e+00]    [9.92e+00, 5.11e+00, 4.62e+00, 4.30e+00]
12000     [4.47e+00, 2.98e+00, 4.35e+00]    [0.00e+00, 5.15e+00, 4.35e+00]    [9.87e+00, 5.15e+00, 4.67e+00, 4.37e+00]
13000     [4.40e+00, 2.92e+00, 4.31e+00]    [0.00e+00, 5.14e+00, 4.31e+00]    [9.71e+00, 5.14e+00, 4.68e+00, 4.40e+00]
14000     [4.36e+00, 2.88e+00, 4.27e+00]    [0.00e+00, 5.14e+00, 4.27e+00]    [9.63e+00, 5.14e+00, 4.69e+00, 4.42e+00]
15000     [4.35e+00, 2.86e+00, 4.24e+00]    [0.00e+00, 5.10e+00, 4.24e+00]    [9.49e+00, 5.10e+00, 4.70e+00, 4.43e+00]
16000     [4.36e+00, 2.88e+00, 4.20e+00]    [0.00e+00, 5.02e+00, 4.20e+00]    [9.29e+00, 5.02e+00, 4.71e+00, 4.44e+00]
17000     [4.28e+00, 2.82e+00, 4.17e+00]    [0.00e+00, 5.00e+00, 4.17e+00]    [9.18e+00, 5.00e+00, 4.74e+00, 4.46e+00]
18000     [4.24e+00, 2.76e+00, 4.15e+00]    [0.00e+00, 4.97e+00, 4.15e+00]    [9.03e+00, 4.97e+00, 4.77e+00, 4.49e+00]
19000     [4.23e+00, 2.71e+00, 4.12e+00]    [0.00e+00, 4.90e+00, 4.12e+00]    [8.81e+00, 4.90e+00, 4.79e+00, 4.51e+00]
20000     [4.18e+00, 2.66e+00, 4.09e+00]    [0.00e+00, 4.86e+00, 4.09e+00]    [8.62e+00, 4.86e+00, 4.83e+00, 4.54e+00]
21000     [4.17e+00, 2.61e+00, 4.07e+00]    [0.00e+00, 4.81e+00, 4.07e+00]    [8.41e+00, 4.81e+00, 4.86e+00, 4.57e+00]
22000     [4.16e+00, 2.56e+00, 4.05e+00]    [0.00e+00, 4.75e+00, 4.05e+00]    [8.12e+00, 4.75e+00, 4.90e+00, 4.59e+00]
23000     [4.18e+00, 2.52e+00, 4.03e+00]    [0.00e+00, 4.72e+00, 4.03e+00]    [7.84e+00, 4.72e+00, 4.94e+00, 4.61e+00]
24000     [4.13e+00, 2.47e+00, 4.01e+00]    [0.00e+00, 4.72e+00, 4.01e+00]    [7.68e+00, 4.72e+00, 4.98e+00, 4.63e+00]
25000     [4.09e+00, 2.40e+00, 3.99e+00]    [0.00e+00, 4.73e+00, 3.99e+00]    [7.51e+00, 4.73e+00, 4.99e+00, 4.65e+00]
26000     [4.07e+00, 2.34e+00, 3.97e+00]    [0.00e+00, 4.74e+00, 3.97e+00]    [7.34e+00, 4.74e+00, 5.03e+00, 4.69e+00]
27000     [4.06e+00, 2.31e+00, 3.96e+00]    [0.00e+00, 4.75e+00, 3.96e+00]    [7.19e+00, 4.75e+00, 5.05e+00, 4.71e+00]
28000     [4.02e+00, 2.32e+00, 3.93e+00]    [0.00e+00, 4.79e+00, 3.93e+00]    [7.21e+00, 4.79e+00, 5.09e+00, 4.74e+00]
29000     [3.99e+00, 2.30e+00, 3.91e+00]    [0.00e+00, 4.82e+00, 3.91e+00]    [7.23e+00, 4.82e+00, 5.12e+00, 4.77e+00]
30000     [3.90e+00, 2.30e+00, 3.89e+00]    [0.00e+00, 4.86e+00, 3.89e+00]    [7.24e+00, 4.86e+00, 5.16e+00, 4.81e+00]

Best model at step 30000:
  train loss: 1.01e+01
  test loss: 8.75e+00
  test metric: [7.24e+00, 4.86e+00, 5.16e+00, 4.81e+00]

'train' took 38.999609 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 4
[ 66 138 115  28 134  70 143   3  34  88]
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.079069 s

'compile' took 0.387216 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.01e+02, 1.00e+02, 2.72e+00]    [0.00e+00, 9.94e+01, 2.72e+00]    [9.94e+01, 9.94e+01, 5.52e-02, 5.52e-02]
1000      [6.93e+01, 7.12e+01, 3.04e+00]    [0.00e+00, 7.90e+01, 3.04e+00]    [7.92e+01, 7.90e+01, 7.29e-01, 7.34e-01]
2000      [2.02e+01, 1.48e+01, 4.25e+00]    [0.00e+00, 7.66e+00, 4.25e+00]    [5.28e+00, 7.66e+00, 2.09e+00, 2.34e+00]
3000      [7.25e+00, 9.87e+00, 4.48e+00]    [0.00e+00, 1.67e+01, 4.48e+00]    [1.60e+01, 1.67e+01, 2.36e+00, 2.37e+00]
4000      [8.19e+00, 5.60e+00, 4.45e+00]    [0.00e+00, 5.50e+00, 4.45e+00]    [6.41e+00, 5.50e+00, 1.99e+00, 1.85e+00]
5000      [7.88e+00, 3.96e+00, 4.46e+00]    [0.00e+00, 1.87e+00, 4.46e+00]    [4.41e+00, 1.87e+00, 1.94e+00, 1.77e+00]
6000      [7.01e+00, 3.88e+00, 4.43e+00]    [0.00e+00, 2.06e+00, 4.43e+00]    [5.98e+00, 2.06e+00, 2.22e+00, 1.95e+00]
7000      [6.41e+00, 3.85e+00, 4.41e+00]    [0.00e+00, 2.29e+00, 4.41e+00]    [6.44e+00, 2.29e+00, 2.48e+00, 2.18e+00]
8000      [5.97e+00, 3.76e+00, 4.39e+00]    [0.00e+00, 2.51e+00, 4.39e+00]    [6.55e+00, 2.51e+00, 2.73e+00, 2.42e+00]
9000      [5.56e+00, 3.72e+00, 4.38e+00]    [0.00e+00, 2.73e+00, 4.38e+00]    [6.76e+00, 2.73e+00, 2.98e+00, 2.66e+00]
10000     [5.32e+00, 3.61e+00, 4.37e+00]    [0.00e+00, 2.92e+00, 4.37e+00]    [6.52e+00, 2.92e+00, 3.18e+00, 2.88e+00]
11000     [5.12e+00, 3.56e+00, 4.35e+00]    [0.00e+00, 3.07e+00, 4.35e+00]    [6.54e+00, 3.07e+00, 3.32e+00, 3.03e+00]
12000     [4.91e+00, 3.53e+00, 4.33e+00]    [0.00e+00, 3.19e+00, 4.33e+00]    [6.31e+00, 3.19e+00, 3.43e+00, 3.16e+00]
13000     [4.75e+00, 3.43e+00, 4.30e+00]    [0.00e+00, 3.30e+00, 4.30e+00]    [6.03e+00, 3.30e+00, 3.51e+00, 3.27e+00]
14000     [4.62e+00, 3.40e+00, 4.28e+00]    [0.00e+00, 3.43e+00, 4.28e+00]    [5.86e+00, 3.43e+00, 3.64e+00, 3.39e+00]
15000     [4.47e+00, 3.35e+00, 4.26e+00]    [0.00e+00, 3.55e+00, 4.26e+00]    [5.78e+00, 3.55e+00, 3.77e+00, 3.50e+00]
16000     [4.38e+00, 3.35e+00, 4.24e+00]    [0.00e+00, 3.65e+00, 4.24e+00]    [5.78e+00, 3.65e+00, 3.88e+00, 3.60e+00]
17000     [4.28e+00, 3.28e+00, 4.21e+00]    [0.00e+00, 3.67e+00, 4.21e+00]    [5.62e+00, 3.67e+00, 3.88e+00, 3.62e+00]
18000     [4.23e+00, 3.25e+00, 4.18e+00]    [0.00e+00, 3.68e+00, 4.18e+00]    [5.48e+00, 3.68e+00, 3.88e+00, 3.63e+00]
19000     [4.16e+00, 3.22e+00, 4.16e+00]    [0.00e+00, 3.70e+00, 4.16e+00]    [5.37e+00, 3.70e+00, 3.88e+00, 3.65e+00]
20000     [4.16e+00, 3.21e+00, 4.13e+00]    [0.00e+00, 3.70e+00, 4.13e+00]    [5.30e+00, 3.70e+00, 3.88e+00, 3.65e+00]
21000     [4.20e+00, 3.22e+00, 4.10e+00]    [0.00e+00, 3.70e+00, 4.10e+00]    [5.27e+00, 3.70e+00, 3.88e+00, 3.65e+00]
22000     [4.09e+00, 3.21e+00, 4.07e+00]    [0.00e+00, 3.70e+00, 4.07e+00]    [5.29e+00, 3.70e+00, 3.88e+00, 3.65e+00]
23000     [4.06e+00, 3.18e+00, 4.04e+00]    [0.00e+00, 3.71e+00, 4.04e+00]    [5.26e+00, 3.71e+00, 3.89e+00, 3.65e+00]
24000     [3.99e+00, 3.14e+00, 4.02e+00]    [0.00e+00, 3.71e+00, 4.02e+00]    [5.27e+00, 3.71e+00, 3.89e+00, 3.66e+00]
25000     [3.97e+00, 3.12e+00, 4.00e+00]    [0.00e+00, 3.71e+00, 4.00e+00]    [5.26e+00, 3.71e+00, 3.89e+00, 3.67e+00]
26000     [3.98e+00, 3.15e+00, 3.97e+00]    [0.00e+00, 3.71e+00, 3.97e+00]    [5.24e+00, 3.71e+00, 3.89e+00, 3.67e+00]
27000     [3.93e+00, 3.10e+00, 3.95e+00]    [0.00e+00, 3.71e+00, 3.95e+00]    [5.20e+00, 3.71e+00, 3.89e+00, 3.68e+00]
28000     [3.92e+00, 3.09e+00, 3.92e+00]    [0.00e+00, 3.71e+00, 3.92e+00]    [5.18e+00, 3.71e+00, 3.89e+00, 3.68e+00]
29000     [3.93e+00, 3.09e+00, 3.90e+00]    [0.00e+00, 3.70e+00, 3.90e+00]    [5.16e+00, 3.70e+00, 3.88e+00, 3.68e+00]
30000     [3.90e+00, 3.07e+00, 3.88e+00]    [0.00e+00, 3.70e+00, 3.88e+00]    [5.11e+00, 3.70e+00, 3.88e+00, 3.69e+00]

Best model at step 30000:
  train loss: 1.08e+01
  test loss: 7.58e+00
  test metric: [5.11e+00, 3.70e+00, 3.88e+00, 3.69e+00]

'train' took 39.448434 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 5
[134   0  54  45  28 123  39  18  72  20]
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.080124 s

'compile' took 0.393993 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [9.97e+01, 9.95e+01, 2.69e+00]    [0.00e+00, 1.00e+02, 2.69e+00]    [1.00e+02, 1.00e+02, 8.16e-02, 8.16e-02]
1000      [6.93e+01, 7.20e+01, 3.02e+00]    [0.00e+00, 8.09e+01, 3.02e+00]    [8.10e+01, 8.09e+01, 1.22e+00, 1.23e+00]
2000      [1.63e+01, 1.19e+01, 4.44e+00]    [0.00e+00, 9.30e+00, 4.44e+00]    [7.95e+00, 9.30e+00, 3.11e+00, 3.28e+00]
3000      [7.07e+00, 9.28e+00, 4.62e+00]    [0.00e+00, 1.59e+01, 4.62e+00]    [1.56e+01, 1.59e+01, 3.58e+00, 3.62e+00]
4000      [7.68e+00, 5.29e+00, 4.58e+00]    [0.00e+00, 7.32e+00, 4.58e+00]    [7.82e+00, 7.32e+00, 3.03e+00, 2.95e+00]
5000      [6.90e+00, 4.04e+00, 4.57e+00]    [0.00e+00, 4.90e+00, 4.57e+00]    [7.67e+00, 4.90e+00, 3.38e+00, 3.16e+00]
6000      [6.01e+00, 3.87e+00, 4.53e+00]    [0.00e+00, 4.63e+00, 4.53e+00]    [9.40e+00, 4.63e+00, 3.72e+00, 3.29e+00]
7000      [5.47e+00, 3.68e+00, 4.49e+00]    [0.00e+00, 4.40e+00, 4.49e+00]    [9.89e+00, 4.40e+00, 3.94e+00, 3.50e+00]
8000      [5.27e+00, 3.45e+00, 4.45e+00]    [0.00e+00, 4.14e+00, 4.45e+00]    [9.34e+00, 4.14e+00, 3.96e+00, 3.64e+00]
9000      [4.95e+00, 3.26e+00, 4.42e+00]    [0.00e+00, 4.05e+00, 4.42e+00]    [9.12e+00, 4.05e+00, 3.99e+00, 3.71e+00]
10000     [4.79e+00, 3.14e+00, 4.39e+00]    [0.00e+00, 4.07e+00, 4.39e+00]    [8.91e+00, 4.07e+00, 4.07e+00, 3.81e+00]
11000     [4.62e+00, 3.04e+00, 4.36e+00]    [0.00e+00, 4.14e+00, 4.36e+00]    [8.71e+00, 4.14e+00, 4.17e+00, 3.91e+00]
12000     [4.49e+00, 2.93e+00, 4.32e+00]    [0.00e+00, 4.21e+00, 4.32e+00]    [8.39e+00, 4.21e+00, 4.26e+00, 4.01e+00]
13000     [4.50e+00, 2.89e+00, 4.29e+00]    [0.00e+00, 4.14e+00, 4.29e+00]    [7.89e+00, 4.14e+00, 4.23e+00, 4.00e+00]
14000     [4.44e+00, 2.78e+00, 4.25e+00]    [0.00e+00, 4.08e+00, 4.25e+00]    [7.55e+00, 4.08e+00, 4.22e+00, 4.00e+00]
15000     [4.37e+00, 2.68e+00, 4.22e+00]    [0.00e+00, 4.03e+00, 4.22e+00]    [7.30e+00, 4.03e+00, 4.25e+00, 4.01e+00]
16000     [4.30e+00, 2.59e+00, 4.18e+00]    [0.00e+00, 4.01e+00, 4.18e+00]    [7.08e+00, 4.01e+00, 4.28e+00, 4.03e+00]
17000     [4.27e+00, 2.54e+00, 4.15e+00]    [0.00e+00, 4.01e+00, 4.15e+00]    [6.93e+00, 4.01e+00, 4.30e+00, 4.02e+00]
18000     [4.20e+00, 2.47e+00, 4.13e+00]    [0.00e+00, 4.03e+00, 4.13e+00]    [6.86e+00, 4.03e+00, 4.32e+00, 4.03e+00]
19000     [4.22e+00, 2.45e+00, 4.10e+00]    [0.00e+00, 4.06e+00, 4.10e+00]    [6.76e+00, 4.06e+00, 4.35e+00, 4.04e+00]
20000     [4.11e+00, 2.35e+00, 4.08e+00]    [0.00e+00, 4.10e+00, 4.08e+00]    [6.66e+00, 4.10e+00, 4.40e+00, 4.07e+00]
21000     [4.08e+00, 2.29e+00, 4.05e+00]    [0.00e+00, 4.15e+00, 4.05e+00]    [6.49e+00, 4.15e+00, 4.44e+00, 4.11e+00]
22000     [4.06e+00, 2.22e+00, 4.03e+00]    [0.00e+00, 4.19e+00, 4.03e+00]    [6.31e+00, 4.19e+00, 4.48e+00, 4.14e+00]
23000     [4.06e+00, 2.22e+00, 4.00e+00]    [0.00e+00, 4.23e+00, 4.00e+00]    [6.25e+00, 4.23e+00, 4.51e+00, 4.16e+00]
24000     [3.93e+00, 2.13e+00, 3.98e+00]    [0.00e+00, 4.25e+00, 3.98e+00]    [6.31e+00, 4.25e+00, 4.53e+00, 4.17e+00]
25000     [3.90e+00, 2.12e+00, 3.95e+00]    [0.00e+00, 4.28e+00, 3.95e+00]    [6.22e+00, 4.28e+00, 4.55e+00, 4.20e+00]
26000     [3.88e+00, 2.08e+00, 3.93e+00]    [0.00e+00, 4.32e+00, 3.93e+00]    [6.10e+00, 4.32e+00, 4.59e+00, 4.25e+00]
27000     [3.82e+00, 2.04e+00, 3.90e+00]    [0.00e+00, 4.36e+00, 3.90e+00]    [6.08e+00, 4.36e+00, 4.62e+00, 4.28e+00]
28000     [3.82e+00, 2.06e+00, 3.88e+00]    [0.00e+00, 4.39e+00, 3.88e+00]    [6.07e+00, 4.39e+00, 4.63e+00, 4.30e+00]
29000     [3.74e+00, 2.01e+00, 3.86e+00]    [0.00e+00, 4.42e+00, 3.86e+00]    [6.08e+00, 4.42e+00, 4.66e+00, 4.33e+00]
30000     [3.69e+00, 1.99e+00, 3.84e+00]    [0.00e+00, 4.45e+00, 3.84e+00]    [6.09e+00, 4.45e+00, 4.68e+00, 4.35e+00]

Best model at step 30000:
  train loss: 9.52e+00
  test loss: 8.29e+00
  test metric: [6.09e+00, 4.45e+00, 4.68e+00, 4.35e+00]

'train' took 39.848852 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 6
[135  40  93  60 123  74 106   4  19  30]
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.076835 s

'compile' took 0.380723 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [9.84e+01, 1.01e+02, 2.94e+00]    [0.00e+00, 1.00e+02, 2.94e+00]    [1.00e+02, 1.00e+02, 1.88e-01, 1.88e-01]
1000      [6.94e+01, 7.02e+01, 3.26e+00]    [0.00e+00, 8.11e+01, 3.26e+00]    [8.14e+01, 8.11e+01, 1.33e+00, 1.35e+00]
2000      [2.29e+01, 1.30e+01, 4.47e+00]    [0.00e+00, 3.17e+00, 4.47e+00]    [3.55e+00, 3.17e+00, 3.65e+00, 3.30e+00]
3000      [9.87e+00, 4.16e+00, 4.82e+00]    [0.00e+00, 3.12e+00, 4.82e+00]    [2.87e+00, 3.12e+00, 3.03e+00, 3.03e+00]
4000      [7.23e+00, 3.32e+00, 4.84e+00]    [0.00e+00, 3.82e+00, 4.84e+00]    [4.03e+00, 3.82e+00, 3.62e+00, 3.61e+00]
5000      [6.13e+00, 2.97e+00, 4.83e+00]    [0.00e+00, 3.84e+00, 4.83e+00]    [5.00e+00, 3.84e+00, 3.88e+00, 3.81e+00]
6000      [5.53e+00, 2.84e+00, 4.81e+00]    [0.00e+00, 4.00e+00, 4.81e+00]    [5.95e+00, 4.00e+00, 4.10e+00, 4.01e+00]
7000      [5.16e+00, 2.68e+00, 4.78e+00]    [0.00e+00, 4.17e+00, 4.78e+00]    [6.55e+00, 4.17e+00, 4.31e+00, 4.19e+00]
8000      [4.87e+00, 2.57e+00, 4.74e+00]    [0.00e+00, 4.33e+00, 4.74e+00]    [7.01e+00, 4.33e+00, 4.48e+00, 4.36e+00]
9000      [4.68e+00, 2.49e+00, 4.71e+00]    [0.00e+00, 4.44e+00, 4.71e+00]    [7.24e+00, 4.44e+00, 4.61e+00, 4.48e+00]
10000     [4.51e+00, 2.45e+00, 4.67e+00]    [0.00e+00, 4.51e+00, 4.67e+00]    [7.20e+00, 4.51e+00, 4.70e+00, 4.57e+00]
11000     [4.38e+00, 2.42e+00, 4.64e+00]    [0.00e+00, 4.53e+00, 4.64e+00]    [6.99e+00, 4.53e+00, 4.75e+00, 4.63e+00]
12000     [4.30e+00, 2.36e+00, 4.61e+00]    [0.00e+00, 4.51e+00, 4.61e+00]    [6.74e+00, 4.51e+00, 4.79e+00, 4.69e+00]
13000     [4.21e+00, 2.32e+00, 4.58e+00]    [0.00e+00, 4.54e+00, 4.58e+00]    [6.67e+00, 4.54e+00, 4.82e+00, 4.73e+00]
14000     [4.14e+00, 2.30e+00, 4.55e+00]    [0.00e+00, 4.56e+00, 4.55e+00]    [6.52e+00, 4.56e+00, 4.86e+00, 4.78e+00]
15000     [4.05e+00, 2.28e+00, 4.52e+00]    [0.00e+00, 4.59e+00, 4.52e+00]    [6.43e+00, 4.59e+00, 4.89e+00, 4.82e+00]
16000     [3.95e+00, 2.23e+00, 4.49e+00]    [0.00e+00, 4.63e+00, 4.49e+00]    [6.39e+00, 4.63e+00, 4.93e+00, 4.86e+00]
17000     [3.92e+00, 2.19e+00, 4.46e+00]    [0.00e+00, 4.66e+00, 4.46e+00]    [6.34e+00, 4.66e+00, 4.97e+00, 4.90e+00]
18000     [3.87e+00, 2.18e+00, 4.43e+00]    [0.00e+00, 4.69e+00, 4.43e+00]    [6.33e+00, 4.69e+00, 5.00e+00, 4.93e+00]
19000     [3.85e+00, 2.19e+00, 4.40e+00]    [0.00e+00, 4.72e+00, 4.40e+00]    [6.29e+00, 4.72e+00, 5.03e+00, 4.97e+00]
20000     [3.78e+00, 2.16e+00, 4.37e+00]    [0.00e+00, 4.74e+00, 4.37e+00]    [6.26e+00, 4.74e+00, 5.06e+00, 5.00e+00]
21000     [3.81e+00, 2.17e+00, 4.34e+00]    [0.00e+00, 4.77e+00, 4.34e+00]    [6.22e+00, 4.77e+00, 5.09e+00, 5.04e+00]
22000     [3.75e+00, 2.17e+00, 4.32e+00]    [0.00e+00, 4.80e+00, 4.32e+00]    [6.21e+00, 4.80e+00, 5.12e+00, 5.07e+00]
23000     [3.70e+00, 2.13e+00, 4.29e+00]    [0.00e+00, 4.82e+00, 4.29e+00]    [6.20e+00, 4.82e+00, 5.15e+00, 5.10e+00]
24000     [3.67e+00, 2.11e+00, 4.27e+00]    [0.00e+00, 4.84e+00, 4.27e+00]    [6.20e+00, 4.84e+00, 5.17e+00, 5.12e+00]
25000     [3.68e+00, 2.13e+00, 4.25e+00]    [0.00e+00, 4.86e+00, 4.25e+00]    [6.19e+00, 4.86e+00, 5.20e+00, 5.15e+00]
26000     [3.65e+00, 2.09e+00, 4.23e+00]    [0.00e+00, 4.88e+00, 4.23e+00]    [6.19e+00, 4.88e+00, 5.22e+00, 5.18e+00]
27000     [3.67e+00, 2.16e+00, 4.21e+00]    [0.00e+00, 4.90e+00, 4.21e+00]    [6.20e+00, 4.90e+00, 5.24e+00, 5.20e+00]
28000     [3.60e+00, 2.10e+00, 4.19e+00]    [0.00e+00, 4.92e+00, 4.19e+00]    [6.19e+00, 4.92e+00, 5.26e+00, 5.23e+00]
29000     [3.54e+00, 2.06e+00, 4.17e+00]    [0.00e+00, 4.94e+00, 4.17e+00]    [6.18e+00, 4.94e+00, 5.29e+00, 5.26e+00]
30000     [3.57e+00, 2.12e+00, 4.15e+00]    [0.00e+00, 4.96e+00, 4.15e+00]    [6.19e+00, 4.96e+00, 5.31e+00, 5.28e+00]

Best model at step 29000:
  train loss: 9.77e+00
  test loss: 9.11e+00
  test metric: [6.18e+00, 4.94e+00, 5.29e+00, 5.26e+00]

'train' took 40.510934 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 7
[103 123  60  16 139  11  78  29 135   3]
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.079933 s

'compile' took 0.380010 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.02e+02, 1.01e+02, 2.78e+00]    [0.00e+00, 9.86e+01, 2.78e+00]    [9.86e+01, 9.86e+01, 5.37e-02, 5.37e-02]
1000      [6.89e+01, 6.97e+01, 3.14e+00]    [0.00e+00, 7.84e+01, 3.14e+00]    [7.85e+01, 7.84e+01, 7.94e-01, 7.98e-01]
2000      [1.83e+01, 1.36e+01, 4.43e+00]    [0.00e+00, 6.33e+00, 4.43e+00]    [4.71e+00, 6.33e+00, 2.06e+00, 2.18e+00]
3000      [8.95e+00, 6.86e+00, 4.60e+00]    [0.00e+00, 7.53e+00, 4.60e+00]    [6.95e+00, 7.53e+00, 2.08e+00, 2.17e+00]
4000      [8.55e+00, 3.47e+00, 4.57e+00]    [0.00e+00, 2.24e+00, 4.57e+00]    [2.21e+00, 2.24e+00, 2.22e+00, 2.22e+00]
5000      [7.41e+00, 3.27e+00, 4.54e+00]    [0.00e+00, 2.58e+00, 4.54e+00]    [2.61e+00, 2.58e+00, 2.54e+00, 2.53e+00]
6000      [6.69e+00, 3.16e+00, 4.53e+00]    [0.00e+00, 2.88e+00, 4.53e+00]    [3.03e+00, 2.88e+00, 2.86e+00, 2.81e+00]
7000      [5.87e+00, 3.00e+00, 4.53e+00]    [0.00e+00, 3.18e+00, 4.53e+00]    [3.76e+00, 3.18e+00, 3.19e+00, 3.10e+00]
8000      [5.23e+00, 2.94e+00, 4.55e+00]    [0.00e+00, 3.44e+00, 4.55e+00]    [4.79e+00, 3.44e+00, 3.50e+00, 3.37e+00]
9000      [4.82e+00, 2.85e+00, 4.56e+00]    [0.00e+00, 3.61e+00, 4.56e+00]    [5.37e+00, 3.61e+00, 3.73e+00, 3.56e+00]
10000     [4.63e+00, 2.70e+00, 4.55e+00]    [0.00e+00, 3.69e+00, 4.55e+00]    [5.48e+00, 3.69e+00, 3.85e+00, 3.64e+00]
11000     [4.55e+00, 2.59e+00, 4.53e+00]    [0.00e+00, 3.72e+00, 4.53e+00]    [5.46e+00, 3.72e+00, 3.90e+00, 3.66e+00]
12000     [4.46e+00, 2.49e+00, 4.51e+00]    [0.00e+00, 3.79e+00, 4.51e+00]    [5.48e+00, 3.79e+00, 3.98e+00, 3.73e+00]
13000     [4.42e+00, 2.41e+00, 4.49e+00]    [0.00e+00, 3.83e+00, 4.49e+00]    [5.43e+00, 3.83e+00, 4.02e+00, 3.76e+00]
14000     [4.38e+00, 2.35e+00, 4.46e+00]    [0.00e+00, 3.85e+00, 4.46e+00]    [5.16e+00, 3.85e+00, 4.01e+00, 3.76e+00]
15000     [4.32e+00, 2.25e+00, 4.43e+00]    [0.00e+00, 3.90e+00, 4.43e+00]    [4.84e+00, 3.90e+00, 4.02e+00, 3.80e+00]
16000     [4.28e+00, 2.16e+00, 4.40e+00]    [0.00e+00, 4.00e+00, 4.40e+00]    [4.61e+00, 4.00e+00, 4.02e+00, 3.87e+00]
17000     [4.30e+00, 2.16e+00, 4.37e+00]    [0.00e+00, 4.01e+00, 4.37e+00]    [4.60e+00, 4.01e+00, 4.01e+00, 3.87e+00]
18000     [4.19e+00, 2.12e+00, 4.34e+00]    [0.00e+00, 4.00e+00, 4.34e+00]    [4.62e+00, 4.00e+00, 4.01e+00, 3.86e+00]
19000     [4.24e+00, 2.20e+00, 4.32e+00]    [0.00e+00, 3.99e+00, 4.32e+00]    [4.66e+00, 3.99e+00, 4.01e+00, 3.85e+00]
20000     [4.19e+00, 2.14e+00, 4.29e+00]    [0.00e+00, 3.99e+00, 4.29e+00]    [4.66e+00, 3.99e+00, 3.99e+00, 3.83e+00]
21000     [4.15e+00, 2.07e+00, 4.27e+00]    [0.00e+00, 3.99e+00, 4.27e+00]    [4.64e+00, 3.99e+00, 3.97e+00, 3.83e+00]
22000     [4.15e+00, 2.06e+00, 4.24e+00]    [0.00e+00, 3.98e+00, 4.24e+00]    [4.64e+00, 3.98e+00, 3.96e+00, 3.81e+00]
23000     [4.10e+00, 2.06e+00, 4.22e+00]    [0.00e+00, 3.98e+00, 4.22e+00]    [4.64e+00, 3.98e+00, 3.96e+00, 3.80e+00]
24000     [4.07e+00, 2.02e+00, 4.19e+00]    [0.00e+00, 3.98e+00, 4.19e+00]    [4.63e+00, 3.98e+00, 3.94e+00, 3.80e+00]
25000     [4.02e+00, 1.99e+00, 4.17e+00]    [0.00e+00, 3.98e+00, 4.17e+00]    [4.63e+00, 3.98e+00, 3.94e+00, 3.79e+00]
26000     [4.02e+00, 1.98e+00, 4.15e+00]    [0.00e+00, 3.98e+00, 4.15e+00]    [4.61e+00, 3.98e+00, 3.92e+00, 3.78e+00]
27000     [4.02e+00, 1.99e+00, 4.13e+00]    [0.00e+00, 3.98e+00, 4.13e+00]    [4.61e+00, 3.98e+00, 3.92e+00, 3.77e+00]
28000     [4.00e+00, 1.97e+00, 4.11e+00]    [0.00e+00, 3.98e+00, 4.11e+00]    [4.59e+00, 3.98e+00, 3.91e+00, 3.76e+00]
29000     [3.96e+00, 1.92e+00, 4.09e+00]    [0.00e+00, 3.98e+00, 4.09e+00]    [4.57e+00, 3.98e+00, 3.89e+00, 3.75e+00]
30000     [3.98e+00, 1.92e+00, 4.07e+00]    [0.00e+00, 3.98e+00, 4.07e+00]    [4.55e+00, 3.98e+00, 3.88e+00, 3.74e+00]

Best model at step 30000:
  train loss: 9.97e+00
  test loss: 8.05e+00
  test metric: [4.55e+00, 3.98e+00, 3.88e+00, 3.74e+00]

'train' took 38.392315 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 8
[119  81  32 134 100  90 132  29  73 131]
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.080239 s

'compile' took 0.378920 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.01e+02, 1.01e+02, 2.82e+00]    [0.00e+00, 9.95e+01, 2.82e+00]    [9.95e+01, 9.95e+01, 4.24e-02, 4.24e-02]
1000      [7.15e+01, 7.26e+01, 3.11e+00]    [0.00e+00, 8.04e+01, 3.11e+00]    [8.08e+01, 8.04e+01, 6.54e-01, 6.68e-01]
2000      [2.25e+01, 1.41e+01, 4.45e+00]    [0.00e+00, 7.38e+00, 4.45e+00]    [2.30e+00, 7.38e+00, 2.40e+00, 2.51e+00]
3000      [7.55e+00, 8.91e+00, 4.74e+00]    [0.00e+00, 1.56e+01, 4.74e+00]    [1.35e+01, 1.56e+01, 3.18e+00, 3.37e+00]
4000      [7.80e+00, 4.58e+00, 4.71e+00]    [0.00e+00, 5.67e+00, 4.71e+00]    [5.86e+00, 5.67e+00, 2.83e+00, 2.81e+00]
5000      [7.07e+00, 3.25e+00, 4.70e+00]    [0.00e+00, 3.45e+00, 4.70e+00]    [4.75e+00, 3.45e+00, 3.19e+00, 3.11e+00]
6000      [5.70e+00, 3.18e+00, 4.71e+00]    [0.00e+00, 3.99e+00, 4.71e+00]    [6.94e+00, 3.99e+00, 3.84e+00, 3.61e+00]
7000      [5.05e+00, 3.02e+00, 4.69e+00]    [0.00e+00, 4.20e+00, 4.69e+00]    [7.72e+00, 4.20e+00, 4.17e+00, 3.91e+00]
8000      [4.81e+00, 2.73e+00, 4.66e+00]    [0.00e+00, 4.10e+00, 4.66e+00]    [7.23e+00, 4.10e+00, 4.38e+00, 4.10e+00]
9000      [4.64e+00, 2.65e+00, 4.61e+00]    [0.00e+00, 4.19e+00, 4.61e+00]    [7.26e+00, 4.19e+00, 4.48e+00, 4.21e+00]
10000     [4.53e+00, 2.58e+00, 4.57e+00]    [0.00e+00, 4.20e+00, 4.57e+00]    [7.10e+00, 4.20e+00, 4.51e+00, 4.25e+00]
11000     [4.39e+00, 2.48e+00, 4.54e+00]    [0.00e+00, 4.21e+00, 4.54e+00]    [6.98e+00, 4.21e+00, 4.54e+00, 4.29e+00]
12000     [4.31e+00, 2.46e+00, 4.51e+00]    [0.00e+00, 4.23e+00, 4.51e+00]    [6.97e+00, 4.23e+00, 4.57e+00, 4.33e+00]
13000     [4.21e+00, 2.36e+00, 4.48e+00]    [0.00e+00, 4.25e+00, 4.48e+00]    [6.88e+00, 4.25e+00, 4.60e+00, 4.36e+00]
14000     [4.14e+00, 2.33e+00, 4.46e+00]    [0.00e+00, 4.28e+00, 4.46e+00]    [6.89e+00, 4.28e+00, 4.64e+00, 4.40e+00]
15000     [4.05e+00, 2.29e+00, 4.43e+00]    [0.00e+00, 4.32e+00, 4.43e+00]    [6.91e+00, 4.32e+00, 4.68e+00, 4.44e+00]
16000     [3.99e+00, 2.26e+00, 4.40e+00]    [0.00e+00, 4.36e+00, 4.40e+00]    [6.77e+00, 4.36e+00, 4.71e+00, 4.50e+00]
17000     [3.96e+00, 2.25e+00, 4.38e+00]    [0.00e+00, 4.41e+00, 4.38e+00]    [6.64e+00, 4.41e+00, 4.75e+00, 4.56e+00]
18000     [3.92e+00, 2.25e+00, 4.35e+00]    [0.00e+00, 4.47e+00, 4.35e+00]    [6.55e+00, 4.47e+00, 4.80e+00, 4.62e+00]
19000     [3.87e+00, 2.19e+00, 4.33e+00]    [0.00e+00, 4.53e+00, 4.33e+00]    [6.47e+00, 4.53e+00, 4.85e+00, 4.68e+00]
20000     [3.86e+00, 2.19e+00, 4.30e+00]    [0.00e+00, 4.57e+00, 4.30e+00]    [6.39e+00, 4.57e+00, 4.88e+00, 4.71e+00]
21000     [3.82e+00, 2.22e+00, 4.27e+00]    [0.00e+00, 4.61e+00, 4.27e+00]    [6.32e+00, 4.61e+00, 4.91e+00, 4.74e+00]
22000     [3.76e+00, 2.14e+00, 4.25e+00]    [0.00e+00, 4.64e+00, 4.25e+00]    [6.29e+00, 4.64e+00, 4.94e+00, 4.78e+00]
23000     [3.70e+00, 2.13e+00, 4.23e+00]    [0.00e+00, 4.67e+00, 4.23e+00]    [6.26e+00, 4.67e+00, 4.98e+00, 4.81e+00]
24000     [3.69e+00, 2.13e+00, 4.21e+00]    [0.00e+00, 4.71e+00, 4.21e+00]    [6.23e+00, 4.71e+00, 5.02e+00, 4.85e+00]
25000     [3.64e+00, 2.09e+00, 4.18e+00]    [0.00e+00, 4.75e+00, 4.18e+00]    [6.17e+00, 4.75e+00, 5.05e+00, 4.88e+00]
26000     [3.60e+00, 2.08e+00, 4.17e+00]    [0.00e+00, 4.78e+00, 4.17e+00]    [6.14e+00, 4.78e+00, 5.08e+00, 4.91e+00]
27000     [3.57e+00, 2.06e+00, 4.15e+00]    [0.00e+00, 4.80e+00, 4.15e+00]    [6.09e+00, 4.80e+00, 5.11e+00, 4.94e+00]
28000     [3.57e+00, 2.06e+00, 4.13e+00]    [0.00e+00, 4.83e+00, 4.13e+00]    [6.05e+00, 4.83e+00, 5.13e+00, 4.97e+00]
29000     [3.57e+00, 2.05e+00, 4.11e+00]    [0.00e+00, 4.86e+00, 4.11e+00]    [6.00e+00, 4.86e+00, 5.16e+00, 5.00e+00]
30000     [3.56e+00, 2.10e+00, 4.10e+00]    [0.00e+00, 4.89e+00, 4.10e+00]    [6.00e+00, 4.89e+00, 5.18e+00, 5.03e+00]

Best model at step 29000:
  train loss: 9.74e+00
  test loss: 8.98e+00
  test metric: [6.00e+00, 4.86e+00, 5.16e+00, 5.00e+00]

'train' took 39.013779 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 9
[111  40 106  37  42  38  81 131  20 109]
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.088978 s

'compile' took 0.386040 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.02e+02, 1.01e+02, 2.86e+00]    [0.00e+00, 9.91e+01, 2.86e+00]    [9.91e+01, 9.91e+01, 7.88e-02, 7.88e-02]
1000      [7.15e+01, 7.32e+01, 3.12e+00]    [0.00e+00, 8.15e+01, 3.12e+00]    [8.17e+01, 8.15e+01, 3.60e-01, 3.64e-01]
2000      [2.43e+01, 1.27e+01, 4.44e+00]    [0.00e+00, 1.71e+00, 4.44e+00]    [3.05e+00, 1.71e+00, 2.17e+00, 1.64e+00]
3000      [1.13e+01, 5.27e+00, 4.71e+00]    [0.00e+00, 3.39e+00, 4.71e+00]    [2.46e+00, 3.39e+00, 1.81e+00, 1.87e+00]
4000      [8.77e+00, 3.66e+00, 4.70e+00]    [0.00e+00, 3.64e+00, 4.70e+00]    [3.51e+00, 3.64e+00, 2.53e+00, 2.55e+00]
5000      [7.58e+00, 3.41e+00, 4.69e+00]    [0.00e+00, 4.32e+00, 4.69e+00]    [4.65e+00, 4.32e+00, 3.10e+00, 3.05e+00]
6000      [6.71e+00, 3.20e+00, 4.69e+00]    [0.00e+00, 4.54e+00, 4.69e+00]    [5.57e+00, 4.54e+00, 3.51e+00, 3.34e+00]
7000      [6.00e+00, 2.94e+00, 4.69e+00]    [0.00e+00, 4.66e+00, 4.69e+00]    [6.75e+00, 4.66e+00, 3.90e+00, 3.55e+00]
8000      [5.52e+00, 2.84e+00, 4.66e+00]    [0.00e+00, 4.88e+00, 4.66e+00]    [7.59e+00, 4.88e+00, 4.12e+00, 3.67e+00]
9000      [5.24e+00, 2.77e+00, 4.63e+00]    [0.00e+00, 5.08e+00, 4.63e+00]    [8.01e+00, 5.08e+00, 4.28e+00, 3.82e+00]
10000     [5.08e+00, 2.71e+00, 4.59e+00]    [0.00e+00, 5.17e+00, 4.59e+00]    [8.17e+00, 5.17e+00, 4.35e+00, 3.88e+00]
11000     [4.96e+00, 2.64e+00, 4.56e+00]    [0.00e+00, 5.20e+00, 4.56e+00]    [8.33e+00, 5.20e+00, 4.42e+00, 3.94e+00]
12000     [4.93e+00, 2.59e+00, 4.52e+00]    [0.00e+00, 5.23e+00, 4.52e+00]    [8.40e+00, 5.23e+00, 4.46e+00, 3.99e+00]
13000     [4.87e+00, 2.46e+00, 4.47e+00]    [0.00e+00, 5.19e+00, 4.47e+00]    [8.27e+00, 5.19e+00, 4.46e+00, 4.01e+00]
14000     [4.86e+00, 2.44e+00, 4.43e+00]    [0.00e+00, 5.20e+00, 4.43e+00]    [8.27e+00, 5.20e+00, 4.48e+00, 4.03e+00]
15000     [4.81e+00, 2.37e+00, 4.40e+00]    [0.00e+00, 5.21e+00, 4.40e+00]    [8.28e+00, 5.21e+00, 4.49e+00, 4.04e+00]
16000     [4.78e+00, 2.39e+00, 4.37e+00]    [0.00e+00, 5.25e+00, 4.37e+00]    [8.32e+00, 5.25e+00, 4.52e+00, 4.06e+00]
17000     [4.71e+00, 2.32e+00, 4.34e+00]    [0.00e+00, 5.26e+00, 4.34e+00]    [8.34e+00, 5.26e+00, 4.54e+00, 4.08e+00]
18000     [4.66e+00, 2.32e+00, 4.31e+00]    [0.00e+00, 5.28e+00, 4.31e+00]    [8.39e+00, 5.28e+00, 4.57e+00, 4.11e+00]
19000     [4.63e+00, 2.31e+00, 4.28e+00]    [0.00e+00, 5.29e+00, 4.28e+00]    [8.38e+00, 5.29e+00, 4.59e+00, 4.13e+00]
20000     [4.63e+00, 2.36e+00, 4.25e+00]    [0.00e+00, 5.31e+00, 4.25e+00]    [8.39e+00, 5.31e+00, 4.60e+00, 4.15e+00]
21000     [4.54e+00, 2.27e+00, 4.23e+00]    [0.00e+00, 5.30e+00, 4.23e+00]    [8.34e+00, 5.30e+00, 4.61e+00, 4.17e+00]
22000     [4.50e+00, 2.28e+00, 4.20e+00]    [0.00e+00, 5.29e+00, 4.20e+00]    [8.31e+00, 5.29e+00, 4.62e+00, 4.18e+00]
23000     [4.46e+00, 2.25e+00, 4.18e+00]    [0.00e+00, 5.29e+00, 4.18e+00]    [8.31e+00, 5.29e+00, 4.63e+00, 4.20e+00]
24000     [4.43e+00, 2.25e+00, 4.16e+00]    [0.00e+00, 5.28e+00, 4.16e+00]    [8.30e+00, 5.28e+00, 4.64e+00, 4.21e+00]
25000     [4.37e+00, 2.23e+00, 4.13e+00]    [0.00e+00, 5.28e+00, 4.13e+00]    [8.33e+00, 5.28e+00, 4.65e+00, 4.22e+00]
26000     [4.39e+00, 2.29e+00, 4.11e+00]    [0.00e+00, 5.29e+00, 4.11e+00]    [8.42e+00, 5.29e+00, 4.66e+00, 4.22e+00]
27000     [4.31e+00, 2.26e+00, 4.09e+00]    [0.00e+00, 5.29e+00, 4.09e+00]    [8.49e+00, 5.29e+00, 4.68e+00, 4.23e+00]
28000     [4.26e+00, 2.23e+00, 4.07e+00]    [0.00e+00, 5.30e+00, 4.07e+00]    [8.56e+00, 5.30e+00, 4.69e+00, 4.24e+00]
29000     [4.26e+00, 2.27e+00, 4.06e+00]    [0.00e+00, 5.32e+00, 4.06e+00]    [8.65e+00, 5.32e+00, 4.71e+00, 4.25e+00]
30000     [4.23e+00, 2.25e+00, 4.04e+00]    [0.00e+00, 5.32e+00, 4.04e+00]    [8.69e+00, 5.32e+00, 4.73e+00, 4.26e+00]

Best model at step 30000:
  train loss: 1.05e+01
  test loss: 9.35e+00
  test metric: [8.69e+00, 5.32e+00, 4.73e+00, 4.26e+00]

'train' took 41.074374 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...
Estar 10
[4.83266573 4.72930886] [0.67804674 0.79662264]
              Case          n     E (GPa)   sy (GPa)     C (GPa)    dP/dh (N/m)      Wp/Wt    E* (GPa)      sy/E*
count    97.000000  97.000000   97.000000  97.000000   97.000000      97.000000  97.000000   97.000000  97.000000
mean    279.030928   0.213917  107.163804   1.327507   92.953350  128652.067967   0.731227  102.813375   0.013835
std     411.446469   0.178797   67.175628   1.126477   66.232911   85768.846959   0.134844   60.541899   0.009753
min       1.000000   0.000000   10.000000   0.030000    2.731300    9801.900000   0.451835   10.880844   0.001399
25%      37.000000   0.100000   50.000000   0.300000   39.269000   53194.000000   0.628612   52.343315   0.005508
50%      67.000000   0.177243  100.806000   1.000000   80.813000  123181.000000   0.740598  100.685905   0.011463
75%      91.000000   0.300000  170.000000   2.000000  144.840000  194391.000000   0.830543  159.806250   0.019105
max    1023.000000   0.500000  210.000000   5.298021  261.330000  336838.000000   0.971835  190.913667   0.038209
              Case          n     E (GPa)   sy (GPa)     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  14.000000   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499   1.436338   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848   1.652156   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000   0.036792    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500   0.303588   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000   0.985823   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000   1.719617   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000   5.298021  239.235773  326727.270700   0.971982
         C (GPa)   dP/dh (N/m)     Wp/Wt  E* (GPa)  sy (GPa)    hm (um)  s0.033 (GPa)  s0.066 (GPa)    s0.1 (GPa)     n
count   6.000000  6.000000e+00  6.000000       6.0     6.000   6.000000         6.000        6.0000  6.000000e+00  6.00
mean   27.442667  5.086449e+06  0.896445      70.2     0.284  10.393377         0.338        0.3521  3.601000e-01  0.08
std     0.423656  1.380619e+05  0.007840       0.0     0.000   0.079388         0.000        0.0000  6.080942e-17  0.00
min    26.987000  4.844687e+06  0.888736      70.2     0.284  10.252010         0.338        0.3521  3.601000e-01  0.08
25%    27.211000  5.040021e+06  0.889265      70.2     0.284  10.375540         0.338        0.3521  3.601000e-01  0.08
50%    27.333500  5.122407e+06  0.896090      70.2     0.284  10.411875         0.338        0.3521  3.601000e-01  0.08
75%    27.561750  5.173140e+06  0.903514      70.2     0.284  10.430668         0.338        0.3521  3.601000e-01  0.08
max    28.194000  5.226581e+06  0.904776      70.2     0.284  10.484140         0.338        0.3521  3.601000e-01  0.08
         C (GPa)   dP/dh (N/m)     Wp/Wt      E* (GPa)  sy (GPa)   hm (um)  s0.033 (GPa)  s0.066 (GPa)  s0.1 (GPa)      n
count   6.000000  6.000000e+00  6.000000  6.000000e+00       6.0  6.000000        6.0000        6.0000      6.0000  6.000
mean   42.730833  4.005429e+06  0.834886  7.340000e+01       0.5  8.281341        0.6175        0.6658      0.6993  0.122
std     1.297025  5.215213e+04  0.003463  1.556721e-14       0.0  0.130865        0.0000        0.0000      0.0000  0.000
min    40.860000  3.959111e+06  0.831285  7.340000e+01       0.5  8.112528        0.6175        0.6658      0.6993  0.122
25%    42.078750  3.978600e+06  0.831943  7.340000e+01       0.5  8.182806        0.6175        0.6658      0.6993  0.122
50%    42.700000  3.985781e+06  0.834454  7.340000e+01       0.5  8.290619        0.6175        0.6658      0.6993  0.122
75%    43.442750  4.013358e+06  0.837797  7.340000e+01       0.5  8.371142        0.6175        0.6658      0.6993  0.122
max    44.563000  4.103142e+06  0.839108  7.340000e+01       0.5  8.449428        0.6175        0.6658      0.6993  0.122

Iteration: 0
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.080112 s

'compile' took 0.385494 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [9.90e+01, 1.05e+02, 2.85e+00]    [0.00e+00, 1.16e+02, 2.85e+00]    [1.16e+02, 1.16e+02, 1.56e-01, 1.56e-01]
1000      [7.88e+01, 5.45e+01, 2.85e+00]    [0.00e+00, 2.48e+01, 2.85e+00]    [2.50e+01, 2.48e+01, 8.48e-01, 8.50e-01]
2000      [3.82e+01, 2.30e+01, 3.94e+00]    [0.00e+00, 9.96e+00, 3.94e+00]    [1.22e+01, 9.96e+00, 2.87e-01, 2.94e-01]
3000      [7.37e+00, 4.35e+00, 4.54e+00]    [0.00e+00, 5.73e+00, 4.54e+00]    [2.92e+00, 5.73e+00, 1.17e+00, 1.21e+00]
4000      [4.85e+00, 2.67e+00, 4.57e+00]    [0.00e+00, 6.92e+00, 4.57e+00]    [5.61e+00, 6.92e+00, 1.31e+00, 1.33e+00]
5000      [4.26e+00, 1.96e+00, 4.51e+00]    [0.00e+00, 5.90e+00, 4.51e+00]    [5.38e+00, 5.90e+00, 1.09e+00, 1.10e+00]
6000      [3.88e+00, 1.56e+00, 4.46e+00]    [0.00e+00, 5.04e+00, 4.46e+00]    [5.09e+00, 5.04e+00, 1.01e+00, 1.01e+00]
7000      [3.68e+00, 1.35e+00, 4.40e+00]    [0.00e+00, 3.65e+00, 4.40e+00]    [3.99e+00, 3.65e+00, 8.51e-01, 8.50e-01]
8000      [3.54e+00, 1.28e+00, 4.35e+00]    [0.00e+00, 3.36e+00, 4.35e+00]    [3.90e+00, 3.36e+00, 9.14e-01, 9.11e-01]
9000      [3.41e+00, 1.09e+00, 4.30e+00]    [0.00e+00, 3.76e+00, 4.30e+00]    [4.50e+00, 3.76e+00, 9.60e-01, 9.56e-01]
10000     [3.30e+00, 1.02e+00, 4.25e+00]    [0.00e+00, 2.11e+00, 4.25e+00]    [2.97e+00, 2.11e+00, 7.53e-01, 7.49e-01]
11000     [3.23e+00, 9.91e-01, 4.21e+00]    [0.00e+00, 1.51e+00, 4.21e+00]    [2.55e+00, 1.51e+00, 6.42e-01, 6.37e-01]
12000     [3.18e+00, 9.15e-01, 4.17e+00]    [0.00e+00, 9.80e-01, 4.17e+00]    [2.18e+00, 9.80e-01, 6.11e-01, 5.53e-01]
13000     [3.08e+00, 8.56e-01, 4.13e+00]    [0.00e+00, 8.61e-01, 4.13e+00]    [2.27e+00, 8.61e-01, 5.70e-01, 4.70e-01]
14000     [3.00e+00, 8.81e-01, 4.09e+00]    [0.00e+00, 6.22e-01, 4.09e+00]    [1.12e+00, 6.22e-01, 5.84e-01, 4.52e-01]
15000     [2.98e+00, 7.46e-01, 4.05e+00]    [0.00e+00, 5.29e-01, 4.05e+00]    [1.97e+00, 5.29e-01, 6.43e-01, 4.25e-01]
16000     [2.95e+00, 7.07e-01, 4.01e+00]    [0.00e+00, 4.59e-01, 4.01e+00]    [1.90e+00, 4.59e-01, 6.40e-01, 4.54e-01]
17000     [2.92e+00, 7.14e-01, 3.97e+00]    [0.00e+00, 9.72e-01, 3.97e+00]    [2.66e+00, 9.72e-01, 6.92e-01, 5.32e-01]
18000     [2.88e+00, 7.00e-01, 3.94e+00]    [0.00e+00, 1.38e+00, 3.94e+00]    [3.16e+00, 1.38e+00, 7.73e-01, 7.61e-01]
19000     [2.85e+00, 6.82e-01, 3.90e+00]    [0.00e+00, 1.59e+00, 3.90e+00]    [3.36e+00, 1.59e+00, 8.14e-01, 8.02e-01]
20000     [2.82e+00, 6.48e-01, 3.87e+00]    [0.00e+00, 2.31e+00, 3.87e+00]    [4.09e+00, 2.31e+00, 9.16e-01, 9.03e-01]
21000     [2.81e+00, 6.47e-01, 3.84e+00]    [0.00e+00, 3.16e+00, 3.84e+00]    [4.99e+00, 3.16e+00, 9.94e-01, 9.79e-01]
22000     [2.78e+00, 6.39e-01, 3.81e+00]    [0.00e+00, 3.80e+00, 3.81e+00]    [5.69e+00, 3.80e+00, 1.01e+00, 9.90e-01]
23000     [2.79e+00, 6.19e-01, 3.78e+00]    [0.00e+00, 7.66e+00, 3.78e+00]    [9.68e+00, 7.66e+00, 1.15e+00, 1.13e+00]
24000     [2.77e+00, 5.60e-01, 3.75e+00]    [0.00e+00, 8.23e+00, 3.75e+00]    [1.03e+01, 8.23e+00, 1.22e+00, 1.20e+00]
25000     [2.75e+00, 6.12e-01, 3.73e+00]    [0.00e+00, 7.96e+00, 3.73e+00]    [1.00e+01, 7.96e+00, 1.26e+00, 1.24e+00]
26000     [2.74e+00, 5.41e-01, 3.70e+00]    [0.00e+00, 8.41e+00, 3.70e+00]    [1.05e+01, 8.41e+00, 1.29e+00, 1.27e+00]
27000     [2.74e+00, 6.67e-01, 3.68e+00]    [0.00e+00, 8.45e+00, 3.68e+00]    [1.05e+01, 8.45e+00, 1.33e+00, 1.31e+00]
28000     [2.73e+00, 5.65e-01, 3.66e+00]    [0.00e+00, 9.05e+00, 3.66e+00]    [1.11e+01, 9.05e+00, 1.35e+00, 1.33e+00]
29000     [2.74e+00, 7.07e-01, 3.63e+00]    [0.00e+00, 8.99e+00, 3.63e+00]    [1.11e+01, 8.99e+00, 1.39e+00, 1.36e+00]
30000     [2.73e+00, 5.18e-01, 3.61e+00]    [0.00e+00, 1.01e+01, 3.61e+00]    [1.22e+01, 1.01e+01, 1.42e+00, 1.39e+00]

Best model at step 30000:
  train loss: 6.86e+00
  test loss: 1.37e+01
  test metric: [1.22e+01, 1.01e+01, 1.42e+00, 1.39e+00]

'train' took 38.092269 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 1
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.076869 s

'compile' took 0.377863 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.00e+02, 9.34e+01, 2.66e+00]    [0.00e+00, 8.13e+01, 2.66e+00]    [8.13e+01, 8.13e+01, 2.28e-01, 2.28e-01]
1000      [7.87e+01, 5.48e+01, 2.70e+00]    [0.00e+00, 2.33e+01, 2.70e+00]    [2.34e+01, 2.33e+01, 8.16e-01, 8.17e-01]
2000      [4.53e+01, 2.67e+01, 3.69e+00]    [0.00e+00, 8.28e+00, 3.69e+00]    [9.89e+00, 8.28e+00, 1.79e-01, 1.72e-01]
3000      [8.85e+00, 5.74e+00, 4.47e+00]    [0.00e+00, 7.03e+00, 4.47e+00]    [2.95e+00, 7.03e+00, 8.73e-01, 9.75e-01]
4000      [5.43e+00, 3.20e+00, 4.52e+00]    [0.00e+00, 6.85e+00, 4.52e+00]    [5.36e+00, 6.85e+00, 8.50e-01, 8.92e-01]
5000      [4.79e+00, 2.50e+00, 4.46e+00]    [0.00e+00, 5.95e+00, 4.46e+00]    [5.46e+00, 5.95e+00, 7.13e-01, 7.29e-01]
6000      [4.37e+00, 2.03e+00, 4.41e+00]    [0.00e+00, 6.03e+00, 4.41e+00]    [5.93e+00, 6.03e+00, 6.95e-01, 7.00e-01]
7000      [4.08e+00, 1.73e+00, 4.35e+00]    [0.00e+00, 5.27e+00, 4.35e+00]    [5.42e+00, 5.27e+00, 6.17e-01, 6.19e-01]
8000      [3.89e+00, 1.66e+00, 4.29e+00]    [0.00e+00, 5.66e+00, 4.29e+00]    [5.94e+00, 5.66e+00, 6.00e-01, 6.01e-01]
9000      [3.78e+00, 1.52e+00, 4.23e+00]    [0.00e+00, 5.04e+00, 4.23e+00]    [5.40e+00, 5.04e+00, 6.08e-01, 6.09e-01]
10000     [3.71e+00, 1.36e+00, 4.18e+00]    [0.00e+00, 5.63e+00, 4.18e+00]    [6.11e+00, 5.63e+00, 5.74e-01, 5.76e-01]
11000     [3.59e+00, 1.34e+00, 4.14e+00]    [0.00e+00, 5.14e+00, 4.14e+00]    [5.76e+00, 5.14e+00, 5.79e-01, 5.80e-01]
12000     [3.51e+00, 1.22e+00, 4.10e+00]    [0.00e+00, 4.93e+00, 4.10e+00]    [5.72e+00, 4.93e+00, 5.64e-01, 5.64e-01]
13000     [3.41e+00, 1.14e+00, 4.06e+00]    [0.00e+00, 4.56e+00, 4.06e+00]    [5.56e+00, 4.56e+00, 5.49e-01, 5.49e-01]
14000     [3.32e+00, 1.05e+00, 4.02e+00]    [0.00e+00, 4.30e+00, 4.02e+00]    [5.46e+00, 4.30e+00, 5.39e-01, 5.38e-01]
15000     [3.30e+00, 9.47e-01, 3.99e+00]    [0.00e+00, 3.55e+00, 3.99e+00]    [4.79e+00, 3.55e+00, 4.41e-01, 4.41e-01]
16000     [3.24e+00, 9.20e-01, 3.95e+00]    [0.00e+00, 2.98e+00, 3.95e+00]    [4.28e+00, 2.98e+00, 4.38e-01, 4.38e-01]
17000     [3.22e+00, 9.61e-01, 3.92e+00]    [0.00e+00, 3.41e+00, 3.92e+00]    [4.76e+00, 3.41e+00, 4.23e-01, 4.23e-01]
18000     [3.18e+00, 9.38e-01, 3.89e+00]    [0.00e+00, 2.18e+00, 3.89e+00]    [3.61e+00, 2.18e+00, 4.44e-01, 4.43e-01]
19000     [3.17e+00, 8.73e-01, 3.86e+00]    [0.00e+00, 2.46e+00, 3.86e+00]    [4.01e+00, 2.46e+00, 4.91e-01, 4.90e-01]
20000     [3.14e+00, 7.58e-01, 3.83e+00]    [0.00e+00, 2.92e+00, 3.83e+00]    [4.59e+00, 2.92e+00, 4.65e-01, 4.64e-01]
21000     [3.14e+00, 6.55e-01, 3.80e+00]    [0.00e+00, 2.38e+00, 3.80e+00]    [4.15e+00, 2.38e+00, 4.33e-01, 4.32e-01]
22000     [3.08e+00, 6.31e-01, 3.78e+00]    [0.00e+00, 1.23e+00, 3.78e+00]    [3.09e+00, 1.23e+00, 3.04e-01, 3.04e-01]
23000     [3.05e+00, 5.86e-01, 3.75e+00]    [0.00e+00, 8.59e-01, 3.75e+00]    [2.82e+00, 8.59e-01, 3.10e-01, 3.09e-01]
24000     [3.02e+00, 6.03e-01, 3.73e+00]    [0.00e+00, 5.15e-01, 3.73e+00]    [2.58e+00, 5.15e-01, 3.03e-01, 3.03e-01]
25000     [3.00e+00, 5.15e-01, 3.70e+00]    [0.00e+00, 8.59e-01, 3.70e+00]    [3.02e+00, 8.59e-01, 3.27e-01, 3.26e-01]
26000     [2.98e+00, 4.93e-01, 3.68e+00]    [0.00e+00, 1.00e+00, 3.68e+00]    [3.26e+00, 1.00e+00, 3.43e-01, 3.41e-01]
27000     [2.94e+00, 6.15e-01, 3.65e+00]    [0.00e+00, 9.97e-01, 3.65e+00]    [3.34e+00, 9.97e-01, 4.11e-01, 4.08e-01]
28000     [2.96e+00, 5.83e-01, 3.63e+00]    [0.00e+00, 9.58e-01, 3.63e+00]    [3.33e+00, 9.58e-01, 3.95e-01, 3.92e-01]
29000     [2.94e+00, 5.03e-01, 3.61e+00]    [0.00e+00, 1.25e+00, 3.61e+00]    [3.64e+00, 1.25e+00, 4.16e-01, 4.13e-01]
30000     [2.91e+00, 6.96e-01, 3.58e+00]    [0.00e+00, 2.52e+00, 3.58e+00]    [4.94e+00, 2.52e+00, 4.95e-01, 4.91e-01]

Best model at step 29000:
  train loss: 7.05e+00
  test loss: 4.86e+00
  test metric: [3.64e+00, 1.25e+00, 4.16e-01, 4.13e-01]

'train' took 39.365431 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 2
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.075474 s

'compile' took 0.371558 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [9.70e+01, 9.99e+01, 2.85e+00]    [0.00e+00, 1.05e+02, 2.85e+00]    [1.05e+02, 1.05e+02, 1.49e-02, 1.49e-02]
1000      [7.88e+01, 5.42e+01, 2.89e+00]    [0.00e+00, 2.46e+01, 2.89e+00]    [2.47e+01, 2.46e+01, 8.64e-01, 8.65e-01]
2000      [4.76e+01, 2.84e+01, 3.73e+00]    [0.00e+00, 1.39e+01, 3.73e+00]    [1.65e+01, 1.39e+01, 4.17e-01, 4.24e-01]
3000      [1.28e+01, 7.52e+00, 4.41e+00]    [0.00e+00, 4.70e+00, 4.41e+00]    [8.10e-01, 4.70e+00, 5.07e-01, 8.25e-01]
4000      [6.06e+00, 3.85e+00, 4.55e+00]    [0.00e+00, 1.19e+01, 4.55e+00]    [9.91e+00, 1.19e+01, 1.26e+00, 1.29e+00]
5000      [4.95e+00, 2.75e+00, 4.53e+00]    [0.00e+00, 1.18e+01, 4.53e+00]    [1.09e+01, 1.18e+01, 1.14e+00, 1.15e+00]
6000      [4.40e+00, 2.09e+00, 4.50e+00]    [0.00e+00, 1.20e+01, 4.50e+00]    [1.18e+01, 1.20e+01, 1.22e+00, 1.21e+00]
7000      [4.06e+00, 1.79e+00, 4.46e+00]    [0.00e+00, 1.11e+01, 4.46e+00]    [1.12e+01, 1.11e+01, 1.18e+00, 1.16e+00]
8000      [3.86e+00, 1.64e+00, 4.41e+00]    [0.00e+00, 1.09e+01, 4.41e+00]    [1.10e+01, 1.09e+01, 1.12e+00, 1.10e+00]
9000      [3.72e+00, 1.52e+00, 4.36e+00]    [0.00e+00, 1.06e+01, 4.36e+00]    [1.08e+01, 1.06e+01, 1.11e+00, 1.08e+00]
10000     [3.62e+00, 1.52e+00, 4.32e+00]    [0.00e+00, 9.68e+00, 4.32e+00]    [9.99e+00, 9.68e+00, 1.12e+00, 1.10e+00]
11000     [3.49e+00, 1.41e+00, 4.29e+00]    [0.00e+00, 8.47e+00, 4.29e+00]    [8.79e+00, 8.47e+00, 9.74e-01, 9.49e-01]
12000     [3.41e+00, 1.28e+00, 4.25e+00]    [0.00e+00, 8.25e+00, 4.25e+00]    [8.60e+00, 8.25e+00, 9.21e-01, 8.96e-01]
13000     [3.29e+00, 1.22e+00, 4.21e+00]    [0.00e+00, 7.65e+00, 4.21e+00]    [8.00e+00, 7.65e+00, 8.63e-01, 8.39e-01]
14000     [3.26e+00, 1.15e+00, 4.17e+00]    [0.00e+00, 7.17e+00, 4.17e+00]    [7.57e+00, 7.17e+00, 8.35e-01, 8.09e-01]
15000     [3.17e+00, 1.06e+00, 4.14e+00]    [0.00e+00, 7.15e+00, 4.14e+00]    [7.58e+00, 7.15e+00, 8.47e-01, 8.19e-01]
16000     [3.15e+00, 1.19e+00, 4.11e+00]    [0.00e+00, 6.92e+00, 4.11e+00]    [7.38e+00, 6.92e+00, 7.46e-01, 7.20e-01]
17000     [3.05e+00, 9.69e-01, 4.07e+00]    [0.00e+00, 5.77e+00, 4.07e+00]    [6.20e+00, 5.77e+00, 7.63e-01, 7.36e-01]
18000     [3.00e+00, 9.02e-01, 4.04e+00]    [0.00e+00, 5.29e+00, 4.04e+00]    [5.69e+00, 5.29e+00, 7.00e-01, 6.74e-01]
19000     [2.98e+00, 9.09e-01, 4.01e+00]    [0.00e+00, 5.12e+00, 4.01e+00]    [5.51e+00, 5.12e+00, 6.78e-01, 6.51e-01]
20000     [2.95e+00, 9.78e-01, 3.98e+00]    [0.00e+00, 3.87e+00, 3.98e+00]    [4.24e+00, 3.87e+00, 6.65e-01, 6.38e-01]
21000     [2.92e+00, 9.73e-01, 3.95e+00]    [0.00e+00, 5.05e+00, 3.95e+00]    [5.42e+00, 5.05e+00, 6.78e-01, 6.51e-01]
22000     [2.85e+00, 8.32e-01, 3.92e+00]    [0.00e+00, 3.17e+00, 3.92e+00]    [3.50e+00, 3.17e+00, 6.14e-01, 5.88e-01]
23000     [2.83e+00, 7.48e-01, 3.90e+00]    [0.00e+00, 3.23e+00, 3.90e+00]    [3.56e+00, 3.23e+00, 5.90e-01, 5.63e-01]
24000     [2.80e+00, 8.38e-01, 3.87e+00]    [0.00e+00, 3.29e+00, 3.87e+00]    [3.63e+00, 3.29e+00, 5.70e-01, 5.44e-01]
25000     [2.76e+00, 6.96e-01, 3.84e+00]    [0.00e+00, 2.22e+00, 3.84e+00]    [2.53e+00, 2.22e+00, 5.13e-01, 4.87e-01]
26000     [2.76e+00, 6.82e-01, 3.81e+00]    [0.00e+00, 1.68e+00, 3.81e+00]    [1.97e+00, 1.68e+00, 4.91e-01, 4.65e-01]
27000     [2.78e+00, 6.74e-01, 3.79e+00]    [0.00e+00, 1.29e+00, 3.79e+00]    [1.56e+00, 1.29e+00, 4.53e-01, 4.28e-01]
28000     [2.73e+00, 6.60e-01, 3.76e+00]    [0.00e+00, 4.44e-01, 3.76e+00]    [6.25e-01, 4.44e-01, 4.03e-01, 3.15e-01]
29000     [2.72e+00, 6.11e-01, 3.74e+00]    [0.00e+00, 3.41e-01, 3.74e+00]    [5.14e-01, 3.41e-01, 3.44e-01, 3.01e-01]
30000     [2.71e+00, 5.99e-01, 3.72e+00]    [0.00e+00, 3.78e-01, 3.72e+00]    [3.34e-01, 3.78e-01, 2.18e-01, 2.02e-01]

Best model at step 30000:
  train loss: 7.02e+00
  test loss: 4.09e+00
  test metric: [3.34e-01, 3.78e-01, 2.18e-01, 2.02e-01]

'train' took 40.568438 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 3
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.087559 s

'compile' took 0.386222 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [9.91e+01, 8.97e+01, 2.62e+00]    [0.00e+00, 7.65e+01, 2.62e+00]    [7.65e+01, 7.65e+01, 2.71e-01, 2.71e-01]
1000      [7.88e+01, 5.47e+01, 2.61e+00]    [0.00e+00, 2.40e+01, 2.61e+00]    [2.42e+01, 2.40e+01, 8.01e-01, 8.04e-01]
2000      [4.60e+01, 2.71e+01, 3.49e+00]    [0.00e+00, 8.43e+00, 3.49e+00]    [1.05e+01, 8.43e+00, 2.27e-01, 2.32e-01]
3000      [8.22e+00, 4.26e+00, 4.29e+00]    [0.00e+00, 1.63e+01, 4.29e+00]    [1.23e+01, 1.63e+01, 1.34e+00, 1.39e+00]
4000      [5.02e+00, 2.79e+00, 4.35e+00]    [0.00e+00, 1.89e+01, 4.35e+00]    [1.71e+01, 1.89e+01, 1.56e+00, 1.58e+00]
5000      [4.41e+00, 2.18e+00, 4.29e+00]    [0.00e+00, 1.80e+01, 4.29e+00]    [1.72e+01, 1.80e+01, 1.34e+00, 1.35e+00]
6000      [4.07e+00, 1.92e+00, 4.23e+00]    [0.00e+00, 1.70e+01, 4.23e+00]    [1.68e+01, 1.70e+01, 1.33e+00, 1.33e+00]
7000      [3.80e+00, 1.77e+00, 4.17e+00]    [0.00e+00, 1.72e+01, 4.17e+00]    [1.74e+01, 1.72e+01, 1.32e+00, 1.31e+00]
8000      [3.70e+00, 1.73e+00, 4.11e+00]    [0.00e+00, 1.67e+01, 4.11e+00]    [1.71e+01, 1.67e+01, 1.24e+00, 1.23e+00]
9000      [3.56e+00, 1.53e+00, 4.06e+00]    [0.00e+00, 1.51e+01, 4.06e+00]    [1.58e+01, 1.51e+01, 1.19e+00, 1.18e+00]
10000     [3.48e+00, 1.46e+00, 4.02e+00]    [0.00e+00, 1.42e+01, 4.02e+00]    [1.50e+01, 1.42e+01, 1.21e+00, 1.19e+00]
11000     [3.38e+00, 1.30e+00, 3.98e+00]    [0.00e+00, 1.38e+01, 3.98e+00]    [1.49e+01, 1.38e+01, 1.19e+00, 1.17e+00]
12000     [3.33e+00, 1.25e+00, 3.94e+00]    [0.00e+00, 1.33e+01, 3.94e+00]    [1.44e+01, 1.33e+01, 1.15e+00, 1.14e+00]
13000     [3.25e+00, 1.18e+00, 3.90e+00]    [0.00e+00, 1.20e+01, 3.90e+00]    [1.32e+01, 1.20e+01, 1.03e+00, 1.01e+00]
14000     [3.19e+00, 1.10e+00, 3.86e+00]    [0.00e+00, 1.15e+01, 3.86e+00]    [1.28e+01, 1.15e+01, 9.85e-01, 9.67e-01]
15000     [3.23e+00, 1.29e+00, 3.83e+00]    [0.00e+00, 9.61e+00, 3.83e+00]    [1.09e+01, 9.61e+00, 9.42e-01, 9.25e-01]
16000     [3.10e+00, 1.01e+00, 3.80e+00]    [0.00e+00, 1.01e+01, 3.80e+00]    [1.14e+01, 1.01e+01, 9.15e-01, 8.98e-01]
17000     [3.07e+00, 1.03e+00, 3.77e+00]    [0.00e+00, 9.04e+00, 3.77e+00]    [1.04e+01, 9.04e+00, 9.16e-01, 8.97e-01]
18000     [3.03e+00, 9.81e-01, 3.73e+00]    [0.00e+00, 9.43e+00, 3.73e+00]    [1.09e+01, 9.43e+00, 8.95e-01, 8.76e-01]
19000     [3.03e+00, 9.17e-01, 3.70e+00]    [0.00e+00, 9.22e+00, 3.70e+00]    [1.08e+01, 9.22e+00, 8.79e-01, 8.59e-01]
20000     [3.01e+00, 9.09e-01, 3.67e+00]    [0.00e+00, 8.64e+00, 3.67e+00]    [1.02e+01, 8.64e+00, 8.47e-01, 8.28e-01]
21000     [2.97e+00, 8.58e-01, 3.64e+00]    [0.00e+00, 8.25e+00, 3.64e+00]    [9.79e+00, 8.25e+00, 8.28e-01, 8.08e-01]
22000     [3.02e+00, 9.00e-01, 3.61e+00]    [0.00e+00, 7.53e+00, 3.61e+00]    [9.08e+00, 7.53e+00, 8.03e-01, 7.83e-01]
23000     [2.93e+00, 8.28e-01, 3.58e+00]    [0.00e+00, 7.45e+00, 3.58e+00]    [9.00e+00, 7.45e+00, 7.96e-01, 7.77e-01]
24000     [2.92e+00, 8.26e-01, 3.56e+00]    [0.00e+00, 7.79e+00, 3.56e+00]    [9.35e+00, 7.79e+00, 7.68e-01, 7.49e-01]
25000     [2.94e+00, 7.68e-01, 3.53e+00]    [0.00e+00, 7.81e+00, 3.53e+00]    [9.38e+00, 7.81e+00, 7.36e-01, 7.17e-01]
26000     [2.89e+00, 7.08e-01, 3.51e+00]    [0.00e+00, 7.29e+00, 3.51e+00]    [8.84e+00, 7.29e+00, 7.02e-01, 6.84e-01]
27000     [2.89e+00, 7.95e-01, 3.48e+00]    [0.00e+00, 7.05e+00, 3.48e+00]    [8.61e+00, 7.05e+00, 6.72e-01, 6.55e-01]
28000     [2.89e+00, 9.27e-01, 3.46e+00]    [0.00e+00, 6.97e+00, 3.46e+00]    [8.54e+00, 6.97e+00, 6.43e-01, 6.26e-01]
29000     [2.85e+00, 7.14e-01, 3.44e+00]    [0.00e+00, 5.26e+00, 3.44e+00]    [6.79e+00, 5.26e+00, 6.22e-01, 6.05e-01]
30000     [2.88e+00, 7.44e-01, 3.41e+00]    [0.00e+00, 4.82e+00, 3.41e+00]    [6.34e+00, 4.82e+00, 6.01e-01, 5.85e-01]

Best model at step 29000:
  train loss: 7.00e+00
  test loss: 8.70e+00
  test metric: [6.79e+00, 5.26e+00, 6.22e-01, 6.05e-01]

'train' took 39.204697 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 4
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.078995 s

'compile' took 0.405832 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.01e+02, 1.10e+02, 2.72e+00]    [0.00e+00, 1.26e+02, 2.72e+00]    [1.26e+02, 1.26e+02, 3.19e-01, 3.19e-01]
1000      [7.85e+01, 5.50e+01, 2.72e+00]    [0.00e+00, 2.27e+01, 2.72e+00]    [2.30e+01, 2.27e+01, 7.72e-01, 7.74e-01]
2000      [4.35e+01, 2.57e+01, 3.57e+00]    [0.00e+00, 8.67e+00, 3.57e+00]    [1.10e+01, 8.67e+00, 2.60e-01, 2.66e-01]
3000      [8.95e+00, 5.09e+00, 4.25e+00]    [0.00e+00, 8.61e+00, 4.25e+00]    [4.88e+00, 8.61e+00, 8.31e-01, 8.61e-01]
4000      [4.54e+00, 2.61e+00, 4.36e+00]    [0.00e+00, 1.30e+01, 4.36e+00]    [1.24e+01, 1.30e+01, 1.18e+00, 1.18e+00]
5000      [3.87e+00, 1.92e+00, 4.35e+00]    [0.00e+00, 1.26e+01, 4.35e+00]    [1.32e+01, 1.26e+01, 1.06e+00, 1.04e+00]
6000      [3.59e+00, 1.69e+00, 4.30e+00]    [0.00e+00, 1.19e+01, 4.30e+00]    [1.28e+01, 1.19e+01, 9.60e-01, 9.44e-01]
7000      [3.48e+00, 1.54e+00, 4.24e+00]    [0.00e+00, 1.05e+01, 4.24e+00]    [1.15e+01, 1.05e+01, 9.50e-01, 9.33e-01]
8000      [3.37e+00, 1.47e+00, 4.19e+00]    [0.00e+00, 9.93e+00, 4.19e+00]    [1.11e+01, 9.93e+00, 9.32e-01, 9.13e-01]
9000      [3.32e+00, 1.39e+00, 4.15e+00]    [0.00e+00, 9.29e+00, 4.15e+00]    [1.07e+01, 9.29e+00, 8.94e-01, 8.73e-01]
10000     [3.23e+00, 1.27e+00, 4.11e+00]    [0.00e+00, 8.12e+00, 4.11e+00]    [9.72e+00, 8.12e+00, 7.62e-01, 7.43e-01]
11000     [3.19e+00, 1.24e+00, 4.07e+00]    [0.00e+00, 6.87e+00, 4.07e+00]    [8.59e+00, 6.87e+00, 6.93e-01, 6.74e-01]
12000     [3.13e+00, 1.23e+00, 4.03e+00]    [0.00e+00, 6.73e+00, 4.03e+00]    [8.59e+00, 6.73e+00, 6.48e-01, 6.30e-01]
13000     [3.10e+00, 1.13e+00, 3.99e+00]    [0.00e+00, 5.66e+00, 3.99e+00]    [7.59e+00, 5.66e+00, 5.91e-01, 5.73e-01]
14000     [3.08e+00, 1.11e+00, 3.95e+00]    [0.00e+00, 4.43e+00, 3.95e+00]    [6.36e+00, 4.43e+00, 5.72e-01, 5.54e-01]
15000     [3.06e+00, 1.08e+00, 3.91e+00]    [0.00e+00, 4.50e+00, 3.91e+00]    [6.47e+00, 4.50e+00, 5.50e-01, 5.33e-01]
16000     [3.08e+00, 1.26e+00, 3.88e+00]    [0.00e+00, 4.61e+00, 3.88e+00]    [6.61e+00, 4.61e+00, 5.27e-01, 5.10e-01]
17000     [3.01e+00, 1.17e+00, 3.84e+00]    [0.00e+00, 2.31e+00, 3.84e+00]    [4.28e+00, 2.31e+00, 5.09e-01, 4.93e-01]
18000     [2.97e+00, 9.68e-01, 3.81e+00]    [0.00e+00, 2.65e+00, 3.81e+00]    [4.66e+00, 2.65e+00, 4.73e-01, 4.58e-01]
19000     [2.96e+00, 9.58e-01, 3.78e+00]    [0.00e+00, 2.08e+00, 3.78e+00]    [4.08e+00, 2.08e+00, 4.50e-01, 4.36e-01]
20000     [2.92e+00, 9.45e-01, 3.75e+00]    [0.00e+00, 1.32e+00, 3.75e+00]    [3.32e+00, 1.32e+00, 4.37e-01, 4.23e-01]
21000     [2.90e+00, 9.16e-01, 3.72e+00]    [0.00e+00, 9.23e-01, 3.72e+00]    [2.92e+00, 9.23e-01, 4.22e-01, 4.08e-01]
22000     [2.91e+00, 1.06e+00, 3.69e+00]    [0.00e+00, 3.12e-01, 3.69e+00]    [1.96e+00, 3.12e-01, 4.16e-01, 2.54e-01]
23000     [2.86e+00, 8.75e-01, 3.66e+00]    [0.00e+00, 2.97e-01, 3.66e+00]    [2.12e+00, 2.97e-01, 4.02e-01, 2.78e-01]
24000     [2.84e+00, 9.22e-01, 3.64e+00]    [0.00e+00, 5.40e-01, 3.64e+00]    [1.54e+00, 5.40e-01, 3.94e-01, 2.51e-01]
25000     [2.87e+00, 9.12e-01, 3.61e+00]    [0.00e+00, 8.33e-01, 3.61e+00]    [1.17e+00, 8.33e-01, 3.63e-01, 3.51e-01]
26000     [2.82e+00, 8.61e-01, 3.59e+00]    [0.00e+00, 1.22e+00, 3.59e+00]    [7.78e-01, 1.22e+00, 3.34e-01, 3.22e-01]
27000     [2.80e+00, 7.95e-01, 3.56e+00]    [0.00e+00, 1.67e+00, 3.56e+00]    [3.45e-01, 1.67e+00, 2.99e-01, 3.13e-01]
28000     [2.77e+00, 7.78e-01, 3.54e+00]    [0.00e+00, 2.14e+00, 3.54e+00]    [3.06e-01, 2.14e+00, 1.69e-01, 2.98e-01]
29000     [2.77e+00, 7.86e-01, 3.52e+00]    [0.00e+00, 2.31e+00, 3.52e+00]    [4.13e-01, 2.31e+00, 1.72e-01, 2.85e-01]
30000     [2.77e+00, 7.57e-01, 3.49e+00]    [0.00e+00, 2.45e+00, 3.49e+00]    [4.99e-01, 2.45e+00, 2.36e-01, 2.80e-01]

Best model at step 30000:
  train loss: 7.02e+00
  test loss: 5.94e+00
  test metric: [4.99e-01, 2.45e+00, 2.36e-01, 2.80e-01]

'train' took 39.171807 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 5
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.077199 s

'compile' took 0.374186 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [9.83e+01, 1.05e+02, 2.66e+00]    [0.00e+00, 1.13e+02, 2.66e+00]    [1.13e+02, 1.13e+02, 1.49e-01, 1.49e-01]
1000      [7.80e+01, 5.32e+01, 2.73e+00]    [0.00e+00, 2.18e+01, 2.73e+00]    [2.18e+01, 2.18e+01, 6.92e-01, 6.91e-01]
2000      [4.31e+01, 2.59e+01, 3.67e+00]    [0.00e+00, 3.61e+00, 3.67e+00]    [3.98e+00, 3.61e+00, 2.38e-01, 2.42e-01]
3000      [6.95e+00, 4.43e+00, 4.50e+00]    [0.00e+00, 1.45e+01, 4.50e+00]    [1.33e+01, 1.45e+01, 1.34e+00, 1.37e+00]
4000      [4.72e+00, 2.92e+00, 4.53e+00]    [0.00e+00, 1.97e+01, 4.53e+00]    [1.94e+01, 1.97e+01, 1.82e+00, 1.83e+00]
5000      [4.15e+00, 2.17e+00, 4.47e+00]    [0.00e+00, 1.75e+01, 4.47e+00]    [1.79e+01, 1.75e+01, 1.68e+00, 1.68e+00]
6000      [3.94e+00, 1.97e+00, 4.38e+00]    [0.00e+00, 1.58e+01, 4.38e+00]    [1.66e+01, 1.58e+01, 1.51e+00, 1.50e+00]
7000      [3.73e+00, 1.82e+00, 4.30e+00]    [0.00e+00, 1.51e+01, 4.30e+00]    [1.62e+01, 1.51e+01, 1.55e+00, 1.53e+00]
8000      [3.57e+00, 1.71e+00, 4.24e+00]    [0.00e+00, 1.44e+01, 4.24e+00]    [1.57e+01, 1.44e+01, 1.58e+00, 1.56e+00]
9000      [3.45e+00, 1.57e+00, 4.18e+00]    [0.00e+00, 1.40e+01, 4.18e+00]    [1.55e+01, 1.40e+01, 1.59e+00, 1.57e+00]
10000     [3.37e+00, 1.54e+00, 4.13e+00]    [0.00e+00, 1.29e+01, 4.13e+00]    [1.46e+01, 1.29e+01, 1.59e+00, 1.57e+00]
11000     [3.22e+00, 1.35e+00, 4.09e+00]    [0.00e+00, 1.15e+01, 4.09e+00]    [1.34e+01, 1.15e+01, 1.42e+00, 1.40e+00]
12000     [3.16e+00, 1.39e+00, 4.04e+00]    [0.00e+00, 1.02e+01, 4.04e+00]    [1.21e+01, 1.02e+01, 1.45e+00, 1.42e+00]
13000     [3.13e+00, 1.36e+00, 3.99e+00]    [0.00e+00, 9.24e+00, 3.99e+00]    [1.12e+01, 9.24e+00, 1.41e+00, 1.38e+00]
14000     [3.12e+00, 1.14e+00, 3.95e+00]    [0.00e+00, 9.71e+00, 3.95e+00]    [1.17e+01, 9.71e+00, 1.42e+00, 1.39e+00]
15000     [3.06e+00, 1.10e+00, 3.91e+00]    [0.00e+00, 8.90e+00, 3.91e+00]    [1.08e+01, 8.90e+00, 1.44e+00, 1.41e+00]
16000     [3.02e+00, 9.96e-01, 3.87e+00]    [0.00e+00, 8.37e+00, 3.87e+00]    [1.03e+01, 8.37e+00, 1.40e+00, 1.37e+00]
17000     [2.96e+00, 9.13e-01, 3.83e+00]    [0.00e+00, 8.14e+00, 3.83e+00]    [9.98e+00, 8.14e+00, 1.39e+00, 1.36e+00]
18000     [2.95e+00, 8.86e-01, 3.80e+00]    [0.00e+00, 6.80e+00, 3.80e+00]    [8.56e+00, 6.80e+00, 1.20e+00, 1.18e+00]
19000     [2.90e+00, 8.25e-01, 3.76e+00]    [0.00e+00, 6.04e+00, 3.76e+00]    [7.75e+00, 6.04e+00, 1.18e+00, 1.16e+00]
20000     [2.89e+00, 8.13e-01, 3.73e+00]    [0.00e+00, 5.47e+00, 3.73e+00]    [7.13e+00, 5.47e+00, 1.19e+00, 1.16e+00]
21000     [2.90e+00, 8.37e-01, 3.70e+00]    [0.00e+00, 5.04e+00, 3.70e+00]    [6.67e+00, 5.04e+00, 1.10e+00, 1.08e+00]
22000     [2.87e+00, 9.09e-01, 3.67e+00]    [0.00e+00, 3.00e+00, 3.67e+00]    [4.58e+00, 3.00e+00, 9.77e-01, 9.56e-01]
23000     [2.85e+00, 7.74e-01, 3.64e+00]    [0.00e+00, 2.78e+00, 3.64e+00]    [4.33e+00, 2.78e+00, 9.13e-01, 8.94e-01]
24000     [2.83e+00, 7.12e-01, 3.61e+00]    [0.00e+00, 2.55e+00, 3.61e+00]    [4.09e+00, 2.55e+00, 8.82e-01, 8.63e-01]
25000     [2.85e+00, 8.59e-01, 3.58e+00]    [0.00e+00, 2.97e+00, 3.58e+00]    [4.49e+00, 2.97e+00, 8.63e-01, 8.44e-01]
26000     [2.85e+00, 7.24e-01, 3.56e+00]    [0.00e+00, 1.74e+00, 3.56e+00]    [3.23e+00, 1.74e+00, 8.13e-01, 7.95e-01]
27000     [2.79e+00, 6.44e-01, 3.53e+00]    [0.00e+00, 1.39e+00, 3.53e+00]    [2.79e+00, 1.39e+00, 8.17e-01, 6.92e-01]
28000     [2.80e+00, 6.57e-01, 3.51e+00]    [0.00e+00, 1.21e+00, 3.51e+00]    [2.51e+00, 1.21e+00, 8.15e-01, 5.52e-01]
29000     [2.76e+00, 5.95e-01, 3.48e+00]    [0.00e+00, 1.17e+00, 3.48e+00]    [2.40e+00, 1.17e+00, 8.36e-01, 5.13e-01]
30000     [2.76e+00, 6.12e-01, 3.46e+00]    [0.00e+00, 1.08e+00, 3.46e+00]    [2.26e+00, 1.08e+00, 8.17e-01, 4.69e-01]

Best model at step 30000:
  train loss: 6.83e+00
  test loss: 4.54e+00
  test metric: [2.26e+00, 1.08e+00, 8.17e-01, 4.69e-01]

'train' took 40.002315 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 6
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.083544 s

'compile' took 0.388921 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.00e+02, 1.04e+02, 2.76e+00]    [0.00e+00, 1.09e+02, 2.76e+00]    [1.09e+02, 1.09e+02, 1.01e-01, 1.01e-01]
1000      [7.87e+01, 5.44e+01, 2.77e+00]    [0.00e+00, 2.37e+01, 2.77e+00]    [2.43e+01, 2.37e+01, 7.80e-01, 7.86e-01]
2000      [4.45e+01, 2.65e+01, 3.64e+00]    [0.00e+00, 8.34e+00, 3.64e+00]    [1.41e+01, 8.34e+00, 2.09e-01, 2.28e-01]
3000      [9.94e+00, 5.41e+00, 4.29e+00]    [0.00e+00, 9.28e+00, 4.29e+00]    [1.75e+00, 9.28e+00, 7.90e-01, 1.15e+00]
4000      [5.77e+00, 3.20e+00, 4.36e+00]    [0.00e+00, 1.28e+01, 4.36e+00]    [9.65e+00, 1.28e+01, 1.51e+00, 1.54e+00]
5000      [4.84e+00, 2.32e+00, 4.32e+00]    [0.00e+00, 1.05e+01, 4.32e+00]    [9.62e+00, 1.05e+01, 1.29e+00, 1.30e+00]
6000      [4.38e+00, 1.89e+00, 4.27e+00]    [0.00e+00, 9.59e+00, 4.27e+00]    [9.67e+00, 9.59e+00, 1.20e+00, 1.20e+00]
7000      [4.05e+00, 1.72e+00, 4.23e+00]    [0.00e+00, 8.34e+00, 4.23e+00]    [8.94e+00, 8.34e+00, 1.11e+00, 1.10e+00]
8000      [3.87e+00, 1.62e+00, 4.18e+00]    [0.00e+00, 8.12e+00, 4.18e+00]    [9.02e+00, 8.12e+00, 1.17e+00, 1.17e+00]
9000      [3.71e+00, 1.45e+00, 4.13e+00]    [0.00e+00, 8.05e+00, 4.13e+00]    [9.25e+00, 8.05e+00, 1.15e+00, 1.14e+00]
10000     [3.59e+00, 1.46e+00, 4.09e+00]    [0.00e+00, 7.96e+00, 4.09e+00]    [9.37e+00, 7.96e+00, 1.12e+00, 1.11e+00]
11000     [3.48e+00, 1.22e+00, 4.05e+00]    [0.00e+00, 5.71e+00, 4.05e+00]    [7.28e+00, 5.71e+00, 9.42e-01, 9.32e-01]
12000     [3.40e+00, 1.08e+00, 4.02e+00]    [0.00e+00, 5.24e+00, 4.02e+00]    [6.97e+00, 5.24e+00, 9.16e-01, 9.05e-01]
13000     [3.33e+00, 9.69e-01, 3.99e+00]    [0.00e+00, 5.40e+00, 3.99e+00]    [7.29e+00, 5.40e+00, 9.14e-01, 9.02e-01]
14000     [3.24e+00, 8.81e-01, 3.96e+00]    [0.00e+00, 5.12e+00, 3.96e+00]    [7.08e+00, 5.12e+00, 9.57e-01, 9.44e-01]
15000     [3.20e+00, 8.42e-01, 3.93e+00]    [0.00e+00, 4.97e+00, 3.93e+00]    [7.00e+00, 4.97e+00, 9.21e-01, 9.08e-01]
16000     [3.19e+00, 8.21e-01, 3.89e+00]    [0.00e+00, 3.66e+00, 3.89e+00]    [5.66e+00, 3.66e+00, 7.59e-01, 7.49e-01]
17000     [3.16e+00, 7.79e-01, 3.86e+00]    [0.00e+00, 2.43e+00, 3.86e+00]    [4.39e+00, 2.43e+00, 6.16e-01, 6.08e-01]
18000     [3.08e+00, 6.82e-01, 3.84e+00]    [0.00e+00, 1.89e+00, 3.84e+00]    [3.84e+00, 1.89e+00, 5.90e-01, 5.83e-01]
19000     [3.07e+00, 7.63e-01, 3.81e+00]    [0.00e+00, 1.73e+00, 3.81e+00]    [3.69e+00, 1.73e+00, 5.42e-01, 5.36e-01]
20000     [3.06e+00, 6.67e-01, 3.78e+00]    [0.00e+00, 8.67e-01, 3.78e+00]    [2.78e+00, 8.67e-01, 5.28e-01, 4.40e-01]
21000     [3.01e+00, 6.60e-01, 3.76e+00]    [0.00e+00, 8.43e-01, 3.76e+00]    [2.76e+00, 8.43e-01, 5.15e-01, 4.32e-01]
22000     [2.99e+00, 7.64e-01, 3.73e+00]    [0.00e+00, 8.89e-01, 3.73e+00]    [2.86e+00, 8.89e-01, 4.85e-01, 4.80e-01]
23000     [2.95e+00, 5.60e-01, 3.71e+00]    [0.00e+00, 4.57e-01, 3.71e+00]    [2.20e+00, 4.57e-01, 5.04e-01, 3.04e-01]
24000     [2.93e+00, 5.52e-01, 3.69e+00]    [0.00e+00, 8.01e-01, 3.69e+00]    [2.66e+00, 8.01e-01, 5.75e-01, 3.76e-01]
25000     [2.91e+00, 6.36e-01, 3.67e+00]    [0.00e+00, 6.26e-01, 3.67e+00]    [2.34e+00, 6.26e-01, 6.31e-01, 3.57e-01]
26000     [2.89e+00, 5.12e-01, 3.65e+00]    [0.00e+00, 1.01e+00, 3.65e+00]    [2.93e+00, 1.01e+00, 6.38e-01, 4.94e-01]
27000     [2.90e+00, 6.15e-01, 3.63e+00]    [0.00e+00, 1.39e+00, 3.63e+00]    [3.41e+00, 1.39e+00, 6.70e-01, 6.61e-01]
28000     [2.86e+00, 4.68e-01, 3.61e+00]    [0.00e+00, 1.44e+00, 3.61e+00]    [3.47e+00, 1.44e+00, 7.48e-01, 7.37e-01]
29000     [2.88e+00, 6.19e-01, 3.59e+00]    [0.00e+00, 1.53e+00, 3.59e+00]    [3.55e+00, 1.53e+00, 6.86e-01, 6.77e-01]
30000     [2.81e+00, 4.47e-01, 3.57e+00]    [0.00e+00, 1.28e+00, 3.57e+00]    [3.30e+00, 1.28e+00, 6.83e-01, 6.74e-01]

Best model at step 30000:
  train loss: 6.83e+00
  test loss: 4.85e+00
  test metric: [3.30e+00, 1.28e+00, 6.83e-01, 6.74e-01]

'train' took 39.911951 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 7
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.078913 s

'compile' took 0.387656 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.02e+02, 9.56e+01, 2.65e+00]    [0.00e+00, 8.61e+01, 2.65e+00]    [8.61e+01, 8.61e+01, 1.36e-01, 1.36e-01]
1000      [7.93e+01, 5.49e+01, 2.63e+00]    [0.00e+00, 2.27e+01, 2.63e+00]    [2.30e+01, 2.27e+01, 7.49e-01, 7.51e-01]
2000      [5.14e+01, 2.99e+01, 3.38e+00]    [0.00e+00, 4.50e+00, 3.38e+00]    [6.45e+00, 4.50e+00, 1.54e-01, 1.60e-01]
3000      [9.57e+00, 5.46e+00, 4.22e+00]    [0.00e+00, 2.24e+01, 4.22e+00]    [1.74e+01, 2.24e+01, 1.38e+00, 1.46e+00]
4000      [5.49e+00, 3.58e+00, 4.28e+00]    [0.00e+00, 2.49e+01, 4.28e+00]    [2.26e+01, 2.49e+01, 1.68e+00, 1.72e+00]
5000      [4.63e+00, 2.77e+00, 4.24e+00]    [0.00e+00, 2.48e+01, 4.24e+00]    [2.35e+01, 2.48e+01, 1.71e+00, 1.74e+00]
6000      [4.12e+00, 2.29e+00, 4.18e+00]    [0.00e+00, 2.33e+01, 4.18e+00]    [2.27e+01, 2.33e+01, 1.51e+00, 1.52e+00]
7000      [3.86e+00, 2.09e+00, 4.11e+00]    [0.00e+00, 2.26e+01, 4.11e+00]    [2.24e+01, 2.26e+01, 1.55e+00, 1.56e+00]
8000      [3.68e+00, 1.91e+00, 4.05e+00]    [0.00e+00, 2.17e+01, 4.05e+00]    [2.16e+01, 2.17e+01, 1.56e+00, 1.55e+00]
9000      [3.49e+00, 1.71e+00, 4.00e+00]    [0.00e+00, 2.08e+01, 4.00e+00]    [2.11e+01, 2.08e+01, 1.53e+00, 1.52e+00]
10000     [3.38e+00, 1.57e+00, 3.95e+00]    [0.00e+00, 1.90e+01, 3.95e+00]    [1.95e+01, 1.90e+01, 1.41e+00, 1.39e+00]
11000     [3.28e+00, 1.45e+00, 3.90e+00]    [0.00e+00, 1.84e+01, 3.90e+00]    [1.91e+01, 1.84e+01, 1.39e+00, 1.37e+00]
12000     [3.19e+00, 1.39e+00, 3.86e+00]    [0.00e+00, 1.71e+01, 3.86e+00]    [1.80e+01, 1.71e+01, 1.35e+00, 1.33e+00]
13000     [3.13e+00, 1.27e+00, 3.83e+00]    [0.00e+00, 1.65e+01, 3.83e+00]    [1.77e+01, 1.65e+01, 1.32e+00, 1.29e+00]
14000     [3.08e+00, 1.19e+00, 3.79e+00]    [0.00e+00, 1.59e+01, 3.79e+00]    [1.73e+01, 1.59e+01, 1.30e+00, 1.27e+00]
15000     [3.03e+00, 1.11e+00, 3.76e+00]    [0.00e+00, 1.55e+01, 3.76e+00]    [1.71e+01, 1.55e+01, 1.32e+00, 1.28e+00]
16000     [3.00e+00, 1.06e+00, 3.73e+00]    [0.00e+00, 1.35e+01, 3.73e+00]    [1.50e+01, 1.35e+01, 1.21e+00, 1.17e+00]
17000     [2.99e+00, 1.02e+00, 3.69e+00]    [0.00e+00, 1.32e+01, 3.69e+00]    [1.48e+01, 1.32e+01, 1.11e+00, 1.08e+00]
18000     [2.98e+00, 9.67e-01, 3.66e+00]    [0.00e+00, 1.20e+01, 3.66e+00]    [1.36e+01, 1.20e+01, 1.06e+00, 1.03e+00]
19000     [2.97e+00, 9.97e-01, 3.63e+00]    [0.00e+00, 1.20e+01, 3.63e+00]    [1.36e+01, 1.20e+01, 1.01e+00, 9.81e-01]
20000     [2.94e+00, 8.60e-01, 3.60e+00]    [0.00e+00, 1.08e+01, 3.60e+00]    [1.24e+01, 1.08e+01, 9.94e-01, 9.65e-01]
21000     [2.95e+00, 1.01e+00, 3.58e+00]    [0.00e+00, 9.23e+00, 3.58e+00]    [1.08e+01, 9.23e+00, 9.79e-01, 9.50e-01]
22000     [2.90e+00, 8.07e-01, 3.55e+00]    [0.00e+00, 9.51e+00, 3.55e+00]    [1.11e+01, 9.51e+00, 9.36e-01, 9.08e-01]
23000     [2.92e+00, 8.15e-01, 3.52e+00]    [0.00e+00, 8.99e+00, 3.52e+00]    [1.06e+01, 8.99e+00, 9.33e-01, 9.04e-01]
24000     [2.87e+00, 8.01e-01, 3.50e+00]    [0.00e+00, 8.11e+00, 3.50e+00]    [9.66e+00, 8.11e+00, 9.08e-01, 8.80e-01]
25000     [2.91e+00, 8.00e-01, 3.48e+00]    [0.00e+00, 7.64e+00, 3.48e+00]    [9.18e+00, 7.64e+00, 8.54e-01, 8.27e-01]
26000     [2.84e+00, 7.00e-01, 3.45e+00]    [0.00e+00, 7.76e+00, 3.45e+00]    [9.31e+00, 7.76e+00, 8.49e-01, 8.22e-01]
27000     [2.83e+00, 7.04e-01, 3.43e+00]    [0.00e+00, 7.51e+00, 3.43e+00]    [9.07e+00, 7.51e+00, 8.46e-01, 8.18e-01]
28000     [2.82e+00, 6.76e-01, 3.41e+00]    [0.00e+00, 6.78e+00, 3.41e+00]    [8.32e+00, 6.78e+00, 8.29e-01, 8.02e-01]
29000     [2.81e+00, 6.79e-01, 3.39e+00]    [0.00e+00, 6.72e+00, 3.39e+00]    [8.27e+00, 6.72e+00, 8.16e-01, 7.89e-01]
30000     [2.77e+00, 6.81e-01, 3.37e+00]    [0.00e+00, 6.38e+00, 3.37e+00]    [7.92e+00, 6.38e+00, 7.87e-01, 7.60e-01]

Best model at step 30000:
  train loss: 6.82e+00
  test loss: 9.75e+00
  test metric: [7.92e+00, 6.38e+00, 7.87e-01, 7.60e-01]

'train' took 38.929668 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 8
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.080268 s

'compile' took 0.388038 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [9.96e+01, 9.92e+01, 2.73e+00]    [0.00e+00, 9.73e+01, 2.73e+00]    [9.73e+01, 9.73e+01, 7.43e-02, 7.43e-02]
1000      [7.83e+01, 5.41e+01, 2.76e+00]    [0.00e+00, 2.34e+01, 2.76e+00]    [2.35e+01, 2.34e+01, 8.27e-01, 8.28e-01]
2000      [4.65e+01, 2.77e+01, 3.56e+00]    [0.00e+00, 3.21e+00, 3.56e+00]    [4.97e+00, 3.21e+00, 1.34e-01, 1.36e-01]
3000      [8.16e+00, 4.63e+00, 4.30e+00]    [0.00e+00, 2.00e+01, 4.30e+00]    [1.58e+01, 2.00e+01, 1.48e+00, 1.54e+00]
4000      [4.90e+00, 3.16e+00, 4.34e+00]    [0.00e+00, 2.17e+01, 4.34e+00]    [1.98e+01, 2.17e+01, 1.77e+00, 1.80e+00]
5000      [4.30e+00, 2.26e+00, 4.30e+00]    [0.00e+00, 1.88e+01, 4.30e+00]    [1.81e+01, 1.88e+01, 1.50e+00, 1.51e+00]
6000      [3.91e+00, 1.86e+00, 4.25e+00]    [0.00e+00, 1.73e+01, 4.25e+00]    [1.71e+01, 1.73e+01, 1.37e+00, 1.37e+00]
7000      [3.74e+00, 1.69e+00, 4.20e+00]    [0.00e+00, 1.73e+01, 4.20e+00]    [1.73e+01, 1.73e+01, 1.34e+00, 1.33e+00]
8000      [3.61e+00, 1.61e+00, 4.15e+00]    [0.00e+00, 1.42e+01, 4.15e+00]    [1.44e+01, 1.42e+01, 1.10e+00, 1.09e+00]
9000      [3.50e+00, 1.52e+00, 4.10e+00]    [0.00e+00, 1.31e+01, 4.10e+00]    [1.36e+01, 1.31e+01, 9.76e-01, 9.64e-01]
10000     [3.44e+00, 1.43e+00, 4.06e+00]    [0.00e+00, 1.23e+01, 4.06e+00]    [1.31e+01, 1.23e+01, 9.46e-01, 9.32e-01]
11000     [3.38e+00, 1.39e+00, 4.01e+00]    [0.00e+00, 1.17e+01, 4.01e+00]    [1.27e+01, 1.17e+01, 9.06e-01, 8.90e-01]
12000     [3.32e+00, 1.37e+00, 3.97e+00]    [0.00e+00, 1.11e+01, 3.97e+00]    [1.23e+01, 1.11e+01, 8.59e-01, 8.42e-01]
13000     [3.24e+00, 1.23e+00, 3.93e+00]    [0.00e+00, 9.36e+00, 3.93e+00]    [1.05e+01, 9.36e+00, 8.48e-01, 8.31e-01]
14000     [3.18e+00, 1.19e+00, 3.90e+00]    [0.00e+00, 8.65e+00, 3.90e+00]    [9.88e+00, 8.65e+00, 7.82e-01, 7.64e-01]
15000     [3.12e+00, 1.12e+00, 3.86e+00]    [0.00e+00, 8.06e+00, 3.86e+00]    [9.34e+00, 8.06e+00, 7.42e-01, 7.24e-01]
16000     [3.10e+00, 1.18e+00, 3.83e+00]    [0.00e+00, 7.23e+00, 3.83e+00]    [8.59e+00, 7.23e+00, 6.28e-01, 6.11e-01]
17000     [3.09e+00, 1.22e+00, 3.80e+00]    [0.00e+00, 6.66e+00, 3.80e+00]    [8.03e+00, 6.66e+00, 5.78e-01, 5.62e-01]
18000     [3.02e+00, 1.08e+00, 3.77e+00]    [0.00e+00, 5.87e+00, 3.77e+00]    [7.24e+00, 5.87e+00, 5.27e-01, 5.11e-01]
19000     [3.02e+00, 1.06e+00, 3.74e+00]    [0.00e+00, 5.06e+00, 3.74e+00]    [6.42e+00, 5.06e+00, 4.86e-01, 4.70e-01]
20000     [2.95e+00, 9.32e-01, 3.71e+00]    [0.00e+00, 4.26e+00, 3.71e+00]    [5.60e+00, 4.26e+00, 4.73e-01, 4.58e-01]
21000     [2.95e+00, 9.40e-01, 3.69e+00]    [0.00e+00, 3.47e+00, 3.69e+00]    [4.79e+00, 3.47e+00, 4.33e-01, 4.19e-01]
22000     [2.97e+00, 1.04e+00, 3.66e+00]    [0.00e+00, 2.66e+00, 3.66e+00]    [3.96e+00, 2.66e+00, 4.29e-01, 4.14e-01]
23000     [2.91e+00, 8.55e-01, 3.63e+00]    [0.00e+00, 2.79e+00, 3.63e+00]    [4.08e+00, 2.79e+00, 3.94e-01, 3.80e-01]
24000     [2.89e+00, 8.78e-01, 3.61e+00]    [0.00e+00, 2.03e+00, 3.61e+00]    [3.29e+00, 2.03e+00, 3.92e-01, 3.78e-01]
25000     [2.88e+00, 7.96e-01, 3.59e+00]    [0.00e+00, 1.86e+00, 3.59e+00]    [3.11e+00, 1.86e+00, 3.63e-01, 3.49e-01]
26000     [2.85e+00, 8.06e-01, 3.56e+00]    [0.00e+00, 1.23e+00, 3.56e+00]    [2.46e+00, 1.23e+00, 3.46e-01, 3.33e-01]
27000     [2.86e+00, 7.75e-01, 3.54e+00]    [0.00e+00, 1.07e+00, 3.54e+00]    [2.27e+00, 1.07e+00, 3.28e-01, 3.15e-01]
28000     [2.83e+00, 7.60e-01, 3.52e+00]    [0.00e+00, 5.85e-01, 3.52e+00]    [1.76e+00, 5.85e-01, 3.10e-01, 2.97e-01]
29000     [2.82e+00, 8.23e-01, 3.50e+00]    [0.00e+00, 2.61e-01, 3.50e+00]    [1.10e+00, 2.61e-01, 3.01e-01, 1.28e-01]
30000     [2.80e+00, 7.11e-01, 3.48e+00]    [0.00e+00, 2.50e-01, 3.48e+00]    [1.19e+00, 2.50e-01, 2.96e-01, 1.46e-01]

Best model at step 30000:
  train loss: 6.98e+00
  test loss: 3.73e+00
  test metric: [1.19e+00, 2.50e-01, 2.96e-01, 1.46e-01]

'train' took 41.165360 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...

Iteration: 9
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.085992 s

'compile' took 0.398182 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [9.76e+01, 1.01e+02, 2.74e+00]    [0.00e+00, 1.03e+02, 2.74e+00]    [1.03e+02, 1.03e+02, 4.80e-02, 4.80e-02]
1000      [7.91e+01, 5.52e+01, 2.71e+00]    [0.00e+00, 2.32e+01, 2.71e+00]    [2.37e+01, 2.32e+01, 7.68e-01, 7.73e-01]
2000      [4.91e+01, 2.87e+01, 3.54e+00]    [0.00e+00, 4.20e+00, 3.54e+00]    [7.34e+00, 4.20e+00, 2.02e-01, 2.01e-01]
3000      [7.21e+00, 4.40e+00, 4.50e+00]    [0.00e+00, 2.33e+01, 4.50e+00]    [1.72e+01, 2.33e+01, 1.70e+00, 1.76e+00]
4000      [4.70e+00, 2.79e+00, 4.55e+00]    [0.00e+00, 2.45e+01, 4.55e+00]    [2.23e+01, 2.45e+01, 1.79e+00, 1.80e+00]
5000      [4.07e+00, 2.27e+00, 4.48e+00]    [0.00e+00, 2.18e+01, 4.48e+00]    [2.14e+01, 2.18e+01, 1.57e+00, 1.57e+00]
6000      [3.76e+00, 2.09e+00, 4.40e+00]    [0.00e+00, 2.16e+01, 4.40e+00]    [2.23e+01, 2.16e+01, 1.62e+00, 1.61e+00]
7000      [3.58e+00, 1.99e+00, 4.32e+00]    [0.00e+00, 2.15e+01, 4.32e+00]    [2.29e+01, 2.15e+01, 1.61e+00, 1.59e+00]
8000      [3.43e+00, 1.80e+00, 4.25e+00]    [0.00e+00, 1.94e+01, 4.25e+00]    [2.10e+01, 1.94e+01, 1.60e+00, 1.58e+00]
9000      [3.31e+00, 1.68e+00, 4.19e+00]    [0.00e+00, 1.78e+01, 4.19e+00]    [1.95e+01, 1.78e+01, 1.50e+00, 1.48e+00]
10000     [3.22e+00, 1.54e+00, 4.14e+00]    [0.00e+00, 1.79e+01, 4.14e+00]    [1.98e+01, 1.79e+01, 1.47e+00, 1.45e+00]
11000     [3.14e+00, 1.46e+00, 4.09e+00]    [0.00e+00, 1.72e+01, 4.09e+00]    [1.94e+01, 1.72e+01, 1.45e+00, 1.43e+00]
12000     [3.10e+00, 1.40e+00, 4.05e+00]    [0.00e+00, 1.63e+01, 4.05e+00]    [1.85e+01, 1.63e+01, 1.41e+00, 1.39e+00]
13000     [3.02e+00, 1.30e+00, 4.01e+00]    [0.00e+00, 1.56e+01, 4.01e+00]    [1.80e+01, 1.56e+01, 1.39e+00, 1.37e+00]
14000     [2.99e+00, 1.23e+00, 3.97e+00]    [0.00e+00, 1.39e+01, 3.97e+00]    [1.64e+01, 1.39e+01, 1.38e+00, 1.36e+00]
15000     [2.99e+00, 1.17e+00, 3.93e+00]    [0.00e+00, 1.34e+01, 3.93e+00]    [1.58e+01, 1.34e+01, 1.33e+00, 1.31e+00]
16000     [2.96e+00, 1.11e+00, 3.90e+00]    [0.00e+00, 1.16e+01, 3.90e+00]    [1.40e+01, 1.16e+01, 1.24e+00, 1.22e+00]
17000     [2.96e+00, 1.14e+00, 3.86e+00]    [0.00e+00, 1.03e+01, 3.86e+00]    [1.27e+01, 1.03e+01, 1.14e+00, 1.13e+00]
18000     [2.94e+00, 1.01e+00, 3.82e+00]    [0.00e+00, 9.75e+00, 3.82e+00]    [1.21e+01, 9.75e+00, 1.04e+00, 1.03e+00]
19000     [2.94e+00, 1.00e+00, 3.79e+00]    [0.00e+00, 9.02e+00, 3.79e+00]    [1.13e+01, 9.02e+00, 1.01e+00, 9.98e-01]
20000     [2.91e+00, 1.01e+00, 3.75e+00]    [0.00e+00, 7.59e+00, 3.75e+00]    [9.88e+00, 7.59e+00, 9.74e-01, 9.61e-01]
21000     [2.88e+00, 9.32e-01, 3.72e+00]    [0.00e+00, 7.30e+00, 3.72e+00]    [9.59e+00, 7.30e+00, 9.51e-01, 9.38e-01]
22000     [2.86e+00, 8.98e-01, 3.69e+00]    [0.00e+00, 6.87e+00, 3.69e+00]    [9.15e+00, 6.87e+00, 9.03e-01, 8.91e-01]
23000     [2.84e+00, 8.98e-01, 3.66e+00]    [0.00e+00, 5.63e+00, 3.66e+00]    [7.89e+00, 5.63e+00, 8.28e-01, 8.18e-01]
24000     [2.84e+00, 8.80e-01, 3.64e+00]    [0.00e+00, 5.02e+00, 3.64e+00]    [7.29e+00, 5.02e+00, 8.08e-01, 7.98e-01]
25000     [2.82e+00, 8.25e-01, 3.61e+00]    [0.00e+00, 4.86e+00, 3.61e+00]    [7.15e+00, 4.86e+00, 7.60e-01, 7.51e-01]
26000     [2.81e+00, 8.81e-01, 3.58e+00]    [0.00e+00, 4.50e+00, 3.58e+00]    [6.82e+00, 4.50e+00, 7.19e-01, 7.11e-01]
27000     [2.80e+00, 7.80e-01, 3.56e+00]    [0.00e+00, 3.58e+00, 3.56e+00]    [5.93e+00, 3.58e+00, 6.73e-01, 6.65e-01]
28000     [2.77e+00, 7.57e-01, 3.54e+00]    [0.00e+00, 3.23e+00, 3.54e+00]    [5.62e+00, 3.23e+00, 6.67e-01, 6.59e-01]
29000     [2.76e+00, 7.29e-01, 3.51e+00]    [0.00e+00, 2.84e+00, 3.51e+00]    [5.27e+00, 2.84e+00, 6.62e-01, 6.55e-01]
30000     [2.75e+00, 7.31e-01, 3.49e+00]    [0.00e+00, 2.28e+00, 3.49e+00]    [4.73e+00, 2.28e+00, 6.05e-01, 5.99e-01]

Best model at step 30000:
  train loss: 6.98e+00
  test loss: 5.77e+00
  test metric: [4.73e+00, 2.28e+00, 6.05e-01, 5.99e-01]

'train' took 40.294230 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...
Estar
[3.07017722 0.55408652] [3.02927713 0.34030252]
              Case          n     E (GPa)   sy (GPa)     C (GPa)    dP/dh (N/m)      Wp/Wt    E* (GPa)      sy/E*
count    97.000000  97.000000   97.000000  97.000000   97.000000      97.000000  97.000000   97.000000  97.000000
mean    279.030928   0.213917  107.163804   1.327507   92.953350  128652.067967   0.731227  102.813375   0.013835
std     411.446469   0.178797   67.175628   1.126477   66.232911   85768.846959   0.134844   60.541899   0.009753
min       1.000000   0.000000   10.000000   0.030000    2.731300    9801.900000   0.451835   10.880844   0.001399
25%      37.000000   0.100000   50.000000   0.300000   39.269000   53194.000000   0.628612   52.343315   0.005508
50%      67.000000   0.177243  100.806000   1.000000   80.813000  123181.000000   0.740598  100.685905   0.011463
75%      91.000000   0.300000  170.000000   2.000000  144.840000  194391.000000   0.830543  159.806250   0.019105
max    1023.000000   0.500000  210.000000   5.298021  261.330000  336838.000000   0.971835  190.913667   0.038209
              Case          n     E (GPa)   sy (GPa)     C (GPa)    dP/dh (N/m)      Wp/Wt
count    14.000000  14.000000   14.000000  14.000000   14.000000      14.000000  14.000000
mean    802.071429   0.141683  100.074499   1.436338   83.395179  127043.116339   0.757835
std     412.214557   0.087468   70.142848   1.652156   75.629024   96045.592932   0.157921
min       6.000000   0.000000   10.000000   0.036792    5.391397   13276.677320   0.452806
25%    1001.250000   0.077031   37.524500   0.303588   30.061256   42136.388600   0.675230
50%    1007.000000   0.150378   79.808000   0.985823   71.391348   98478.987680   0.784977
75%    1012.750000   0.195295  155.424000   1.719617   97.621153  202124.474350   0.870086
max    1018.000000   0.300000  210.000000   5.298021  239.235773  326727.270700   0.971982
             Case     C (GPa)    dP/dh (N/m)       Wp/Wt     hm (um)        Pm (N)  E* (GPa)  sy (GPa)  s0.008 (GPa)  s0.015 (GPa)  s0.033 (GPa)
count  144.000000  144.000000     144.000000  144.000000  144.000000  1.440000e+02    144.00   144.000    144.000000      144.0000       144.000
mean    71.500000  137.415980  195873.789365    0.718020    0.256371  9.000020e-03    109.63     1.168      1.232767        1.2634         1.285
std     41.713307    9.039179    3895.400652    0.009209    0.009170  2.199332e-08      0.00     0.000      0.000000        0.0000         0.000
min      0.000000   99.250944  187462.619200    0.697845    0.241892  8.999963e-03    109.63     1.168      1.232767        1.2634         1.285
25%     35.750000  134.713027  193513.104325    0.712033    0.250375  9.000005e-03    109.63     1.168      1.232767        1.2634         1.285
50%     71.500000  137.683012  195333.420050    0.717445    0.255671  9.000021e-03    109.63     1.168      1.232767        1.2634         1.285
75%    107.250000  143.568658  198029.934450    0.723614    0.258474  9.000034e-03    109.63     1.168      1.232767        1.2634         1.285
max    143.000000  153.816655  207237.679100    0.744966    0.301131  9.000066e-03    109.63     1.168      1.232767        1.2634         1.285
Using backend: tensorflow.compat.v1

WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building multifidelity neural network...
C:\Users\josep\AppData\Roaming\Python\Python311\site-packages\deepxde\nn\tensorflow_compat_v1\mfnn.py:114: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.078190 s

'compile' took 0.375524 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Initializing variables...
Training model...

Step      Train loss                        Test loss                         Test metric
0         [9.86e+01, 9.86e+01, 2.95e+00]    [0.00e+00, 1.01e+02, 2.95e+00]    [1.01e+02, 1.01e+02, 2.34e-02, 2.34e-02]
1000      [7.22e+01, 6.34e+01, 3.24e+00]    [0.00e+00, 8.90e+01, 3.24e+00]    [8.92e+01, 8.90e+01, 1.20e+00, 1.22e+00]
2000      [2.11e+01, 1.57e+01, 4.47e+00]    [0.00e+00, 7.48e+00, 4.47e+00]    [3.58e+00, 7.48e+00, 4.14e+00, 3.69e+00]
3000      [5.95e+00, 2.20e+00, 4.58e+00]    [0.00e+00, 3.76e+01, 4.58e+00]    [3.31e+01, 3.76e+01, 4.92e+00, 5.08e+00]
4000      [5.24e+00, 1.26e+00, 4.48e+00]    [0.00e+00, 3.71e+01, 4.48e+00]    [3.47e+01, 3.71e+01, 4.62e+00, 4.70e+00]
5000      [4.83e+00, 9.67e-01, 4.41e+00]    [0.00e+00, 3.67e+01, 4.41e+00]    [3.53e+01, 3.67e+01, 4.44e+00, 4.48e+00]
6000      [4.47e+00, 9.52e-01, 4.35e+00]    [0.00e+00, 3.65e+01, 4.35e+00]    [3.57e+01, 3.65e+01, 4.34e+00, 4.37e+00]
7000      [4.16e+00, 8.48e-01, 4.30e+00]    [0.00e+00, 3.63e+01, 4.30e+00]    [3.60e+01, 3.63e+01, 4.27e+00, 4.28e+00]
8000      [3.90e+00, 7.50e-01, 4.26e+00]    [0.00e+00, 3.60e+01, 4.26e+00]    [3.61e+01, 3.60e+01, 4.24e+00, 4.23e+00]
9000      [3.71e+00, 7.18e-01, 4.22e+00]    [0.00e+00, 3.57e+01, 4.22e+00]    [3.60e+01, 3.57e+01, 4.23e+00, 4.22e+00]
10000     [3.58e+00, 7.24e-01, 4.19e+00]    [0.00e+00, 3.55e+01, 4.19e+00]    [3.60e+01, 3.55e+01, 4.22e+00, 4.21e+00]
11000     [3.45e+00, 6.37e-01, 4.16e+00]    [0.00e+00, 3.52e+01, 4.16e+00]    [3.59e+01, 3.52e+01, 4.22e+00, 4.20e+00]
12000     [3.32e+00, 6.34e-01, 4.12e+00]    [0.00e+00, 3.48e+01, 4.12e+00]    [3.57e+01, 3.48e+01, 4.21e+00, 4.19e+00]
13000     [3.22e+00, 5.65e-01, 4.10e+00]    [0.00e+00, 3.46e+01, 4.10e+00]    [3.56e+01, 3.46e+01, 4.21e+00, 4.18e+00]
14000     [3.13e+00, 5.52e-01, 4.07e+00]    [0.00e+00, 3.44e+01, 4.07e+00]    [3.54e+01, 3.44e+01, 4.21e+00, 4.18e+00]
15000     [3.09e+00, 5.56e-01, 4.04e+00]    [0.00e+00, 3.41e+01, 4.04e+00]    [3.52e+01, 3.41e+01, 4.25e+00, 4.21e+00]
16000     [3.06e+00, 5.56e-01, 4.02e+00]    [0.00e+00, 3.38e+01, 4.02e+00]    [3.51e+01, 3.38e+01, 4.29e+00, 4.25e+00]
17000     [3.01e+00, 5.46e-01, 3.99e+00]    [0.00e+00, 3.37e+01, 3.99e+00]    [3.49e+01, 3.37e+01, 4.32e+00, 4.28e+00]
18000     [3.06e+00, 5.87e-01, 3.97e+00]    [0.00e+00, 3.33e+01, 3.97e+00]    [3.47e+01, 3.33e+01, 4.34e+00, 4.29e+00]
19000     [2.92e+00, 4.04e-01, 3.95e+00]    [0.00e+00, 3.32e+01, 3.95e+00]    [3.47e+01, 3.32e+01, 4.37e+00, 4.32e+00]
20000     [2.91e+00, 3.96e-01, 3.92e+00]    [0.00e+00, 3.30e+01, 3.92e+00]    [3.45e+01, 3.30e+01, 4.40e+00, 4.35e+00]
21000     [2.93e+00, 4.60e-01, 3.90e+00]    [0.00e+00, 3.28e+01, 3.90e+00]    [3.45e+01, 3.28e+01, 4.42e+00, 4.37e+00]
22000     [2.88e+00, 3.47e-01, 3.88e+00]    [0.00e+00, 3.28e+01, 3.88e+00]    [3.45e+01, 3.28e+01, 4.46e+00, 4.40e+00]
23000     [2.85e+00, 3.06e-01, 3.86e+00]    [0.00e+00, 3.27e+01, 3.86e+00]    [3.45e+01, 3.27e+01, 4.48e+00, 4.42e+00]
24000     [2.91e+00, 3.76e-01, 3.84e+00]    [0.00e+00, 3.27e+01, 3.84e+00]    [3.45e+01, 3.27e+01, 4.50e+00, 4.45e+00]
25000     [2.83e+00, 2.90e-01, 3.81e+00]    [0.00e+00, 3.27e+01, 3.81e+00]    [3.44e+01, 3.27e+01, 4.53e+00, 4.47e+00]
26000     [2.88e+00, 3.41e-01, 3.79e+00]    [0.00e+00, 3.26e+01, 3.79e+00]    [3.43e+01, 3.26e+01, 4.54e+00, 4.49e+00]
27000     [2.81e+00, 2.72e-01, 3.77e+00]    [0.00e+00, 3.26e+01, 3.77e+00]    [3.43e+01, 3.26e+01, 4.57e+00, 4.51e+00]
28000     [2.81e+00, 2.27e-01, 3.75e+00]    [0.00e+00, 3.25e+01, 3.75e+00]    [3.42e+01, 3.25e+01, 4.59e+00, 4.53e+00]
29000     [2.84e+00, 3.11e-01, 3.73e+00]    [0.00e+00, 3.25e+01, 3.73e+00]    [3.42e+01, 3.25e+01, 4.61e+00, 4.55e+00]
30000     [2.78e+00, 2.08e-01, 3.71e+00]    [0.00e+00, 3.24e+01, 3.71e+00]    [3.41e+01, 3.24e+01, 4.62e+00, 4.56e+00]

Best model at step 30000:
  train loss: 6.70e+00
  test loss: 3.61e+01
  test metric: [3.41e+01, 3.24e+01, 4.62e+00, 4.56e+00]

'train' took 39.386432 s

Saving loss history to C:\Users\josep\GitHub\deep-learning-for-indentation\src\loss.dat ...
Saving training data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\train.dat ...
Saving test data to C:\Users\josep\GitHub\deep-learning-for-indentation\src\test.dat ...